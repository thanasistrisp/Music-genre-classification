{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08-3P-NUQWXo"
      },
      "source": [
        "<center><span style=\"font-size:xx-large;\">ΕΠ08 Machine Learning - Pattern Recognition</span></center>\n",
        "<center><span style=\"font-size:xx-large;\">3η Εργασία</span></center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zUTfl8T0QWXw"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "\tfrom google.colab import output\n",
        "\t!gdown 1jaCTRxpAF6lM4t2A0y3u9LOZfV0pbZLK\n",
        "\t!unzip data.zip\n",
        "\t%rm - rf data.zip\n",
        "\toutput.clear()\n",
        "except:\n",
        "\tprint(\"Need google colab as client\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4LnJp7cPQWXy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_path(data, representation, flag):\n",
        "    return 'music_genre_data_di/' + data + '/' + representation + '/' + ('labels.npy' if flag == 0 else 'X.npy')\n",
        "\n",
        "def npy_loader(path):\n",
        "    if 'labels' in path:\n",
        "        data = np.load(path)\n",
        "        labels_map = ({x:i for i, x in enumerate(np.unique(data))})\n",
        "        temp = np.array([])\n",
        "        temp = temp.astype(np.int64)\n",
        "        for i in range(0, len(data)):\n",
        "            temp = np.append(temp, int(labels_map[data[i]]))\n",
        "        return temp\n",
        "    else:\n",
        "        temp = torch.from_numpy(np.load(path)).type(torch.FloatTensor)\n",
        "        return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r4niu80QWXz",
        "outputId": "f974056f-f35d-4151-875e-760642e298e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 26])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3200"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "rep = 'mfccs'\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_train_labels, file_train_sound):\n",
        "        super(CustomDataset, self).__init__()\n",
        "        self._labels = file_train_labels\n",
        "        self._sound = file_train_sound\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sound = self._sound[index]\n",
        "        labels = self._labels[index]\n",
        "        return sound[None, :], labels\n",
        "\n",
        "training_data = CustomDataset(npy_loader(get_path('train', rep, 0)), npy_loader(get_path('train', rep, 1)))\n",
        "val_data = CustomDataset(npy_loader(get_path('val', rep, 0)), npy_loader(get_path('val', rep, 1)))\n",
        "test_data = CustomDataset(npy_loader(get_path('test', rep, 0)), npy_loader(get_path('test', rep, 1)))\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=800, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1376, shuffle=False)\n",
        "\n",
        "print(training_data[0][0].shape)\n",
        "len(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g22Mnze3QWX2",
        "outputId": "12afa1e1-ace6-415b-e8a8-e550efb230ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SwnbmyYJQWX2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IzK8P-ltQWX3"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(26, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.layers(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_xtPpCxQWX3",
        "outputId": "3dcef4ae-f86e-4a15-8f22-1a55a50ffedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=26, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIlcYiAxQWX4"
      },
      "source": [
        "![neuralnetwork.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrtTsze0QWYB",
        "outputId": "9f6da297-fae5-452a-eb55-23aeae7617ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: tensor([3], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1,26, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=-1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FvGOS-87QWYD"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def train_nn(num_epochs, optimizer, dataloader, loss_fn, model, learning_rate):\n",
        "    # copy by value the model\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    model_copy.train()\n",
        "    optimizer = optimizer(model_copy.parameters(), lr=learning_rate)\n",
        "    for t in range(num_epochs):\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        size = len(dataloader.dataset)\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            pred = model_copy(X)\n",
        "            # loss_fn defined above to be  nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step() # updating the weights of neural network\n",
        "\n",
        "            # per batch report the value of the loss function on the training set\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return model_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFdlJb1pQWYE",
        "outputId": "8e20fbeb-67df-479b-f5c8-b6d8ac5eb52f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.3\n"
          ]
        }
      ],
      "source": [
        "%pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jp-o8WRvQWYF"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.functional import f1_score\n",
        "\n",
        "def test_nn(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    # initialize the loss function\n",
        "    loss = 0\n",
        "    # initialize the number of correct predictions\n",
        "    correct = 0\n",
        "    # initialize the confusion matrix\n",
        "    confusion_matrix = torch.zeros(4, 4)\n",
        "    # initialize the f1 score\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            pred = model(X)\n",
        "            loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            _, preds = torch.max(pred, 1)\n",
        "            for t, p in zip(y.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "    loss /= size\n",
        "    correct /= size\n",
        "    # calculate f1 macro averaged\n",
        "    f1 = f1_score(pred, y, num_classes=4, average='macro')\n",
        "    # calculate confusion matrix\n",
        "    return loss, f1.item(), correct, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xp-lj4FLQWYH"
      },
      "outputs": [],
      "source": [
        "# define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# define the hyperparameters of the optimization/training process\n",
        "learning_rate = 2e-3\n",
        "num_epochs = 30\n",
        "# define the optimizer object\n",
        "# Stochastic Gradient Descent\n",
        "optimizer = torch.optim.SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd7VSYhKQWYH",
        "outputId": "c1608bfb-4171-49e8-c502-0f0b9b3986e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
            "loss: 1.321976  [  928/ 3200]\n",
            "loss: 1.346422  [  944/ 3200]\n",
            "loss: 1.347357  [  960/ 3200]\n",
            "loss: 1.299327  [  976/ 3200]\n",
            "loss: 1.311090  [  992/ 3200]\n",
            "loss: 1.309946  [ 1008/ 3200]\n",
            "loss: 1.352619  [ 1024/ 3200]\n",
            "loss: 1.369092  [ 1040/ 3200]\n",
            "loss: 1.348496  [ 1056/ 3200]\n",
            "loss: 1.354751  [ 1072/ 3200]\n",
            "loss: 1.326494  [ 1088/ 3200]\n",
            "loss: 1.368757  [ 1104/ 3200]\n",
            "loss: 1.313691  [ 1120/ 3200]\n",
            "loss: 1.371408  [ 1136/ 3200]\n",
            "loss: 1.325421  [ 1152/ 3200]\n",
            "loss: 1.356895  [ 1168/ 3200]\n",
            "loss: 1.347346  [ 1184/ 3200]\n",
            "loss: 1.350955  [ 1200/ 3200]\n",
            "loss: 1.382154  [ 1216/ 3200]\n",
            "loss: 1.309241  [ 1232/ 3200]\n",
            "loss: 1.361070  [ 1248/ 3200]\n",
            "loss: 1.320360  [ 1264/ 3200]\n",
            "loss: 1.327271  [ 1280/ 3200]\n",
            "loss: 1.351011  [ 1296/ 3200]\n",
            "loss: 1.325349  [ 1312/ 3200]\n",
            "loss: 1.347853  [ 1328/ 3200]\n",
            "loss: 1.344605  [ 1344/ 3200]\n",
            "loss: 1.357755  [ 1360/ 3200]\n",
            "loss: 1.378933  [ 1376/ 3200]\n",
            "loss: 1.352506  [ 1392/ 3200]\n",
            "loss: 1.329163  [ 1408/ 3200]\n",
            "loss: 1.329754  [ 1424/ 3200]\n",
            "loss: 1.316021  [ 1440/ 3200]\n",
            "loss: 1.320387  [ 1456/ 3200]\n",
            "loss: 1.310329  [ 1472/ 3200]\n",
            "loss: 1.364339  [ 1488/ 3200]\n",
            "loss: 1.328479  [ 1504/ 3200]\n",
            "loss: 1.332371  [ 1520/ 3200]\n",
            "loss: 1.276243  [ 1536/ 3200]\n",
            "loss: 1.405941  [ 1552/ 3200]\n",
            "loss: 1.403027  [ 1568/ 3200]\n",
            "loss: 1.337599  [ 1584/ 3200]\n",
            "loss: 1.325790  [ 1600/ 3200]\n",
            "loss: 1.333076  [ 1616/ 3200]\n",
            "loss: 1.343691  [ 1632/ 3200]\n",
            "loss: 1.335606  [ 1648/ 3200]\n",
            "loss: 1.364839  [ 1664/ 3200]\n",
            "loss: 1.314877  [ 1680/ 3200]\n",
            "loss: 1.279089  [ 1696/ 3200]\n",
            "loss: 1.402757  [ 1712/ 3200]\n",
            "loss: 1.352380  [ 1728/ 3200]\n",
            "loss: 1.290205  [ 1744/ 3200]\n",
            "loss: 1.268676  [ 1760/ 3200]\n",
            "loss: 1.392776  [ 1776/ 3200]\n",
            "loss: 1.369625  [ 1792/ 3200]\n",
            "loss: 1.325840  [ 1808/ 3200]\n",
            "loss: 1.326566  [ 1824/ 3200]\n",
            "loss: 1.306781  [ 1840/ 3200]\n",
            "loss: 1.339331  [ 1856/ 3200]\n",
            "loss: 1.366528  [ 1872/ 3200]\n",
            "loss: 1.347020  [ 1888/ 3200]\n",
            "loss: 1.326036  [ 1904/ 3200]\n",
            "loss: 1.321215  [ 1920/ 3200]\n",
            "loss: 1.378884  [ 1936/ 3200]\n",
            "loss: 1.337203  [ 1952/ 3200]\n",
            "loss: 1.323440  [ 1968/ 3200]\n",
            "loss: 1.319267  [ 1984/ 3200]\n",
            "loss: 1.349026  [ 2000/ 3200]\n",
            "loss: 1.335442  [ 2016/ 3200]\n",
            "loss: 1.312866  [ 2032/ 3200]\n",
            "loss: 1.325050  [ 2048/ 3200]\n",
            "loss: 1.297227  [ 2064/ 3200]\n",
            "loss: 1.301272  [ 2080/ 3200]\n",
            "loss: 1.284869  [ 2096/ 3200]\n",
            "loss: 1.354185  [ 2112/ 3200]\n",
            "loss: 1.354046  [ 2128/ 3200]\n",
            "loss: 1.329045  [ 2144/ 3200]\n",
            "loss: 1.313226  [ 2160/ 3200]\n",
            "loss: 1.301359  [ 2176/ 3200]\n",
            "loss: 1.312914  [ 2192/ 3200]\n",
            "loss: 1.329915  [ 2208/ 3200]\n",
            "loss: 1.347049  [ 2224/ 3200]\n",
            "loss: 1.290159  [ 2240/ 3200]\n",
            "loss: 1.335328  [ 2256/ 3200]\n",
            "loss: 1.297499  [ 2272/ 3200]\n",
            "loss: 1.353615  [ 2288/ 3200]\n",
            "loss: 1.337361  [ 2304/ 3200]\n",
            "loss: 1.322568  [ 2320/ 3200]\n",
            "loss: 1.333734  [ 2336/ 3200]\n",
            "loss: 1.362036  [ 2352/ 3200]\n",
            "loss: 1.325346  [ 2368/ 3200]\n",
            "loss: 1.326290  [ 2384/ 3200]\n",
            "loss: 1.350247  [ 2400/ 3200]\n",
            "loss: 1.337786  [ 2416/ 3200]\n",
            "loss: 1.328582  [ 2432/ 3200]\n",
            "loss: 1.337147  [ 2448/ 3200]\n",
            "loss: 1.351941  [ 2464/ 3200]\n",
            "loss: 1.322071  [ 2480/ 3200]\n",
            "loss: 1.373479  [ 2496/ 3200]\n",
            "loss: 1.314927  [ 2512/ 3200]\n",
            "loss: 1.382082  [ 2528/ 3200]\n",
            "loss: 1.301372  [ 2544/ 3200]\n",
            "loss: 1.288763  [ 2560/ 3200]\n",
            "loss: 1.325960  [ 2576/ 3200]\n",
            "loss: 1.357667  [ 2592/ 3200]\n",
            "loss: 1.334974  [ 2608/ 3200]\n",
            "loss: 1.345586  [ 2624/ 3200]\n",
            "loss: 1.340471  [ 2640/ 3200]\n",
            "loss: 1.295971  [ 2656/ 3200]\n",
            "loss: 1.326063  [ 2672/ 3200]\n",
            "loss: 1.352900  [ 2688/ 3200]\n",
            "loss: 1.338059  [ 2704/ 3200]\n",
            "loss: 1.276139  [ 2720/ 3200]\n",
            "loss: 1.341672  [ 2736/ 3200]\n",
            "loss: 1.308985  [ 2752/ 3200]\n",
            "loss: 1.315553  [ 2768/ 3200]\n",
            "loss: 1.331840  [ 2784/ 3200]\n",
            "loss: 1.361342  [ 2800/ 3200]\n",
            "loss: 1.306311  [ 2816/ 3200]\n",
            "loss: 1.335818  [ 2832/ 3200]\n",
            "loss: 1.276765  [ 2848/ 3200]\n",
            "loss: 1.275745  [ 2864/ 3200]\n",
            "loss: 1.337760  [ 2880/ 3200]\n",
            "loss: 1.331267  [ 2896/ 3200]\n",
            "loss: 1.348862  [ 2912/ 3200]\n",
            "loss: 1.281089  [ 2928/ 3200]\n",
            "loss: 1.202437  [ 2944/ 3200]\n",
            "loss: 1.347126  [ 2960/ 3200]\n",
            "loss: 1.269052  [ 2976/ 3200]\n",
            "loss: 1.298893  [ 2992/ 3200]\n",
            "loss: 1.329510  [ 3008/ 3200]\n",
            "loss: 1.408722  [ 3024/ 3200]\n",
            "loss: 1.337621  [ 3040/ 3200]\n",
            "loss: 1.352659  [ 3056/ 3200]\n",
            "loss: 1.379282  [ 3072/ 3200]\n",
            "loss: 1.277234  [ 3088/ 3200]\n",
            "loss: 1.310651  [ 3104/ 3200]\n",
            "loss: 1.351147  [ 3120/ 3200]\n",
            "loss: 1.316256  [ 3136/ 3200]\n",
            "loss: 1.321385  [ 3152/ 3200]\n",
            "loss: 1.319715  [ 3168/ 3200]\n",
            "loss: 1.234969  [ 3184/ 3200]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.360214  [    0/ 3200]\n",
            "loss: 1.325443  [   16/ 3200]\n",
            "loss: 1.313947  [   32/ 3200]\n",
            "loss: 1.319362  [   48/ 3200]\n",
            "loss: 1.346130  [   64/ 3200]\n",
            "loss: 1.358452  [   80/ 3200]\n",
            "loss: 1.294379  [   96/ 3200]\n",
            "loss: 1.250732  [  112/ 3200]\n",
            "loss: 1.281253  [  128/ 3200]\n",
            "loss: 1.302954  [  144/ 3200]\n",
            "loss: 1.388358  [  160/ 3200]\n",
            "loss: 1.332088  [  176/ 3200]\n",
            "loss: 1.368914  [  192/ 3200]\n",
            "loss: 1.326795  [  208/ 3200]\n",
            "loss: 1.331231  [  224/ 3200]\n",
            "loss: 1.336526  [  240/ 3200]\n",
            "loss: 1.320366  [  256/ 3200]\n",
            "loss: 1.283992  [  272/ 3200]\n",
            "loss: 1.263224  [  288/ 3200]\n",
            "loss: 1.393348  [  304/ 3200]\n",
            "loss: 1.343703  [  320/ 3200]\n",
            "loss: 1.333969  [  336/ 3200]\n",
            "loss: 1.297127  [  352/ 3200]\n",
            "loss: 1.267414  [  368/ 3200]\n",
            "loss: 1.387630  [  384/ 3200]\n",
            "loss: 1.317877  [  400/ 3200]\n",
            "loss: 1.324143  [  416/ 3200]\n",
            "loss: 1.332539  [  432/ 3200]\n",
            "loss: 1.296766  [  448/ 3200]\n",
            "loss: 1.325109  [  464/ 3200]\n",
            "loss: 1.307953  [  480/ 3200]\n",
            "loss: 1.295989  [  496/ 3200]\n",
            "loss: 1.348154  [  512/ 3200]\n",
            "loss: 1.269898  [  528/ 3200]\n",
            "loss: 1.391795  [  544/ 3200]\n",
            "loss: 1.341066  [  560/ 3200]\n",
            "loss: 1.325361  [  576/ 3200]\n",
            "loss: 1.366168  [  592/ 3200]\n",
            "loss: 1.297903  [  608/ 3200]\n",
            "loss: 1.342360  [  624/ 3200]\n",
            "loss: 1.333233  [  640/ 3200]\n",
            "loss: 1.299164  [  656/ 3200]\n",
            "loss: 1.292305  [  672/ 3200]\n",
            "loss: 1.331711  [  688/ 3200]\n",
            "loss: 1.294408  [  704/ 3200]\n",
            "loss: 1.346801  [  720/ 3200]\n",
            "loss: 1.283638  [  736/ 3200]\n",
            "loss: 1.388989  [  752/ 3200]\n",
            "loss: 1.350433  [  768/ 3200]\n",
            "loss: 1.368493  [  784/ 3200]\n",
            "loss: 1.302014  [  800/ 3200]\n",
            "loss: 1.333439  [  816/ 3200]\n",
            "loss: 1.310214  [  832/ 3200]\n",
            "loss: 1.313041  [  848/ 3200]\n",
            "loss: 1.299992  [  864/ 3200]\n",
            "loss: 1.335259  [  880/ 3200]\n",
            "loss: 1.303370  [  896/ 3200]\n",
            "loss: 1.338256  [  912/ 3200]\n",
            "loss: 1.318640  [  928/ 3200]\n",
            "loss: 1.304286  [  944/ 3200]\n",
            "loss: 1.273898  [  960/ 3200]\n",
            "loss: 1.343898  [  976/ 3200]\n",
            "loss: 1.310214  [  992/ 3200]\n",
            "loss: 1.329034  [ 1008/ 3200]\n",
            "loss: 1.304598  [ 1024/ 3200]\n",
            "loss: 1.338365  [ 1040/ 3200]\n",
            "loss: 1.348089  [ 1056/ 3200]\n",
            "loss: 1.337668  [ 1072/ 3200]\n",
            "loss: 1.304412  [ 1088/ 3200]\n",
            "loss: 1.309765  [ 1104/ 3200]\n",
            "loss: 1.294207  [ 1120/ 3200]\n",
            "loss: 1.274603  [ 1136/ 3200]\n",
            "loss: 1.348325  [ 1152/ 3200]\n",
            "loss: 1.336689  [ 1168/ 3200]\n",
            "loss: 1.304193  [ 1184/ 3200]\n",
            "loss: 1.336495  [ 1200/ 3200]\n",
            "loss: 1.321135  [ 1216/ 3200]\n",
            "loss: 1.278291  [ 1232/ 3200]\n",
            "loss: 1.327116  [ 1248/ 3200]\n",
            "loss: 1.300900  [ 1264/ 3200]\n",
            "loss: 1.340279  [ 1280/ 3200]\n",
            "loss: 1.326877  [ 1296/ 3200]\n",
            "loss: 1.309432  [ 1312/ 3200]\n",
            "loss: 1.329938  [ 1328/ 3200]\n",
            "loss: 1.346467  [ 1344/ 3200]\n",
            "loss: 1.339024  [ 1360/ 3200]\n",
            "loss: 1.278982  [ 1376/ 3200]\n",
            "loss: 1.312497  [ 1392/ 3200]\n",
            "loss: 1.299299  [ 1408/ 3200]\n",
            "loss: 1.328971  [ 1424/ 3200]\n",
            "loss: 1.318225  [ 1440/ 3200]\n",
            "loss: 1.336242  [ 1456/ 3200]\n",
            "loss: 1.306664  [ 1472/ 3200]\n",
            "loss: 1.308430  [ 1488/ 3200]\n",
            "loss: 1.315116  [ 1504/ 3200]\n",
            "loss: 1.340581  [ 1520/ 3200]\n",
            "loss: 1.309247  [ 1536/ 3200]\n",
            "loss: 1.329120  [ 1552/ 3200]\n",
            "loss: 1.353668  [ 1568/ 3200]\n",
            "loss: 1.319969  [ 1584/ 3200]\n",
            "loss: 1.332131  [ 1600/ 3200]\n",
            "loss: 1.321069  [ 1616/ 3200]\n",
            "loss: 1.346524  [ 1632/ 3200]\n",
            "loss: 1.316387  [ 1648/ 3200]\n",
            "loss: 1.322803  [ 1664/ 3200]\n",
            "loss: 1.333947  [ 1680/ 3200]\n",
            "loss: 1.293989  [ 1696/ 3200]\n",
            "loss: 1.318092  [ 1712/ 3200]\n",
            "loss: 1.336305  [ 1728/ 3200]\n",
            "loss: 1.333173  [ 1744/ 3200]\n",
            "loss: 1.360270  [ 1760/ 3200]\n",
            "loss: 1.342146  [ 1776/ 3200]\n",
            "loss: 1.266655  [ 1792/ 3200]\n",
            "loss: 1.278270  [ 1808/ 3200]\n",
            "loss: 1.328651  [ 1824/ 3200]\n",
            "loss: 1.376382  [ 1840/ 3200]\n",
            "loss: 1.317844  [ 1856/ 3200]\n",
            "loss: 1.289999  [ 1872/ 3200]\n",
            "loss: 1.331531  [ 1888/ 3200]\n",
            "loss: 1.322592  [ 1904/ 3200]\n",
            "loss: 1.328619  [ 1920/ 3200]\n",
            "loss: 1.296409  [ 1936/ 3200]\n",
            "loss: 1.301080  [ 1952/ 3200]\n",
            "loss: 1.276559  [ 1968/ 3200]\n",
            "loss: 1.319613  [ 1984/ 3200]\n",
            "loss: 1.280391  [ 2000/ 3200]\n",
            "loss: 1.282308  [ 2016/ 3200]\n",
            "loss: 1.306698  [ 2032/ 3200]\n",
            "loss: 1.295913  [ 2048/ 3200]\n",
            "loss: 1.353759  [ 2064/ 3200]\n",
            "loss: 1.382004  [ 2080/ 3200]\n",
            "loss: 1.303240  [ 2096/ 3200]\n",
            "loss: 1.370223  [ 2112/ 3200]\n",
            "loss: 1.331884  [ 2128/ 3200]\n",
            "loss: 1.343006  [ 2144/ 3200]\n",
            "loss: 1.307759  [ 2160/ 3200]\n",
            "loss: 1.346158  [ 2176/ 3200]\n",
            "loss: 1.320999  [ 2192/ 3200]\n",
            "loss: 1.307323  [ 2208/ 3200]\n",
            "loss: 1.316386  [ 2224/ 3200]\n",
            "loss: 1.273177  [ 2240/ 3200]\n",
            "loss: 1.321604  [ 2256/ 3200]\n",
            "loss: 1.272365  [ 2272/ 3200]\n",
            "loss: 1.359779  [ 2288/ 3200]\n",
            "loss: 1.359246  [ 2304/ 3200]\n",
            "loss: 1.420794  [ 2320/ 3200]\n",
            "loss: 1.279234  [ 2336/ 3200]\n",
            "loss: 1.290670  [ 2352/ 3200]\n",
            "loss: 1.278322  [ 2368/ 3200]\n",
            "loss: 1.314368  [ 2384/ 3200]\n",
            "loss: 1.307837  [ 2400/ 3200]\n",
            "loss: 1.294584  [ 2416/ 3200]\n",
            "loss: 1.339110  [ 2432/ 3200]\n",
            "loss: 1.330479  [ 2448/ 3200]\n",
            "loss: 1.365564  [ 2464/ 3200]\n",
            "loss: 1.324399  [ 2480/ 3200]\n",
            "loss: 1.316272  [ 2496/ 3200]\n",
            "loss: 1.270536  [ 2512/ 3200]\n",
            "loss: 1.332760  [ 2528/ 3200]\n",
            "loss: 1.332403  [ 2544/ 3200]\n",
            "loss: 1.272796  [ 2560/ 3200]\n",
            "loss: 1.309348  [ 2576/ 3200]\n",
            "loss: 1.372841  [ 2592/ 3200]\n",
            "loss: 1.333481  [ 2608/ 3200]\n",
            "loss: 1.357338  [ 2624/ 3200]\n",
            "loss: 1.316150  [ 2640/ 3200]\n",
            "loss: 1.305254  [ 2656/ 3200]\n",
            "loss: 1.296715  [ 2672/ 3200]\n",
            "loss: 1.274810  [ 2688/ 3200]\n",
            "loss: 1.325463  [ 2704/ 3200]\n",
            "loss: 1.322518  [ 2720/ 3200]\n",
            "loss: 1.346592  [ 2736/ 3200]\n",
            "loss: 1.295575  [ 2752/ 3200]\n",
            "loss: 1.348594  [ 2768/ 3200]\n",
            "loss: 1.329183  [ 2784/ 3200]\n",
            "loss: 1.300424  [ 2800/ 3200]\n",
            "loss: 1.360591  [ 2816/ 3200]\n",
            "loss: 1.320497  [ 2832/ 3200]\n",
            "loss: 1.318344  [ 2848/ 3200]\n",
            "loss: 1.335294  [ 2864/ 3200]\n",
            "loss: 1.302015  [ 2880/ 3200]\n",
            "loss: 1.335150  [ 2896/ 3200]\n",
            "loss: 1.354436  [ 2912/ 3200]\n",
            "loss: 1.325789  [ 2928/ 3200]\n",
            "loss: 1.307401  [ 2944/ 3200]\n",
            "loss: 1.322926  [ 2960/ 3200]\n",
            "loss: 1.338422  [ 2976/ 3200]\n",
            "loss: 1.278384  [ 2992/ 3200]\n",
            "loss: 1.326785  [ 3008/ 3200]\n",
            "loss: 1.286340  [ 3024/ 3200]\n",
            "loss: 1.325446  [ 3040/ 3200]\n",
            "loss: 1.315226  [ 3056/ 3200]\n",
            "loss: 1.355314  [ 3072/ 3200]\n",
            "loss: 1.350545  [ 3088/ 3200]\n",
            "loss: 1.279052  [ 3104/ 3200]\n",
            "loss: 1.338204  [ 3120/ 3200]\n",
            "loss: 1.324034  [ 3136/ 3200]\n",
            "loss: 1.322714  [ 3152/ 3200]\n",
            "loss: 1.327518  [ 3168/ 3200]\n",
            "loss: 1.299260  [ 3184/ 3200]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.321436  [    0/ 3200]\n",
            "loss: 1.336102  [   16/ 3200]\n",
            "loss: 1.294495  [   32/ 3200]\n",
            "loss: 1.294844  [   48/ 3200]\n",
            "loss: 1.328233  [   64/ 3200]\n",
            "loss: 1.324077  [   80/ 3200]\n",
            "loss: 1.342028  [   96/ 3200]\n",
            "loss: 1.325154  [  112/ 3200]\n",
            "loss: 1.321246  [  128/ 3200]\n",
            "loss: 1.337298  [  144/ 3200]\n",
            "loss: 1.316967  [  160/ 3200]\n",
            "loss: 1.321515  [  176/ 3200]\n",
            "loss: 1.322798  [  192/ 3200]\n",
            "loss: 1.324505  [  208/ 3200]\n",
            "loss: 1.341181  [  224/ 3200]\n",
            "loss: 1.335785  [  240/ 3200]\n",
            "loss: 1.289384  [  256/ 3200]\n",
            "loss: 1.339475  [  272/ 3200]\n",
            "loss: 1.266829  [  288/ 3200]\n",
            "loss: 1.278632  [  304/ 3200]\n",
            "loss: 1.352213  [  320/ 3200]\n",
            "loss: 1.279225  [  336/ 3200]\n",
            "loss: 1.275981  [  352/ 3200]\n",
            "loss: 1.326639  [  368/ 3200]\n",
            "loss: 1.304717  [  384/ 3200]\n",
            "loss: 1.290050  [  400/ 3200]\n",
            "loss: 1.269963  [  416/ 3200]\n",
            "loss: 1.254540  [  432/ 3200]\n",
            "loss: 1.290283  [  448/ 3200]\n",
            "loss: 1.350627  [  464/ 3200]\n",
            "loss: 1.290946  [  480/ 3200]\n",
            "loss: 1.318894  [  496/ 3200]\n",
            "loss: 1.315262  [  512/ 3200]\n",
            "loss: 1.310056  [  528/ 3200]\n",
            "loss: 1.354913  [  544/ 3200]\n",
            "loss: 1.323264  [  560/ 3200]\n",
            "loss: 1.284322  [  576/ 3200]\n",
            "loss: 1.297403  [  592/ 3200]\n",
            "loss: 1.294925  [  608/ 3200]\n",
            "loss: 1.318692  [  624/ 3200]\n",
            "loss: 1.316381  [  640/ 3200]\n",
            "loss: 1.305171  [  656/ 3200]\n",
            "loss: 1.321356  [  672/ 3200]\n",
            "loss: 1.290929  [  688/ 3200]\n",
            "loss: 1.315671  [  704/ 3200]\n",
            "loss: 1.299426  [  720/ 3200]\n",
            "loss: 1.264139  [  736/ 3200]\n",
            "loss: 1.274545  [  752/ 3200]\n",
            "loss: 1.259260  [  768/ 3200]\n",
            "loss: 1.343253  [  784/ 3200]\n",
            "loss: 1.331044  [  800/ 3200]\n",
            "loss: 1.332392  [  816/ 3200]\n",
            "loss: 1.283070  [  832/ 3200]\n",
            "loss: 1.314778  [  848/ 3200]\n",
            "loss: 1.310321  [  864/ 3200]\n",
            "loss: 1.354066  [  880/ 3200]\n",
            "loss: 1.357819  [  896/ 3200]\n",
            "loss: 1.322183  [  912/ 3200]\n",
            "loss: 1.347480  [  928/ 3200]\n",
            "loss: 1.299686  [  944/ 3200]\n",
            "loss: 1.338000  [  960/ 3200]\n",
            "loss: 1.269633  [  976/ 3200]\n",
            "loss: 1.353925  [  992/ 3200]\n",
            "loss: 1.353584  [ 1008/ 3200]\n",
            "loss: 1.336500  [ 1024/ 3200]\n",
            "loss: 1.309168  [ 1040/ 3200]\n",
            "loss: 1.282108  [ 1056/ 3200]\n",
            "loss: 1.305452  [ 1072/ 3200]\n",
            "loss: 1.313515  [ 1088/ 3200]\n",
            "loss: 1.310734  [ 1104/ 3200]\n",
            "loss: 1.340427  [ 1120/ 3200]\n",
            "loss: 1.333000  [ 1136/ 3200]\n",
            "loss: 1.328088  [ 1152/ 3200]\n",
            "loss: 1.301454  [ 1168/ 3200]\n",
            "loss: 1.311001  [ 1184/ 3200]\n",
            "loss: 1.274182  [ 1200/ 3200]\n",
            "loss: 1.285053  [ 1216/ 3200]\n",
            "loss: 1.302482  [ 1232/ 3200]\n",
            "loss: 1.322804  [ 1248/ 3200]\n",
            "loss: 1.300553  [ 1264/ 3200]\n",
            "loss: 1.318045  [ 1280/ 3200]\n",
            "loss: 1.299587  [ 1296/ 3200]\n",
            "loss: 1.288145  [ 1312/ 3200]\n",
            "loss: 1.343489  [ 1328/ 3200]\n",
            "loss: 1.290044  [ 1344/ 3200]\n",
            "loss: 1.303630  [ 1360/ 3200]\n",
            "loss: 1.307631  [ 1376/ 3200]\n",
            "loss: 1.307953  [ 1392/ 3200]\n",
            "loss: 1.296591  [ 1408/ 3200]\n",
            "loss: 1.320120  [ 1424/ 3200]\n",
            "loss: 1.321029  [ 1440/ 3200]\n",
            "loss: 1.333007  [ 1456/ 3200]\n",
            "loss: 1.278929  [ 1472/ 3200]\n",
            "loss: 1.264806  [ 1488/ 3200]\n",
            "loss: 1.310045  [ 1504/ 3200]\n",
            "loss: 1.273853  [ 1520/ 3200]\n",
            "loss: 1.291023  [ 1536/ 3200]\n",
            "loss: 1.274323  [ 1552/ 3200]\n",
            "loss: 1.274707  [ 1568/ 3200]\n",
            "loss: 1.349254  [ 1584/ 3200]\n",
            "loss: 1.278657  [ 1600/ 3200]\n",
            "loss: 1.279287  [ 1616/ 3200]\n",
            "loss: 1.252496  [ 1632/ 3200]\n",
            "loss: 1.338698  [ 1648/ 3200]\n",
            "loss: 1.261730  [ 1664/ 3200]\n",
            "loss: 1.376087  [ 1680/ 3200]\n",
            "loss: 1.278711  [ 1696/ 3200]\n",
            "loss: 1.355383  [ 1712/ 3200]\n",
            "loss: 1.339376  [ 1728/ 3200]\n",
            "loss: 1.250170  [ 1744/ 3200]\n",
            "loss: 1.207619  [ 1760/ 3200]\n",
            "loss: 1.297775  [ 1776/ 3200]\n",
            "loss: 1.378237  [ 1792/ 3200]\n",
            "loss: 1.286591  [ 1808/ 3200]\n",
            "loss: 1.265777  [ 1824/ 3200]\n",
            "loss: 1.359849  [ 1840/ 3200]\n",
            "loss: 1.323065  [ 1856/ 3200]\n",
            "loss: 1.285472  [ 1872/ 3200]\n",
            "loss: 1.322163  [ 1888/ 3200]\n",
            "loss: 1.254412  [ 1904/ 3200]\n",
            "loss: 1.338107  [ 1920/ 3200]\n",
            "loss: 1.343458  [ 1936/ 3200]\n",
            "loss: 1.299600  [ 1952/ 3200]\n",
            "loss: 1.326722  [ 1968/ 3200]\n",
            "loss: 1.348733  [ 1984/ 3200]\n",
            "loss: 1.296353  [ 2000/ 3200]\n",
            "loss: 1.218214  [ 2016/ 3200]\n",
            "loss: 1.239178  [ 2032/ 3200]\n",
            "loss: 1.310805  [ 2048/ 3200]\n",
            "loss: 1.350345  [ 2064/ 3200]\n",
            "loss: 1.295979  [ 2080/ 3200]\n",
            "loss: 1.319300  [ 2096/ 3200]\n",
            "loss: 1.282780  [ 2112/ 3200]\n",
            "loss: 1.284130  [ 2128/ 3200]\n",
            "loss: 1.286442  [ 2144/ 3200]\n",
            "loss: 1.314455  [ 2160/ 3200]\n",
            "loss: 1.343928  [ 2176/ 3200]\n",
            "loss: 1.308248  [ 2192/ 3200]\n",
            "loss: 1.282641  [ 2208/ 3200]\n",
            "loss: 1.273833  [ 2224/ 3200]\n",
            "loss: 1.293035  [ 2240/ 3200]\n",
            "loss: 1.267963  [ 2256/ 3200]\n",
            "loss: 1.276865  [ 2272/ 3200]\n",
            "loss: 1.304308  [ 2288/ 3200]\n",
            "loss: 1.323468  [ 2304/ 3200]\n",
            "loss: 1.292684  [ 2320/ 3200]\n",
            "loss: 1.304545  [ 2336/ 3200]\n",
            "loss: 1.261941  [ 2352/ 3200]\n",
            "loss: 1.351122  [ 2368/ 3200]\n",
            "loss: 1.287399  [ 2384/ 3200]\n",
            "loss: 1.346577  [ 2400/ 3200]\n",
            "loss: 1.314041  [ 2416/ 3200]\n",
            "loss: 1.239650  [ 2432/ 3200]\n",
            "loss: 1.305696  [ 2448/ 3200]\n",
            "loss: 1.321620  [ 2464/ 3200]\n",
            "loss: 1.441974  [ 2480/ 3200]\n",
            "loss: 1.275274  [ 2496/ 3200]\n",
            "loss: 1.348637  [ 2512/ 3200]\n",
            "loss: 1.368900  [ 2528/ 3200]\n",
            "loss: 1.297508  [ 2544/ 3200]\n",
            "loss: 1.291400  [ 2560/ 3200]\n",
            "loss: 1.286747  [ 2576/ 3200]\n",
            "loss: 1.308479  [ 2592/ 3200]\n",
            "loss: 1.255324  [ 2608/ 3200]\n",
            "loss: 1.307313  [ 2624/ 3200]\n",
            "loss: 1.313786  [ 2640/ 3200]\n",
            "loss: 1.331655  [ 2656/ 3200]\n",
            "loss: 1.319215  [ 2672/ 3200]\n",
            "loss: 1.334810  [ 2688/ 3200]\n",
            "loss: 1.310337  [ 2704/ 3200]\n",
            "loss: 1.298404  [ 2720/ 3200]\n",
            "loss: 1.248112  [ 2736/ 3200]\n",
            "loss: 1.316960  [ 2752/ 3200]\n",
            "loss: 1.342944  [ 2768/ 3200]\n",
            "loss: 1.292320  [ 2784/ 3200]\n",
            "loss: 1.292319  [ 2800/ 3200]\n",
            "loss: 1.292989  [ 2816/ 3200]\n",
            "loss: 1.288223  [ 2832/ 3200]\n",
            "loss: 1.283096  [ 2848/ 3200]\n",
            "loss: 1.310257  [ 2864/ 3200]\n",
            "loss: 1.326548  [ 2880/ 3200]\n",
            "loss: 1.279799  [ 2896/ 3200]\n",
            "loss: 1.311653  [ 2912/ 3200]\n",
            "loss: 1.326089  [ 2928/ 3200]\n",
            "loss: 1.269724  [ 2944/ 3200]\n",
            "loss: 1.296879  [ 2960/ 3200]\n",
            "loss: 1.360987  [ 2976/ 3200]\n",
            "loss: 1.289541  [ 2992/ 3200]\n",
            "loss: 1.314574  [ 3008/ 3200]\n",
            "loss: 1.259216  [ 3024/ 3200]\n",
            "loss: 1.298061  [ 3040/ 3200]\n",
            "loss: 1.321302  [ 3056/ 3200]\n",
            "loss: 1.302564  [ 3072/ 3200]\n",
            "loss: 1.249672  [ 3088/ 3200]\n",
            "loss: 1.261757  [ 3104/ 3200]\n",
            "loss: 1.308146  [ 3120/ 3200]\n",
            "loss: 1.267310  [ 3136/ 3200]\n",
            "loss: 1.322458  [ 3152/ 3200]\n",
            "loss: 1.234886  [ 3168/ 3200]\n",
            "loss: 1.242201  [ 3184/ 3200]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.272848  [    0/ 3200]\n",
            "loss: 1.339504  [   16/ 3200]\n",
            "loss: 1.192408  [   32/ 3200]\n",
            "loss: 1.278687  [   48/ 3200]\n",
            "loss: 1.278642  [   64/ 3200]\n",
            "loss: 1.331159  [   80/ 3200]\n",
            "loss: 1.287642  [   96/ 3200]\n",
            "loss: 1.291627  [  112/ 3200]\n",
            "loss: 1.322350  [  128/ 3200]\n",
            "loss: 1.239982  [  144/ 3200]\n",
            "loss: 1.327243  [  160/ 3200]\n",
            "loss: 1.353016  [  176/ 3200]\n",
            "loss: 1.340524  [  192/ 3200]\n",
            "loss: 1.281034  [  208/ 3200]\n",
            "loss: 1.320844  [  224/ 3200]\n",
            "loss: 1.269548  [  240/ 3200]\n",
            "loss: 1.362285  [  256/ 3200]\n",
            "loss: 1.313248  [  272/ 3200]\n",
            "loss: 1.338458  [  288/ 3200]\n",
            "loss: 1.304814  [  304/ 3200]\n",
            "loss: 1.286190  [  320/ 3200]\n",
            "loss: 1.299219  [  336/ 3200]\n",
            "loss: 1.302215  [  352/ 3200]\n",
            "loss: 1.307738  [  368/ 3200]\n",
            "loss: 1.298518  [  384/ 3200]\n",
            "loss: 1.331364  [  400/ 3200]\n",
            "loss: 1.294960  [  416/ 3200]\n",
            "loss: 1.277514  [  432/ 3200]\n",
            "loss: 1.290356  [  448/ 3200]\n",
            "loss: 1.303466  [  464/ 3200]\n",
            "loss: 1.284427  [  480/ 3200]\n",
            "loss: 1.266255  [  496/ 3200]\n",
            "loss: 1.329438  [  512/ 3200]\n",
            "loss: 1.293608  [  528/ 3200]\n",
            "loss: 1.321569  [  544/ 3200]\n",
            "loss: 1.291988  [  560/ 3200]\n",
            "loss: 1.294356  [  576/ 3200]\n",
            "loss: 1.315173  [  592/ 3200]\n",
            "loss: 1.294492  [  608/ 3200]\n",
            "loss: 1.277701  [  624/ 3200]\n",
            "loss: 1.306002  [  640/ 3200]\n",
            "loss: 1.290413  [  656/ 3200]\n",
            "loss: 1.324379  [  672/ 3200]\n",
            "loss: 1.336442  [  688/ 3200]\n",
            "loss: 1.283445  [  704/ 3200]\n",
            "loss: 1.227639  [  720/ 3200]\n",
            "loss: 1.281433  [  736/ 3200]\n",
            "loss: 1.191704  [  752/ 3200]\n",
            "loss: 1.334839  [  768/ 3200]\n",
            "loss: 1.316778  [  784/ 3200]\n",
            "loss: 1.315288  [  800/ 3200]\n",
            "loss: 1.323039  [  816/ 3200]\n",
            "loss: 1.338323  [  832/ 3200]\n",
            "loss: 1.236357  [  848/ 3200]\n",
            "loss: 1.298070  [  864/ 3200]\n",
            "loss: 1.306754  [  880/ 3200]\n",
            "loss: 1.227369  [  896/ 3200]\n",
            "loss: 1.314726  [  912/ 3200]\n",
            "loss: 1.372023  [  928/ 3200]\n",
            "loss: 1.250745  [  944/ 3200]\n",
            "loss: 1.345858  [  960/ 3200]\n",
            "loss: 1.302856  [  976/ 3200]\n",
            "loss: 1.289472  [  992/ 3200]\n",
            "loss: 1.278970  [ 1008/ 3200]\n",
            "loss: 1.322174  [ 1024/ 3200]\n",
            "loss: 1.302772  [ 1040/ 3200]\n",
            "loss: 1.280477  [ 1056/ 3200]\n",
            "loss: 1.275860  [ 1072/ 3200]\n",
            "loss: 1.281850  [ 1088/ 3200]\n",
            "loss: 1.258618  [ 1104/ 3200]\n",
            "loss: 1.324472  [ 1120/ 3200]\n",
            "loss: 1.285413  [ 1136/ 3200]\n",
            "loss: 1.331257  [ 1152/ 3200]\n",
            "loss: 1.341513  [ 1168/ 3200]\n",
            "loss: 1.303407  [ 1184/ 3200]\n",
            "loss: 1.332545  [ 1200/ 3200]\n",
            "loss: 1.259154  [ 1216/ 3200]\n",
            "loss: 1.248253  [ 1232/ 3200]\n",
            "loss: 1.309334  [ 1248/ 3200]\n",
            "loss: 1.373095  [ 1264/ 3200]\n",
            "loss: 1.326408  [ 1280/ 3200]\n",
            "loss: 1.239867  [ 1296/ 3200]\n",
            "loss: 1.230826  [ 1312/ 3200]\n",
            "loss: 1.234588  [ 1328/ 3200]\n",
            "loss: 1.378961  [ 1344/ 3200]\n",
            "loss: 1.318082  [ 1360/ 3200]\n",
            "loss: 1.281946  [ 1376/ 3200]\n",
            "loss: 1.306525  [ 1392/ 3200]\n",
            "loss: 1.219445  [ 1408/ 3200]\n",
            "loss: 1.286977  [ 1424/ 3200]\n",
            "loss: 1.315022  [ 1440/ 3200]\n",
            "loss: 1.287975  [ 1456/ 3200]\n",
            "loss: 1.302397  [ 1472/ 3200]\n",
            "loss: 1.214067  [ 1488/ 3200]\n",
            "loss: 1.275744  [ 1504/ 3200]\n",
            "loss: 1.250011  [ 1520/ 3200]\n",
            "loss: 1.312911  [ 1536/ 3200]\n",
            "loss: 1.311498  [ 1552/ 3200]\n",
            "loss: 1.244602  [ 1568/ 3200]\n",
            "loss: 1.262487  [ 1584/ 3200]\n",
            "loss: 1.279394  [ 1600/ 3200]\n",
            "loss: 1.282891  [ 1616/ 3200]\n",
            "loss: 1.313034  [ 1632/ 3200]\n",
            "loss: 1.317065  [ 1648/ 3200]\n",
            "loss: 1.308114  [ 1664/ 3200]\n",
            "loss: 1.334707  [ 1680/ 3200]\n",
            "loss: 1.352998  [ 1696/ 3200]\n",
            "loss: 1.315237  [ 1712/ 3200]\n",
            "loss: 1.268434  [ 1728/ 3200]\n",
            "loss: 1.280764  [ 1744/ 3200]\n",
            "loss: 1.358068  [ 1760/ 3200]\n",
            "loss: 1.204594  [ 1776/ 3200]\n",
            "loss: 1.272934  [ 1792/ 3200]\n",
            "loss: 1.276460  [ 1808/ 3200]\n",
            "loss: 1.316567  [ 1824/ 3200]\n",
            "loss: 1.310356  [ 1840/ 3200]\n",
            "loss: 1.293500  [ 1856/ 3200]\n",
            "loss: 1.313312  [ 1872/ 3200]\n",
            "loss: 1.256010  [ 1888/ 3200]\n",
            "loss: 1.278244  [ 1904/ 3200]\n",
            "loss: 1.304728  [ 1920/ 3200]\n",
            "loss: 1.210541  [ 1936/ 3200]\n",
            "loss: 1.261513  [ 1952/ 3200]\n",
            "loss: 1.304802  [ 1968/ 3200]\n",
            "loss: 1.268764  [ 1984/ 3200]\n",
            "loss: 1.325554  [ 2000/ 3200]\n",
            "loss: 1.280079  [ 2016/ 3200]\n",
            "loss: 1.280497  [ 2032/ 3200]\n",
            "loss: 1.295675  [ 2048/ 3200]\n",
            "loss: 1.269486  [ 2064/ 3200]\n",
            "loss: 1.368575  [ 2080/ 3200]\n",
            "loss: 1.271149  [ 2096/ 3200]\n",
            "loss: 1.260205  [ 2112/ 3200]\n",
            "loss: 1.307147  [ 2128/ 3200]\n",
            "loss: 1.292879  [ 2144/ 3200]\n",
            "loss: 1.255822  [ 2160/ 3200]\n",
            "loss: 1.324635  [ 2176/ 3200]\n",
            "loss: 1.259727  [ 2192/ 3200]\n",
            "loss: 1.330985  [ 2208/ 3200]\n",
            "loss: 1.281947  [ 2224/ 3200]\n",
            "loss: 1.245053  [ 2240/ 3200]\n",
            "loss: 1.224549  [ 2256/ 3200]\n",
            "loss: 1.335806  [ 2272/ 3200]\n",
            "loss: 1.365837  [ 2288/ 3200]\n",
            "loss: 1.244932  [ 2304/ 3200]\n",
            "loss: 1.274427  [ 2320/ 3200]\n",
            "loss: 1.351080  [ 2336/ 3200]\n",
            "loss: 1.345133  [ 2352/ 3200]\n",
            "loss: 1.242019  [ 2368/ 3200]\n",
            "loss: 1.322188  [ 2384/ 3200]\n",
            "loss: 1.300811  [ 2400/ 3200]\n",
            "loss: 1.215275  [ 2416/ 3200]\n",
            "loss: 1.264647  [ 2432/ 3200]\n",
            "loss: 1.338054  [ 2448/ 3200]\n",
            "loss: 1.305653  [ 2464/ 3200]\n",
            "loss: 1.292636  [ 2480/ 3200]\n",
            "loss: 1.310783  [ 2496/ 3200]\n",
            "loss: 1.295980  [ 2512/ 3200]\n",
            "loss: 1.308355  [ 2528/ 3200]\n",
            "loss: 1.305909  [ 2544/ 3200]\n",
            "loss: 1.211710  [ 2560/ 3200]\n",
            "loss: 1.280442  [ 2576/ 3200]\n",
            "loss: 1.277643  [ 2592/ 3200]\n",
            "loss: 1.292326  [ 2608/ 3200]\n",
            "loss: 1.300470  [ 2624/ 3200]\n",
            "loss: 1.233254  [ 2640/ 3200]\n",
            "loss: 1.325721  [ 2656/ 3200]\n",
            "loss: 1.241821  [ 2672/ 3200]\n",
            "loss: 1.275415  [ 2688/ 3200]\n",
            "loss: 1.254106  [ 2704/ 3200]\n",
            "loss: 1.233643  [ 2720/ 3200]\n",
            "loss: 1.318576  [ 2736/ 3200]\n",
            "loss: 1.315283  [ 2752/ 3200]\n",
            "loss: 1.282434  [ 2768/ 3200]\n",
            "loss: 1.321139  [ 2784/ 3200]\n",
            "loss: 1.270333  [ 2800/ 3200]\n",
            "loss: 1.254800  [ 2816/ 3200]\n",
            "loss: 1.252534  [ 2832/ 3200]\n",
            "loss: 1.297166  [ 2848/ 3200]\n",
            "loss: 1.262982  [ 2864/ 3200]\n",
            "loss: 1.259302  [ 2880/ 3200]\n",
            "loss: 1.273377  [ 2896/ 3200]\n",
            "loss: 1.297874  [ 2912/ 3200]\n",
            "loss: 1.288817  [ 2928/ 3200]\n",
            "loss: 1.323447  [ 2944/ 3200]\n",
            "loss: 1.267837  [ 2960/ 3200]\n",
            "loss: 1.278835  [ 2976/ 3200]\n",
            "loss: 1.240827  [ 2992/ 3200]\n",
            "loss: 1.326528  [ 3008/ 3200]\n",
            "loss: 1.252946  [ 3024/ 3200]\n",
            "loss: 1.285174  [ 3040/ 3200]\n",
            "loss: 1.330239  [ 3056/ 3200]\n",
            "loss: 1.204531  [ 3072/ 3200]\n",
            "loss: 1.367226  [ 3088/ 3200]\n",
            "loss: 1.272276  [ 3104/ 3200]\n",
            "loss: 1.276850  [ 3120/ 3200]\n",
            "loss: 1.320153  [ 3136/ 3200]\n",
            "loss: 1.255539  [ 3152/ 3200]\n",
            "loss: 1.312645  [ 3168/ 3200]\n",
            "loss: 1.267814  [ 3184/ 3200]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.216768  [    0/ 3200]\n",
            "loss: 1.286750  [   16/ 3200]\n",
            "loss: 1.241196  [   32/ 3200]\n",
            "loss: 1.354612  [   48/ 3200]\n",
            "loss: 1.222192  [   64/ 3200]\n",
            "loss: 1.347746  [   80/ 3200]\n",
            "loss: 1.295918  [   96/ 3200]\n",
            "loss: 1.273755  [  112/ 3200]\n",
            "loss: 1.258450  [  128/ 3200]\n",
            "loss: 1.269694  [  144/ 3200]\n",
            "loss: 1.254101  [  160/ 3200]\n",
            "loss: 1.309124  [  176/ 3200]\n",
            "loss: 1.278537  [  192/ 3200]\n",
            "loss: 1.312027  [  208/ 3200]\n",
            "loss: 1.257693  [  224/ 3200]\n",
            "loss: 1.299924  [  240/ 3200]\n",
            "loss: 1.290001  [  256/ 3200]\n",
            "loss: 1.266291  [  272/ 3200]\n",
            "loss: 1.313904  [  288/ 3200]\n",
            "loss: 1.320855  [  304/ 3200]\n",
            "loss: 1.283342  [  320/ 3200]\n",
            "loss: 1.288049  [  336/ 3200]\n",
            "loss: 1.265357  [  352/ 3200]\n",
            "loss: 1.279694  [  368/ 3200]\n",
            "loss: 1.216116  [  384/ 3200]\n",
            "loss: 1.243475  [  400/ 3200]\n",
            "loss: 1.445101  [  416/ 3200]\n",
            "loss: 1.282275  [  432/ 3200]\n",
            "loss: 1.292523  [  448/ 3200]\n",
            "loss: 1.295343  [  464/ 3200]\n",
            "loss: 1.291334  [  480/ 3200]\n",
            "loss: 1.246677  [  496/ 3200]\n",
            "loss: 1.365727  [  512/ 3200]\n",
            "loss: 1.252103  [  528/ 3200]\n",
            "loss: 1.353644  [  544/ 3200]\n",
            "loss: 1.336079  [  560/ 3200]\n",
            "loss: 1.267495  [  576/ 3200]\n",
            "loss: 1.302757  [  592/ 3200]\n",
            "loss: 1.289447  [  608/ 3200]\n",
            "loss: 1.295131  [  624/ 3200]\n",
            "loss: 1.246546  [  640/ 3200]\n",
            "loss: 1.274564  [  656/ 3200]\n",
            "loss: 1.230096  [  672/ 3200]\n",
            "loss: 1.418322  [  688/ 3200]\n",
            "loss: 1.213596  [  704/ 3200]\n",
            "loss: 1.339995  [  720/ 3200]\n",
            "loss: 1.254155  [  736/ 3200]\n",
            "loss: 1.230963  [  752/ 3200]\n",
            "loss: 1.320835  [  768/ 3200]\n",
            "loss: 1.266891  [  784/ 3200]\n",
            "loss: 1.294190  [  800/ 3200]\n",
            "loss: 1.296340  [  816/ 3200]\n",
            "loss: 1.286704  [  832/ 3200]\n",
            "loss: 1.254797  [  848/ 3200]\n",
            "loss: 1.268059  [  864/ 3200]\n",
            "loss: 1.277887  [  880/ 3200]\n",
            "loss: 1.230832  [  896/ 3200]\n",
            "loss: 1.313300  [  912/ 3200]\n",
            "loss: 1.346458  [  928/ 3200]\n",
            "loss: 1.244398  [  944/ 3200]\n",
            "loss: 1.274515  [  960/ 3200]\n",
            "loss: 1.383454  [  976/ 3200]\n",
            "loss: 1.313040  [  992/ 3200]\n",
            "loss: 1.248935  [ 1008/ 3200]\n",
            "loss: 1.340412  [ 1024/ 3200]\n",
            "loss: 1.236878  [ 1040/ 3200]\n",
            "loss: 1.321523  [ 1056/ 3200]\n",
            "loss: 1.226830  [ 1072/ 3200]\n",
            "loss: 1.294626  [ 1088/ 3200]\n",
            "loss: 1.321720  [ 1104/ 3200]\n",
            "loss: 1.261234  [ 1120/ 3200]\n",
            "loss: 1.292173  [ 1136/ 3200]\n",
            "loss: 1.277921  [ 1152/ 3200]\n",
            "loss: 1.264992  [ 1168/ 3200]\n",
            "loss: 1.308989  [ 1184/ 3200]\n",
            "loss: 1.316991  [ 1200/ 3200]\n",
            "loss: 1.267009  [ 1216/ 3200]\n",
            "loss: 1.244640  [ 1232/ 3200]\n",
            "loss: 1.384761  [ 1248/ 3200]\n",
            "loss: 1.307956  [ 1264/ 3200]\n",
            "loss: 1.244126  [ 1280/ 3200]\n",
            "loss: 1.306046  [ 1296/ 3200]\n",
            "loss: 1.351493  [ 1312/ 3200]\n",
            "loss: 1.248180  [ 1328/ 3200]\n",
            "loss: 1.252245  [ 1344/ 3200]\n",
            "loss: 1.154368  [ 1360/ 3200]\n",
            "loss: 1.193269  [ 1376/ 3200]\n",
            "loss: 1.257374  [ 1392/ 3200]\n",
            "loss: 1.203209  [ 1408/ 3200]\n",
            "loss: 1.350354  [ 1424/ 3200]\n",
            "loss: 1.248398  [ 1440/ 3200]\n",
            "loss: 1.217377  [ 1456/ 3200]\n",
            "loss: 1.227120  [ 1472/ 3200]\n",
            "loss: 1.363641  [ 1488/ 3200]\n",
            "loss: 1.144492  [ 1504/ 3200]\n",
            "loss: 1.188203  [ 1520/ 3200]\n",
            "loss: 1.362655  [ 1536/ 3200]\n",
            "loss: 1.317401  [ 1552/ 3200]\n",
            "loss: 1.263565  [ 1568/ 3200]\n",
            "loss: 1.271904  [ 1584/ 3200]\n",
            "loss: 1.269352  [ 1600/ 3200]\n",
            "loss: 1.319425  [ 1616/ 3200]\n",
            "loss: 1.260568  [ 1632/ 3200]\n",
            "loss: 1.227139  [ 1648/ 3200]\n",
            "loss: 1.361630  [ 1664/ 3200]\n",
            "loss: 1.236768  [ 1680/ 3200]\n",
            "loss: 1.269096  [ 1696/ 3200]\n",
            "loss: 1.326392  [ 1712/ 3200]\n",
            "loss: 1.337063  [ 1728/ 3200]\n",
            "loss: 1.309791  [ 1744/ 3200]\n",
            "loss: 1.300379  [ 1760/ 3200]\n",
            "loss: 1.234981  [ 1776/ 3200]\n",
            "loss: 1.249144  [ 1792/ 3200]\n",
            "loss: 1.313668  [ 1808/ 3200]\n",
            "loss: 1.246915  [ 1824/ 3200]\n",
            "loss: 1.266953  [ 1840/ 3200]\n",
            "loss: 1.257414  [ 1856/ 3200]\n",
            "loss: 1.229943  [ 1872/ 3200]\n",
            "loss: 1.257718  [ 1888/ 3200]\n",
            "loss: 1.222871  [ 1904/ 3200]\n",
            "loss: 1.285718  [ 1920/ 3200]\n",
            "loss: 1.244220  [ 1936/ 3200]\n",
            "loss: 1.279772  [ 1952/ 3200]\n",
            "loss: 1.246481  [ 1968/ 3200]\n",
            "loss: 1.326348  [ 1984/ 3200]\n",
            "loss: 1.305153  [ 2000/ 3200]\n",
            "loss: 1.285726  [ 2016/ 3200]\n",
            "loss: 1.258675  [ 2032/ 3200]\n",
            "loss: 1.273948  [ 2048/ 3200]\n",
            "loss: 1.245029  [ 2064/ 3200]\n",
            "loss: 1.343252  [ 2080/ 3200]\n",
            "loss: 1.191710  [ 2096/ 3200]\n",
            "loss: 1.255399  [ 2112/ 3200]\n",
            "loss: 1.274639  [ 2128/ 3200]\n",
            "loss: 1.279534  [ 2144/ 3200]\n",
            "loss: 1.287590  [ 2160/ 3200]\n",
            "loss: 1.302944  [ 2176/ 3200]\n",
            "loss: 1.328477  [ 2192/ 3200]\n",
            "loss: 1.265502  [ 2208/ 3200]\n",
            "loss: 1.224156  [ 2224/ 3200]\n",
            "loss: 1.215456  [ 2240/ 3200]\n",
            "loss: 1.340662  [ 2256/ 3200]\n",
            "loss: 1.301296  [ 2272/ 3200]\n",
            "loss: 1.281602  [ 2288/ 3200]\n",
            "loss: 1.301608  [ 2304/ 3200]\n",
            "loss: 1.272229  [ 2320/ 3200]\n",
            "loss: 1.240957  [ 2336/ 3200]\n",
            "loss: 1.225984  [ 2352/ 3200]\n",
            "loss: 1.242488  [ 2368/ 3200]\n",
            "loss: 1.307527  [ 2384/ 3200]\n",
            "loss: 1.226607  [ 2400/ 3200]\n",
            "loss: 1.235788  [ 2416/ 3200]\n",
            "loss: 1.253237  [ 2432/ 3200]\n",
            "loss: 1.322744  [ 2448/ 3200]\n",
            "loss: 1.261912  [ 2464/ 3200]\n",
            "loss: 1.200860  [ 2480/ 3200]\n",
            "loss: 1.245436  [ 2496/ 3200]\n",
            "loss: 1.237892  [ 2512/ 3200]\n",
            "loss: 1.227744  [ 2528/ 3200]\n",
            "loss: 1.238151  [ 2544/ 3200]\n",
            "loss: 1.293290  [ 2560/ 3200]\n",
            "loss: 1.294458  [ 2576/ 3200]\n",
            "loss: 1.308142  [ 2592/ 3200]\n",
            "loss: 1.313131  [ 2608/ 3200]\n",
            "loss: 1.259979  [ 2624/ 3200]\n",
            "loss: 1.253530  [ 2640/ 3200]\n",
            "loss: 1.316905  [ 2656/ 3200]\n",
            "loss: 1.268271  [ 2672/ 3200]\n",
            "loss: 1.260196  [ 2688/ 3200]\n",
            "loss: 1.267368  [ 2704/ 3200]\n",
            "loss: 1.313762  [ 2720/ 3200]\n",
            "loss: 1.229520  [ 2736/ 3200]\n",
            "loss: 1.199091  [ 2752/ 3200]\n",
            "loss: 1.265404  [ 2768/ 3200]\n",
            "loss: 1.239180  [ 2784/ 3200]\n",
            "loss: 1.282451  [ 2800/ 3200]\n",
            "loss: 1.248006  [ 2816/ 3200]\n",
            "loss: 1.323482  [ 2832/ 3200]\n",
            "loss: 1.179263  [ 2848/ 3200]\n",
            "loss: 1.278227  [ 2864/ 3200]\n",
            "loss: 1.225018  [ 2880/ 3200]\n",
            "loss: 1.278347  [ 2896/ 3200]\n",
            "loss: 1.204649  [ 2912/ 3200]\n",
            "loss: 1.404058  [ 2928/ 3200]\n",
            "loss: 1.237514  [ 2944/ 3200]\n",
            "loss: 1.210858  [ 2960/ 3200]\n",
            "loss: 1.252034  [ 2976/ 3200]\n",
            "loss: 1.150632  [ 2992/ 3200]\n",
            "loss: 1.333715  [ 3008/ 3200]\n",
            "loss: 1.283100  [ 3024/ 3200]\n",
            "loss: 1.186981  [ 3040/ 3200]\n",
            "loss: 1.261094  [ 3056/ 3200]\n",
            "loss: 1.290787  [ 3072/ 3200]\n",
            "loss: 1.304278  [ 3088/ 3200]\n",
            "loss: 1.316271  [ 3104/ 3200]\n",
            "loss: 1.239890  [ 3120/ 3200]\n",
            "loss: 1.289363  [ 3136/ 3200]\n",
            "loss: 1.227446  [ 3152/ 3200]\n",
            "loss: 1.227381  [ 3168/ 3200]\n",
            "loss: 1.230762  [ 3184/ 3200]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.333096  [    0/ 3200]\n",
            "loss: 1.268815  [   16/ 3200]\n",
            "loss: 1.191432  [   32/ 3200]\n",
            "loss: 1.245367  [   48/ 3200]\n",
            "loss: 1.243940  [   64/ 3200]\n",
            "loss: 1.272474  [   80/ 3200]\n",
            "loss: 1.210259  [   96/ 3200]\n",
            "loss: 1.319244  [  112/ 3200]\n",
            "loss: 1.319448  [  128/ 3200]\n",
            "loss: 1.237195  [  144/ 3200]\n",
            "loss: 1.358945  [  160/ 3200]\n",
            "loss: 1.285689  [  176/ 3200]\n",
            "loss: 1.302060  [  192/ 3200]\n",
            "loss: 1.231964  [  208/ 3200]\n",
            "loss: 1.231963  [  224/ 3200]\n",
            "loss: 1.301194  [  240/ 3200]\n",
            "loss: 1.326767  [  256/ 3200]\n",
            "loss: 1.264775  [  272/ 3200]\n",
            "loss: 1.224535  [  288/ 3200]\n",
            "loss: 1.317810  [  304/ 3200]\n",
            "loss: 1.271694  [  320/ 3200]\n",
            "loss: 1.283324  [  336/ 3200]\n",
            "loss: 1.218364  [  352/ 3200]\n",
            "loss: 1.305295  [  368/ 3200]\n",
            "loss: 1.285758  [  384/ 3200]\n",
            "loss: 1.292596  [  400/ 3200]\n",
            "loss: 1.225213  [  416/ 3200]\n",
            "loss: 1.233516  [  432/ 3200]\n",
            "loss: 1.216961  [  448/ 3200]\n",
            "loss: 1.241699  [  464/ 3200]\n",
            "loss: 1.366007  [  480/ 3200]\n",
            "loss: 1.223830  [  496/ 3200]\n",
            "loss: 1.244093  [  512/ 3200]\n",
            "loss: 1.298483  [  528/ 3200]\n",
            "loss: 1.302283  [  544/ 3200]\n",
            "loss: 1.293211  [  560/ 3200]\n",
            "loss: 1.221105  [  576/ 3200]\n",
            "loss: 1.173489  [  592/ 3200]\n",
            "loss: 1.262754  [  608/ 3200]\n",
            "loss: 1.257268  [  624/ 3200]\n",
            "loss: 1.202746  [  640/ 3200]\n",
            "loss: 1.212917  [  656/ 3200]\n",
            "loss: 1.223473  [  672/ 3200]\n",
            "loss: 1.304758  [  688/ 3200]\n",
            "loss: 1.278330  [  704/ 3200]\n",
            "loss: 1.323381  [  720/ 3200]\n",
            "loss: 1.307887  [  736/ 3200]\n",
            "loss: 1.202214  [  752/ 3200]\n",
            "loss: 1.253367  [  768/ 3200]\n",
            "loss: 1.254307  [  784/ 3200]\n",
            "loss: 1.289276  [  800/ 3200]\n",
            "loss: 1.333638  [  816/ 3200]\n",
            "loss: 1.102729  [  832/ 3200]\n",
            "loss: 1.272142  [  848/ 3200]\n",
            "loss: 1.224435  [  864/ 3200]\n",
            "loss: 1.294168  [  880/ 3200]\n",
            "loss: 1.282475  [  896/ 3200]\n",
            "loss: 1.205068  [  912/ 3200]\n",
            "loss: 1.198036  [  928/ 3200]\n",
            "loss: 1.281334  [  944/ 3200]\n",
            "loss: 1.252343  [  960/ 3200]\n",
            "loss: 1.357447  [  976/ 3200]\n",
            "loss: 1.202264  [  992/ 3200]\n",
            "loss: 1.240801  [ 1008/ 3200]\n",
            "loss: 1.295269  [ 1024/ 3200]\n",
            "loss: 1.246178  [ 1040/ 3200]\n",
            "loss: 1.253276  [ 1056/ 3200]\n",
            "loss: 1.188942  [ 1072/ 3200]\n",
            "loss: 1.272841  [ 1088/ 3200]\n",
            "loss: 1.297527  [ 1104/ 3200]\n",
            "loss: 1.249505  [ 1120/ 3200]\n",
            "loss: 1.294564  [ 1136/ 3200]\n",
            "loss: 1.163961  [ 1152/ 3200]\n",
            "loss: 1.136930  [ 1168/ 3200]\n",
            "loss: 1.315317  [ 1184/ 3200]\n",
            "loss: 1.302875  [ 1200/ 3200]\n",
            "loss: 1.354806  [ 1216/ 3200]\n",
            "loss: 1.316110  [ 1232/ 3200]\n",
            "loss: 1.360246  [ 1248/ 3200]\n",
            "loss: 1.279276  [ 1264/ 3200]\n",
            "loss: 1.249179  [ 1280/ 3200]\n",
            "loss: 1.230764  [ 1296/ 3200]\n",
            "loss: 1.185114  [ 1312/ 3200]\n",
            "loss: 1.314268  [ 1328/ 3200]\n",
            "loss: 1.200711  [ 1344/ 3200]\n",
            "loss: 1.155377  [ 1360/ 3200]\n",
            "loss: 1.221677  [ 1376/ 3200]\n",
            "loss: 1.216226  [ 1392/ 3200]\n",
            "loss: 1.296995  [ 1408/ 3200]\n",
            "loss: 1.210130  [ 1424/ 3200]\n",
            "loss: 1.216320  [ 1440/ 3200]\n",
            "loss: 1.249768  [ 1456/ 3200]\n",
            "loss: 1.269658  [ 1472/ 3200]\n",
            "loss: 1.253461  [ 1488/ 3200]\n",
            "loss: 1.306401  [ 1504/ 3200]\n",
            "loss: 1.271582  [ 1520/ 3200]\n",
            "loss: 1.274225  [ 1536/ 3200]\n",
            "loss: 1.225974  [ 1552/ 3200]\n",
            "loss: 1.199820  [ 1568/ 3200]\n",
            "loss: 1.237606  [ 1584/ 3200]\n",
            "loss: 1.284239  [ 1600/ 3200]\n",
            "loss: 1.167420  [ 1616/ 3200]\n",
            "loss: 1.213306  [ 1632/ 3200]\n",
            "loss: 1.170554  [ 1648/ 3200]\n",
            "loss: 1.311606  [ 1664/ 3200]\n",
            "loss: 1.340903  [ 1680/ 3200]\n",
            "loss: 1.295496  [ 1696/ 3200]\n",
            "loss: 1.289623  [ 1712/ 3200]\n",
            "loss: 1.239011  [ 1728/ 3200]\n",
            "loss: 1.324714  [ 1744/ 3200]\n",
            "loss: 1.196595  [ 1760/ 3200]\n",
            "loss: 1.319664  [ 1776/ 3200]\n",
            "loss: 1.213558  [ 1792/ 3200]\n",
            "loss: 1.174397  [ 1808/ 3200]\n",
            "loss: 1.180940  [ 1824/ 3200]\n",
            "loss: 1.270270  [ 1840/ 3200]\n",
            "loss: 1.199723  [ 1856/ 3200]\n",
            "loss: 1.286734  [ 1872/ 3200]\n",
            "loss: 1.259194  [ 1888/ 3200]\n",
            "loss: 1.214823  [ 1904/ 3200]\n",
            "loss: 1.228366  [ 1920/ 3200]\n",
            "loss: 1.275237  [ 1936/ 3200]\n",
            "loss: 1.248169  [ 1952/ 3200]\n",
            "loss: 1.304796  [ 1968/ 3200]\n",
            "loss: 1.361442  [ 1984/ 3200]\n",
            "loss: 1.264923  [ 2000/ 3200]\n",
            "loss: 1.248603  [ 2016/ 3200]\n",
            "loss: 1.333049  [ 2032/ 3200]\n",
            "loss: 1.223783  [ 2048/ 3200]\n",
            "loss: 1.274976  [ 2064/ 3200]\n",
            "loss: 1.222762  [ 2080/ 3200]\n",
            "loss: 1.274594  [ 2096/ 3200]\n",
            "loss: 1.239078  [ 2112/ 3200]\n",
            "loss: 1.196713  [ 2128/ 3200]\n",
            "loss: 1.230394  [ 2144/ 3200]\n",
            "loss: 1.280739  [ 2160/ 3200]\n",
            "loss: 1.240315  [ 2176/ 3200]\n",
            "loss: 1.262936  [ 2192/ 3200]\n",
            "loss: 1.264285  [ 2208/ 3200]\n",
            "loss: 1.238968  [ 2224/ 3200]\n",
            "loss: 1.265669  [ 2240/ 3200]\n",
            "loss: 1.243733  [ 2256/ 3200]\n",
            "loss: 1.221325  [ 2272/ 3200]\n",
            "loss: 1.238518  [ 2288/ 3200]\n",
            "loss: 1.355054  [ 2304/ 3200]\n",
            "loss: 1.308713  [ 2320/ 3200]\n",
            "loss: 1.321920  [ 2336/ 3200]\n",
            "loss: 1.231489  [ 2352/ 3200]\n",
            "loss: 1.241611  [ 2368/ 3200]\n",
            "loss: 1.255055  [ 2384/ 3200]\n",
            "loss: 1.273886  [ 2400/ 3200]\n",
            "loss: 1.229564  [ 2416/ 3200]\n",
            "loss: 1.253159  [ 2432/ 3200]\n",
            "loss: 1.233666  [ 2448/ 3200]\n",
            "loss: 1.247929  [ 2464/ 3200]\n",
            "loss: 1.240195  [ 2480/ 3200]\n",
            "loss: 1.162801  [ 2496/ 3200]\n",
            "loss: 1.194732  [ 2512/ 3200]\n",
            "loss: 1.221993  [ 2528/ 3200]\n",
            "loss: 1.225236  [ 2544/ 3200]\n",
            "loss: 1.298351  [ 2560/ 3200]\n",
            "loss: 1.246498  [ 2576/ 3200]\n",
            "loss: 1.306309  [ 2592/ 3200]\n",
            "loss: 1.231906  [ 2608/ 3200]\n",
            "loss: 1.344401  [ 2624/ 3200]\n",
            "loss: 1.286773  [ 2640/ 3200]\n",
            "loss: 1.190082  [ 2656/ 3200]\n",
            "loss: 1.265205  [ 2672/ 3200]\n",
            "loss: 1.293863  [ 2688/ 3200]\n",
            "loss: 1.286561  [ 2704/ 3200]\n",
            "loss: 1.319343  [ 2720/ 3200]\n",
            "loss: 1.227905  [ 2736/ 3200]\n",
            "loss: 1.236739  [ 2752/ 3200]\n",
            "loss: 1.243007  [ 2768/ 3200]\n",
            "loss: 1.318465  [ 2784/ 3200]\n",
            "loss: 1.325551  [ 2800/ 3200]\n",
            "loss: 1.222836  [ 2816/ 3200]\n",
            "loss: 1.276346  [ 2832/ 3200]\n",
            "loss: 1.222692  [ 2848/ 3200]\n",
            "loss: 1.217568  [ 2864/ 3200]\n",
            "loss: 1.278401  [ 2880/ 3200]\n",
            "loss: 1.156129  [ 2896/ 3200]\n",
            "loss: 1.253441  [ 2912/ 3200]\n",
            "loss: 1.341781  [ 2928/ 3200]\n",
            "loss: 1.222768  [ 2944/ 3200]\n",
            "loss: 1.218375  [ 2960/ 3200]\n",
            "loss: 1.158051  [ 2976/ 3200]\n",
            "loss: 1.193183  [ 2992/ 3200]\n",
            "loss: 1.195995  [ 3008/ 3200]\n",
            "loss: 1.284762  [ 3024/ 3200]\n",
            "loss: 1.238137  [ 3040/ 3200]\n",
            "loss: 1.178552  [ 3056/ 3200]\n",
            "loss: 1.231056  [ 3072/ 3200]\n",
            "loss: 1.170409  [ 3088/ 3200]\n",
            "loss: 1.219652  [ 3104/ 3200]\n",
            "loss: 1.219813  [ 3120/ 3200]\n",
            "loss: 1.234861  [ 3136/ 3200]\n",
            "loss: 1.158375  [ 3152/ 3200]\n",
            "loss: 1.146168  [ 3168/ 3200]\n",
            "loss: 1.259829  [ 3184/ 3200]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.263708  [    0/ 3200]\n",
            "loss: 1.191701  [   16/ 3200]\n",
            "loss: 1.234585  [   32/ 3200]\n",
            "loss: 1.239277  [   48/ 3200]\n",
            "loss: 1.350146  [   64/ 3200]\n",
            "loss: 1.322656  [   80/ 3200]\n",
            "loss: 1.251875  [   96/ 3200]\n",
            "loss: 1.155036  [  112/ 3200]\n",
            "loss: 1.283861  [  128/ 3200]\n",
            "loss: 1.130116  [  144/ 3200]\n",
            "loss: 1.348084  [  160/ 3200]\n",
            "loss: 1.337298  [  176/ 3200]\n",
            "loss: 1.261515  [  192/ 3200]\n",
            "loss: 1.342954  [  208/ 3200]\n",
            "loss: 1.232582  [  224/ 3200]\n",
            "loss: 1.243672  [  240/ 3200]\n",
            "loss: 1.330201  [  256/ 3200]\n",
            "loss: 1.365194  [  272/ 3200]\n",
            "loss: 1.253395  [  288/ 3200]\n",
            "loss: 1.310073  [  304/ 3200]\n",
            "loss: 1.224481  [  320/ 3200]\n",
            "loss: 1.189637  [  336/ 3200]\n",
            "loss: 1.248016  [  352/ 3200]\n",
            "loss: 1.196765  [  368/ 3200]\n",
            "loss: 1.174535  [  384/ 3200]\n",
            "loss: 1.195229  [  400/ 3200]\n",
            "loss: 1.277938  [  416/ 3200]\n",
            "loss: 1.309401  [  432/ 3200]\n",
            "loss: 1.279196  [  448/ 3200]\n",
            "loss: 1.237015  [  464/ 3200]\n",
            "loss: 1.211320  [  480/ 3200]\n",
            "loss: 1.219027  [  496/ 3200]\n",
            "loss: 1.245803  [  512/ 3200]\n",
            "loss: 1.310945  [  528/ 3200]\n",
            "loss: 1.194038  [  544/ 3200]\n",
            "loss: 1.166322  [  560/ 3200]\n",
            "loss: 1.221586  [  576/ 3200]\n",
            "loss: 1.297507  [  592/ 3200]\n",
            "loss: 1.209022  [  608/ 3200]\n",
            "loss: 1.222465  [  624/ 3200]\n",
            "loss: 1.300708  [  640/ 3200]\n",
            "loss: 1.262443  [  656/ 3200]\n",
            "loss: 1.185909  [  672/ 3200]\n",
            "loss: 1.154258  [  688/ 3200]\n",
            "loss: 1.249016  [  704/ 3200]\n",
            "loss: 1.229259  [  720/ 3200]\n",
            "loss: 1.292196  [  736/ 3200]\n",
            "loss: 1.242857  [  752/ 3200]\n",
            "loss: 1.159938  [  768/ 3200]\n",
            "loss: 1.247877  [  784/ 3200]\n",
            "loss: 1.291127  [  800/ 3200]\n",
            "loss: 1.227352  [  816/ 3200]\n",
            "loss: 1.147052  [  832/ 3200]\n",
            "loss: 1.285662  [  848/ 3200]\n",
            "loss: 1.165031  [  864/ 3200]\n",
            "loss: 1.277764  [  880/ 3200]\n",
            "loss: 1.174353  [  896/ 3200]\n",
            "loss: 1.235155  [  912/ 3200]\n",
            "loss: 1.308806  [  928/ 3200]\n",
            "loss: 1.276221  [  944/ 3200]\n",
            "loss: 1.229541  [  960/ 3200]\n",
            "loss: 1.256719  [  976/ 3200]\n",
            "loss: 1.246968  [  992/ 3200]\n",
            "loss: 1.243058  [ 1008/ 3200]\n",
            "loss: 1.240027  [ 1024/ 3200]\n",
            "loss: 1.231169  [ 1040/ 3200]\n",
            "loss: 1.132349  [ 1056/ 3200]\n",
            "loss: 1.286694  [ 1072/ 3200]\n",
            "loss: 1.236486  [ 1088/ 3200]\n",
            "loss: 1.289970  [ 1104/ 3200]\n",
            "loss: 1.199940  [ 1120/ 3200]\n",
            "loss: 1.189879  [ 1136/ 3200]\n",
            "loss: 1.202363  [ 1152/ 3200]\n",
            "loss: 1.268589  [ 1168/ 3200]\n",
            "loss: 1.309338  [ 1184/ 3200]\n",
            "loss: 1.241036  [ 1200/ 3200]\n",
            "loss: 1.263480  [ 1216/ 3200]\n",
            "loss: 1.173620  [ 1232/ 3200]\n",
            "loss: 1.174317  [ 1248/ 3200]\n",
            "loss: 1.175699  [ 1264/ 3200]\n",
            "loss: 1.199064  [ 1280/ 3200]\n",
            "loss: 1.247443  [ 1296/ 3200]\n",
            "loss: 1.233773  [ 1312/ 3200]\n",
            "loss: 1.215073  [ 1328/ 3200]\n",
            "loss: 1.219680  [ 1344/ 3200]\n",
            "loss: 1.205162  [ 1360/ 3200]\n",
            "loss: 1.235942  [ 1376/ 3200]\n",
            "loss: 1.248996  [ 1392/ 3200]\n",
            "loss: 1.142707  [ 1408/ 3200]\n",
            "loss: 1.263468  [ 1424/ 3200]\n",
            "loss: 1.212611  [ 1440/ 3200]\n",
            "loss: 1.311710  [ 1456/ 3200]\n",
            "loss: 1.265262  [ 1472/ 3200]\n",
            "loss: 1.275677  [ 1488/ 3200]\n",
            "loss: 1.217998  [ 1504/ 3200]\n",
            "loss: 1.161136  [ 1520/ 3200]\n",
            "loss: 1.242619  [ 1536/ 3200]\n",
            "loss: 1.254692  [ 1552/ 3200]\n",
            "loss: 1.234951  [ 1568/ 3200]\n",
            "loss: 1.262024  [ 1584/ 3200]\n",
            "loss: 1.224068  [ 1600/ 3200]\n",
            "loss: 1.210145  [ 1616/ 3200]\n",
            "loss: 1.112960  [ 1632/ 3200]\n",
            "loss: 1.256716  [ 1648/ 3200]\n",
            "loss: 1.242838  [ 1664/ 3200]\n",
            "loss: 1.393483  [ 1680/ 3200]\n",
            "loss: 1.274513  [ 1696/ 3200]\n",
            "loss: 1.173256  [ 1712/ 3200]\n",
            "loss: 1.223349  [ 1728/ 3200]\n",
            "loss: 1.250427  [ 1744/ 3200]\n",
            "loss: 1.222928  [ 1760/ 3200]\n",
            "loss: 1.158700  [ 1776/ 3200]\n",
            "loss: 1.077109  [ 1792/ 3200]\n",
            "loss: 1.285042  [ 1808/ 3200]\n",
            "loss: 1.209086  [ 1824/ 3200]\n",
            "loss: 1.150309  [ 1840/ 3200]\n",
            "loss: 1.188785  [ 1856/ 3200]\n",
            "loss: 1.177009  [ 1872/ 3200]\n",
            "loss: 1.268839  [ 1888/ 3200]\n",
            "loss: 1.237163  [ 1904/ 3200]\n",
            "loss: 1.217532  [ 1920/ 3200]\n",
            "loss: 1.203746  [ 1936/ 3200]\n",
            "loss: 1.176222  [ 1952/ 3200]\n",
            "loss: 1.264113  [ 1968/ 3200]\n",
            "loss: 1.306783  [ 1984/ 3200]\n",
            "loss: 1.184209  [ 2000/ 3200]\n",
            "loss: 1.169122  [ 2016/ 3200]\n",
            "loss: 1.337116  [ 2032/ 3200]\n",
            "loss: 1.197114  [ 2048/ 3200]\n",
            "loss: 1.251991  [ 2064/ 3200]\n",
            "loss: 1.232319  [ 2080/ 3200]\n",
            "loss: 1.131697  [ 2096/ 3200]\n",
            "loss: 1.220686  [ 2112/ 3200]\n",
            "loss: 1.208714  [ 2128/ 3200]\n",
            "loss: 1.268544  [ 2144/ 3200]\n",
            "loss: 1.088898  [ 2160/ 3200]\n",
            "loss: 1.239415  [ 2176/ 3200]\n",
            "loss: 1.283742  [ 2192/ 3200]\n",
            "loss: 1.352657  [ 2208/ 3200]\n",
            "loss: 1.265201  [ 2224/ 3200]\n",
            "loss: 1.189315  [ 2240/ 3200]\n",
            "loss: 1.298222  [ 2256/ 3200]\n",
            "loss: 1.179922  [ 2272/ 3200]\n",
            "loss: 1.274240  [ 2288/ 3200]\n",
            "loss: 1.142915  [ 2304/ 3200]\n",
            "loss: 1.284546  [ 2320/ 3200]\n",
            "loss: 1.301110  [ 2336/ 3200]\n",
            "loss: 1.273470  [ 2352/ 3200]\n",
            "loss: 1.238047  [ 2368/ 3200]\n",
            "loss: 1.262509  [ 2384/ 3200]\n",
            "loss: 1.215060  [ 2400/ 3200]\n",
            "loss: 1.320473  [ 2416/ 3200]\n",
            "loss: 1.254689  [ 2432/ 3200]\n",
            "loss: 1.240272  [ 2448/ 3200]\n",
            "loss: 1.192093  [ 2464/ 3200]\n",
            "loss: 1.282750  [ 2480/ 3200]\n",
            "loss: 1.197836  [ 2496/ 3200]\n",
            "loss: 1.198208  [ 2512/ 3200]\n",
            "loss: 1.253415  [ 2528/ 3200]\n",
            "loss: 1.340567  [ 2544/ 3200]\n",
            "loss: 1.124553  [ 2560/ 3200]\n",
            "loss: 1.282314  [ 2576/ 3200]\n",
            "loss: 1.261729  [ 2592/ 3200]\n",
            "loss: 1.192206  [ 2608/ 3200]\n",
            "loss: 1.170314  [ 2624/ 3200]\n",
            "loss: 1.306780  [ 2640/ 3200]\n",
            "loss: 1.157516  [ 2656/ 3200]\n",
            "loss: 1.364086  [ 2672/ 3200]\n",
            "loss: 1.231386  [ 2688/ 3200]\n",
            "loss: 1.214909  [ 2704/ 3200]\n",
            "loss: 1.233968  [ 2720/ 3200]\n",
            "loss: 1.264526  [ 2736/ 3200]\n",
            "loss: 1.186471  [ 2752/ 3200]\n",
            "loss: 1.196784  [ 2768/ 3200]\n",
            "loss: 1.349900  [ 2784/ 3200]\n",
            "loss: 1.198621  [ 2800/ 3200]\n",
            "loss: 1.214658  [ 2816/ 3200]\n",
            "loss: 1.148789  [ 2832/ 3200]\n",
            "loss: 1.252638  [ 2848/ 3200]\n",
            "loss: 1.195214  [ 2864/ 3200]\n",
            "loss: 1.351400  [ 2880/ 3200]\n",
            "loss: 1.129622  [ 2896/ 3200]\n",
            "loss: 1.217504  [ 2912/ 3200]\n",
            "loss: 1.330934  [ 2928/ 3200]\n",
            "loss: 1.175032  [ 2944/ 3200]\n",
            "loss: 1.256272  [ 2960/ 3200]\n",
            "loss: 1.213888  [ 2976/ 3200]\n",
            "loss: 1.234618  [ 2992/ 3200]\n",
            "loss: 1.217463  [ 3008/ 3200]\n",
            "loss: 1.215712  [ 3024/ 3200]\n",
            "loss: 1.261914  [ 3040/ 3200]\n",
            "loss: 1.137960  [ 3056/ 3200]\n",
            "loss: 1.182752  [ 3072/ 3200]\n",
            "loss: 1.246059  [ 3088/ 3200]\n",
            "loss: 1.213752  [ 3104/ 3200]\n",
            "loss: 1.213348  [ 3120/ 3200]\n",
            "loss: 1.340177  [ 3136/ 3200]\n",
            "loss: 1.181973  [ 3152/ 3200]\n",
            "loss: 1.323791  [ 3168/ 3200]\n",
            "loss: 1.289057  [ 3184/ 3200]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.260976  [    0/ 3200]\n",
            "loss: 1.234524  [   16/ 3200]\n",
            "loss: 1.250945  [   32/ 3200]\n",
            "loss: 1.115149  [   48/ 3200]\n",
            "loss: 1.179727  [   64/ 3200]\n",
            "loss: 1.130437  [   80/ 3200]\n",
            "loss: 1.219900  [   96/ 3200]\n",
            "loss: 1.171359  [  112/ 3200]\n",
            "loss: 1.151902  [  128/ 3200]\n",
            "loss: 1.295215  [  144/ 3200]\n",
            "loss: 1.214118  [  160/ 3200]\n",
            "loss: 1.222395  [  176/ 3200]\n",
            "loss: 1.293270  [  192/ 3200]\n",
            "loss: 1.186147  [  208/ 3200]\n",
            "loss: 1.185238  [  224/ 3200]\n",
            "loss: 1.265418  [  240/ 3200]\n",
            "loss: 1.226991  [  256/ 3200]\n",
            "loss: 1.231898  [  272/ 3200]\n",
            "loss: 1.210208  [  288/ 3200]\n",
            "loss: 1.176807  [  304/ 3200]\n",
            "loss: 1.367129  [  320/ 3200]\n",
            "loss: 1.273972  [  336/ 3200]\n",
            "loss: 1.167383  [  352/ 3200]\n",
            "loss: 1.244401  [  368/ 3200]\n",
            "loss: 1.198030  [  384/ 3200]\n",
            "loss: 1.209017  [  400/ 3200]\n",
            "loss: 1.142753  [  416/ 3200]\n",
            "loss: 1.330236  [  432/ 3200]\n",
            "loss: 1.201864  [  448/ 3200]\n",
            "loss: 1.151005  [  464/ 3200]\n",
            "loss: 1.192917  [  480/ 3200]\n",
            "loss: 1.208814  [  496/ 3200]\n",
            "loss: 1.221648  [  512/ 3200]\n",
            "loss: 1.318272  [  528/ 3200]\n",
            "loss: 1.197126  [  544/ 3200]\n",
            "loss: 1.204187  [  560/ 3200]\n",
            "loss: 1.286561  [  576/ 3200]\n",
            "loss: 1.274509  [  592/ 3200]\n",
            "loss: 1.332611  [  608/ 3200]\n",
            "loss: 1.230872  [  624/ 3200]\n",
            "loss: 1.205578  [  640/ 3200]\n",
            "loss: 1.238334  [  656/ 3200]\n",
            "loss: 1.165421  [  672/ 3200]\n",
            "loss: 1.156378  [  688/ 3200]\n",
            "loss: 1.346240  [  704/ 3200]\n",
            "loss: 1.184660  [  720/ 3200]\n",
            "loss: 1.193298  [  736/ 3200]\n",
            "loss: 1.178813  [  752/ 3200]\n",
            "loss: 1.252714  [  768/ 3200]\n",
            "loss: 1.192296  [  784/ 3200]\n",
            "loss: 1.236379  [  800/ 3200]\n",
            "loss: 1.323635  [  816/ 3200]\n",
            "loss: 1.269123  [  832/ 3200]\n",
            "loss: 1.236290  [  848/ 3200]\n",
            "loss: 1.159859  [  864/ 3200]\n",
            "loss: 1.238654  [  880/ 3200]\n",
            "loss: 1.184561  [  896/ 3200]\n",
            "loss: 1.168292  [  912/ 3200]\n",
            "loss: 1.310896  [  928/ 3200]\n",
            "loss: 1.180474  [  944/ 3200]\n",
            "loss: 1.299725  [  960/ 3200]\n",
            "loss: 1.139551  [  976/ 3200]\n",
            "loss: 1.089961  [  992/ 3200]\n",
            "loss: 1.215004  [ 1008/ 3200]\n",
            "loss: 1.311938  [ 1024/ 3200]\n",
            "loss: 1.181583  [ 1040/ 3200]\n",
            "loss: 1.144053  [ 1056/ 3200]\n",
            "loss: 1.273988  [ 1072/ 3200]\n",
            "loss: 1.117146  [ 1088/ 3200]\n",
            "loss: 1.228504  [ 1104/ 3200]\n",
            "loss: 1.246540  [ 1120/ 3200]\n",
            "loss: 1.184648  [ 1136/ 3200]\n",
            "loss: 1.206398  [ 1152/ 3200]\n",
            "loss: 1.279505  [ 1168/ 3200]\n",
            "loss: 1.136324  [ 1184/ 3200]\n",
            "loss: 1.217150  [ 1200/ 3200]\n",
            "loss: 1.174360  [ 1216/ 3200]\n",
            "loss: 1.212936  [ 1232/ 3200]\n",
            "loss: 1.270375  [ 1248/ 3200]\n",
            "loss: 1.175339  [ 1264/ 3200]\n",
            "loss: 1.218129  [ 1280/ 3200]\n",
            "loss: 1.269187  [ 1296/ 3200]\n",
            "loss: 1.116510  [ 1312/ 3200]\n",
            "loss: 1.371658  [ 1328/ 3200]\n",
            "loss: 1.160731  [ 1344/ 3200]\n",
            "loss: 1.140419  [ 1360/ 3200]\n",
            "loss: 1.183935  [ 1376/ 3200]\n",
            "loss: 1.298144  [ 1392/ 3200]\n",
            "loss: 1.231185  [ 1408/ 3200]\n",
            "loss: 1.233204  [ 1424/ 3200]\n",
            "loss: 1.159224  [ 1440/ 3200]\n",
            "loss: 1.190618  [ 1456/ 3200]\n",
            "loss: 1.177076  [ 1472/ 3200]\n",
            "loss: 1.197527  [ 1488/ 3200]\n",
            "loss: 1.239905  [ 1504/ 3200]\n",
            "loss: 1.255010  [ 1520/ 3200]\n",
            "loss: 1.215707  [ 1536/ 3200]\n",
            "loss: 1.257636  [ 1552/ 3200]\n",
            "loss: 1.232409  [ 1568/ 3200]\n",
            "loss: 1.258025  [ 1584/ 3200]\n",
            "loss: 1.251162  [ 1600/ 3200]\n",
            "loss: 1.261738  [ 1616/ 3200]\n",
            "loss: 1.175331  [ 1632/ 3200]\n",
            "loss: 1.200857  [ 1648/ 3200]\n",
            "loss: 1.212088  [ 1664/ 3200]\n",
            "loss: 1.098546  [ 1680/ 3200]\n",
            "loss: 1.150204  [ 1696/ 3200]\n",
            "loss: 1.198503  [ 1712/ 3200]\n",
            "loss: 1.131728  [ 1728/ 3200]\n",
            "loss: 1.213183  [ 1744/ 3200]\n",
            "loss: 1.207922  [ 1760/ 3200]\n",
            "loss: 1.162292  [ 1776/ 3200]\n",
            "loss: 1.275386  [ 1792/ 3200]\n",
            "loss: 1.209057  [ 1808/ 3200]\n",
            "loss: 1.161114  [ 1824/ 3200]\n",
            "loss: 1.219393  [ 1840/ 3200]\n",
            "loss: 1.157196  [ 1856/ 3200]\n",
            "loss: 1.299577  [ 1872/ 3200]\n",
            "loss: 1.202614  [ 1888/ 3200]\n",
            "loss: 1.201495  [ 1904/ 3200]\n",
            "loss: 1.301102  [ 1920/ 3200]\n",
            "loss: 1.215353  [ 1936/ 3200]\n",
            "loss: 1.318691  [ 1952/ 3200]\n",
            "loss: 1.130277  [ 1968/ 3200]\n",
            "loss: 1.177523  [ 1984/ 3200]\n",
            "loss: 1.207558  [ 2000/ 3200]\n",
            "loss: 1.298558  [ 2016/ 3200]\n",
            "loss: 1.233958  [ 2032/ 3200]\n",
            "loss: 1.158279  [ 2048/ 3200]\n",
            "loss: 1.241385  [ 2064/ 3200]\n",
            "loss: 1.204248  [ 2080/ 3200]\n",
            "loss: 1.207291  [ 2096/ 3200]\n",
            "loss: 1.174198  [ 2112/ 3200]\n",
            "loss: 1.193637  [ 2128/ 3200]\n",
            "loss: 1.188690  [ 2144/ 3200]\n",
            "loss: 1.148911  [ 2160/ 3200]\n",
            "loss: 1.263521  [ 2176/ 3200]\n",
            "loss: 1.178919  [ 2192/ 3200]\n",
            "loss: 1.149354  [ 2208/ 3200]\n",
            "loss: 1.213690  [ 2224/ 3200]\n",
            "loss: 1.254892  [ 2240/ 3200]\n",
            "loss: 1.208775  [ 2256/ 3200]\n",
            "loss: 1.162253  [ 2272/ 3200]\n",
            "loss: 1.274937  [ 2288/ 3200]\n",
            "loss: 1.204178  [ 2304/ 3200]\n",
            "loss: 1.096129  [ 2320/ 3200]\n",
            "loss: 1.251085  [ 2336/ 3200]\n",
            "loss: 1.251071  [ 2352/ 3200]\n",
            "loss: 1.135456  [ 2368/ 3200]\n",
            "loss: 1.307152  [ 2384/ 3200]\n",
            "loss: 1.092298  [ 2400/ 3200]\n",
            "loss: 1.278746  [ 2416/ 3200]\n",
            "loss: 1.250560  [ 2432/ 3200]\n",
            "loss: 1.207844  [ 2448/ 3200]\n",
            "loss: 1.121083  [ 2464/ 3200]\n",
            "loss: 1.135189  [ 2480/ 3200]\n",
            "loss: 1.200458  [ 2496/ 3200]\n",
            "loss: 1.327229  [ 2512/ 3200]\n",
            "loss: 1.144646  [ 2528/ 3200]\n",
            "loss: 1.173688  [ 2544/ 3200]\n",
            "loss: 1.186107  [ 2560/ 3200]\n",
            "loss: 1.290702  [ 2576/ 3200]\n",
            "loss: 1.262006  [ 2592/ 3200]\n",
            "loss: 1.195731  [ 2608/ 3200]\n",
            "loss: 1.275097  [ 2624/ 3200]\n",
            "loss: 1.185894  [ 2640/ 3200]\n",
            "loss: 1.192158  [ 2656/ 3200]\n",
            "loss: 1.190594  [ 2672/ 3200]\n",
            "loss: 1.243040  [ 2688/ 3200]\n",
            "loss: 1.204547  [ 2704/ 3200]\n",
            "loss: 1.185119  [ 2720/ 3200]\n",
            "loss: 1.305615  [ 2736/ 3200]\n",
            "loss: 1.210173  [ 2752/ 3200]\n",
            "loss: 1.213495  [ 2768/ 3200]\n",
            "loss: 1.190581  [ 2784/ 3200]\n",
            "loss: 1.182116  [ 2800/ 3200]\n",
            "loss: 1.239669  [ 2816/ 3200]\n",
            "loss: 1.192331  [ 2832/ 3200]\n",
            "loss: 1.156951  [ 2848/ 3200]\n",
            "loss: 1.172111  [ 2864/ 3200]\n",
            "loss: 1.166480  [ 2880/ 3200]\n",
            "loss: 1.298488  [ 2896/ 3200]\n",
            "loss: 1.220635  [ 2912/ 3200]\n",
            "loss: 1.190198  [ 2928/ 3200]\n",
            "loss: 1.226918  [ 2944/ 3200]\n",
            "loss: 1.251651  [ 2960/ 3200]\n",
            "loss: 1.130701  [ 2976/ 3200]\n",
            "loss: 1.218826  [ 2992/ 3200]\n",
            "loss: 1.225261  [ 3008/ 3200]\n",
            "loss: 1.144251  [ 3024/ 3200]\n",
            "loss: 1.246845  [ 3040/ 3200]\n",
            "loss: 1.214037  [ 3056/ 3200]\n",
            "loss: 1.270966  [ 3072/ 3200]\n",
            "loss: 1.279242  [ 3088/ 3200]\n",
            "loss: 1.208759  [ 3104/ 3200]\n",
            "loss: 1.182871  [ 3120/ 3200]\n",
            "loss: 1.160207  [ 3136/ 3200]\n",
            "loss: 1.116887  [ 3152/ 3200]\n",
            "loss: 1.244307  [ 3168/ 3200]\n",
            "loss: 1.167509  [ 3184/ 3200]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.187988  [    0/ 3200]\n",
            "loss: 1.262011  [   16/ 3200]\n",
            "loss: 1.131664  [   32/ 3200]\n",
            "loss: 1.258119  [   48/ 3200]\n",
            "loss: 1.315867  [   64/ 3200]\n",
            "loss: 1.278390  [   80/ 3200]\n",
            "loss: 1.247889  [   96/ 3200]\n",
            "loss: 1.135092  [  112/ 3200]\n",
            "loss: 1.216841  [  128/ 3200]\n",
            "loss: 1.109645  [  144/ 3200]\n",
            "loss: 1.193875  [  160/ 3200]\n",
            "loss: 1.162223  [  176/ 3200]\n",
            "loss: 1.230244  [  192/ 3200]\n",
            "loss: 1.185883  [  208/ 3200]\n",
            "loss: 1.217005  [  224/ 3200]\n",
            "loss: 1.174846  [  240/ 3200]\n",
            "loss: 1.209401  [  256/ 3200]\n",
            "loss: 1.147561  [  272/ 3200]\n",
            "loss: 1.236992  [  288/ 3200]\n",
            "loss: 1.194774  [  304/ 3200]\n",
            "loss: 1.309883  [  320/ 3200]\n",
            "loss: 1.254093  [  336/ 3200]\n",
            "loss: 1.258763  [  352/ 3200]\n",
            "loss: 1.243221  [  368/ 3200]\n",
            "loss: 1.138332  [  384/ 3200]\n",
            "loss: 1.230116  [  400/ 3200]\n",
            "loss: 1.177680  [  416/ 3200]\n",
            "loss: 1.220173  [  432/ 3200]\n",
            "loss: 1.218618  [  448/ 3200]\n",
            "loss: 1.198678  [  464/ 3200]\n",
            "loss: 1.231516  [  480/ 3200]\n",
            "loss: 1.131533  [  496/ 3200]\n",
            "loss: 1.199461  [  512/ 3200]\n",
            "loss: 1.144778  [  528/ 3200]\n",
            "loss: 1.092749  [  544/ 3200]\n",
            "loss: 1.107873  [  560/ 3200]\n",
            "loss: 1.220128  [  576/ 3200]\n",
            "loss: 1.090515  [  592/ 3200]\n",
            "loss: 1.122413  [  608/ 3200]\n",
            "loss: 1.209431  [  624/ 3200]\n",
            "loss: 1.247350  [  640/ 3200]\n",
            "loss: 1.193458  [  656/ 3200]\n",
            "loss: 1.140515  [  672/ 3200]\n",
            "loss: 1.099466  [  688/ 3200]\n",
            "loss: 1.255095  [  704/ 3200]\n",
            "loss: 1.199713  [  720/ 3200]\n",
            "loss: 1.270971  [  736/ 3200]\n",
            "loss: 1.222928  [  752/ 3200]\n",
            "loss: 1.121081  [  768/ 3200]\n",
            "loss: 1.198906  [  784/ 3200]\n",
            "loss: 1.219838  [  800/ 3200]\n",
            "loss: 1.080162  [  816/ 3200]\n",
            "loss: 1.156960  [  832/ 3200]\n",
            "loss: 1.237850  [  848/ 3200]\n",
            "loss: 1.296082  [  864/ 3200]\n",
            "loss: 1.153680  [  880/ 3200]\n",
            "loss: 1.144968  [  896/ 3200]\n",
            "loss: 1.117648  [  912/ 3200]\n",
            "loss: 1.446796  [  928/ 3200]\n",
            "loss: 1.465958  [  944/ 3200]\n",
            "loss: 1.189602  [  960/ 3200]\n",
            "loss: 1.208086  [  976/ 3200]\n",
            "loss: 1.217901  [  992/ 3200]\n",
            "loss: 1.217478  [ 1008/ 3200]\n",
            "loss: 1.210053  [ 1024/ 3200]\n",
            "loss: 1.184165  [ 1040/ 3200]\n",
            "loss: 1.130442  [ 1056/ 3200]\n",
            "loss: 1.206440  [ 1072/ 3200]\n",
            "loss: 1.150686  [ 1088/ 3200]\n",
            "loss: 1.144825  [ 1104/ 3200]\n",
            "loss: 1.139946  [ 1120/ 3200]\n",
            "loss: 1.305141  [ 1136/ 3200]\n",
            "loss: 1.183796  [ 1152/ 3200]\n",
            "loss: 1.204851  [ 1168/ 3200]\n",
            "loss: 1.245159  [ 1184/ 3200]\n",
            "loss: 1.336904  [ 1200/ 3200]\n",
            "loss: 1.218330  [ 1216/ 3200]\n",
            "loss: 1.238140  [ 1232/ 3200]\n",
            "loss: 1.107755  [ 1248/ 3200]\n",
            "loss: 1.093653  [ 1264/ 3200]\n",
            "loss: 1.196473  [ 1280/ 3200]\n",
            "loss: 1.095284  [ 1296/ 3200]\n",
            "loss: 1.237820  [ 1312/ 3200]\n",
            "loss: 1.234297  [ 1328/ 3200]\n",
            "loss: 1.267074  [ 1344/ 3200]\n",
            "loss: 1.214945  [ 1360/ 3200]\n",
            "loss: 1.130701  [ 1376/ 3200]\n",
            "loss: 1.182441  [ 1392/ 3200]\n",
            "loss: 1.135393  [ 1408/ 3200]\n",
            "loss: 1.214882  [ 1424/ 3200]\n",
            "loss: 1.169595  [ 1440/ 3200]\n",
            "loss: 1.236269  [ 1456/ 3200]\n",
            "loss: 1.098660  [ 1472/ 3200]\n",
            "loss: 1.306022  [ 1488/ 3200]\n",
            "loss: 1.011973  [ 1504/ 3200]\n",
            "loss: 1.325021  [ 1520/ 3200]\n",
            "loss: 1.158193  [ 1536/ 3200]\n",
            "loss: 1.271725  [ 1552/ 3200]\n",
            "loss: 1.292269  [ 1568/ 3200]\n",
            "loss: 1.271887  [ 1584/ 3200]\n",
            "loss: 1.272458  [ 1600/ 3200]\n",
            "loss: 1.125329  [ 1616/ 3200]\n",
            "loss: 1.256111  [ 1632/ 3200]\n",
            "loss: 1.188280  [ 1648/ 3200]\n",
            "loss: 1.147622  [ 1664/ 3200]\n",
            "loss: 1.157045  [ 1680/ 3200]\n",
            "loss: 1.190240  [ 1696/ 3200]\n",
            "loss: 1.223684  [ 1712/ 3200]\n",
            "loss: 1.229973  [ 1728/ 3200]\n",
            "loss: 1.183535  [ 1744/ 3200]\n",
            "loss: 1.118995  [ 1760/ 3200]\n",
            "loss: 1.261475  [ 1776/ 3200]\n",
            "loss: 1.101478  [ 1792/ 3200]\n",
            "loss: 1.228228  [ 1808/ 3200]\n",
            "loss: 1.036759  [ 1824/ 3200]\n",
            "loss: 1.167966  [ 1840/ 3200]\n",
            "loss: 1.210043  [ 1856/ 3200]\n",
            "loss: 1.160769  [ 1872/ 3200]\n",
            "loss: 1.354277  [ 1888/ 3200]\n",
            "loss: 1.172501  [ 1904/ 3200]\n",
            "loss: 1.103311  [ 1920/ 3200]\n",
            "loss: 1.244433  [ 1936/ 3200]\n",
            "loss: 1.224401  [ 1952/ 3200]\n",
            "loss: 1.204586  [ 1968/ 3200]\n",
            "loss: 1.214474  [ 1984/ 3200]\n",
            "loss: 1.160661  [ 2000/ 3200]\n",
            "loss: 1.217157  [ 2016/ 3200]\n",
            "loss: 1.271640  [ 2032/ 3200]\n",
            "loss: 1.233850  [ 2048/ 3200]\n",
            "loss: 1.221264  [ 2064/ 3200]\n",
            "loss: 1.179971  [ 2080/ 3200]\n",
            "loss: 1.129270  [ 2096/ 3200]\n",
            "loss: 1.145191  [ 2112/ 3200]\n",
            "loss: 1.032830  [ 2128/ 3200]\n",
            "loss: 1.058049  [ 2144/ 3200]\n",
            "loss: 1.180395  [ 2160/ 3200]\n",
            "loss: 1.304903  [ 2176/ 3200]\n",
            "loss: 1.251184  [ 2192/ 3200]\n",
            "loss: 1.117063  [ 2208/ 3200]\n",
            "loss: 1.285817  [ 2224/ 3200]\n",
            "loss: 1.103909  [ 2240/ 3200]\n",
            "loss: 1.228333  [ 2256/ 3200]\n",
            "loss: 1.059012  [ 2272/ 3200]\n",
            "loss: 1.212970  [ 2288/ 3200]\n",
            "loss: 1.196895  [ 2304/ 3200]\n",
            "loss: 1.287822  [ 2320/ 3200]\n",
            "loss: 1.095536  [ 2336/ 3200]\n",
            "loss: 1.239755  [ 2352/ 3200]\n",
            "loss: 1.222731  [ 2368/ 3200]\n",
            "loss: 1.161806  [ 2384/ 3200]\n",
            "loss: 1.258156  [ 2400/ 3200]\n",
            "loss: 1.147122  [ 2416/ 3200]\n",
            "loss: 1.117628  [ 2432/ 3200]\n",
            "loss: 1.112847  [ 2448/ 3200]\n",
            "loss: 1.039134  [ 2464/ 3200]\n",
            "loss: 1.259364  [ 2480/ 3200]\n",
            "loss: 1.111909  [ 2496/ 3200]\n",
            "loss: 1.170586  [ 2512/ 3200]\n",
            "loss: 1.136730  [ 2528/ 3200]\n",
            "loss: 1.211926  [ 2544/ 3200]\n",
            "loss: 0.981094  [ 2560/ 3200]\n",
            "loss: 1.254583  [ 2576/ 3200]\n",
            "loss: 1.342681  [ 2592/ 3200]\n",
            "loss: 1.167253  [ 2608/ 3200]\n",
            "loss: 1.205996  [ 2624/ 3200]\n",
            "loss: 1.192460  [ 2640/ 3200]\n",
            "loss: 1.169003  [ 2656/ 3200]\n",
            "loss: 1.073300  [ 2672/ 3200]\n",
            "loss: 1.295181  [ 2688/ 3200]\n",
            "loss: 1.218311  [ 2704/ 3200]\n",
            "loss: 1.059189  [ 2720/ 3200]\n",
            "loss: 1.320632  [ 2736/ 3200]\n",
            "loss: 1.156443  [ 2752/ 3200]\n",
            "loss: 1.100001  [ 2768/ 3200]\n",
            "loss: 1.158988  [ 2784/ 3200]\n",
            "loss: 1.150841  [ 2800/ 3200]\n",
            "loss: 1.212695  [ 2816/ 3200]\n",
            "loss: 1.227922  [ 2832/ 3200]\n",
            "loss: 1.154207  [ 2848/ 3200]\n",
            "loss: 1.148751  [ 2864/ 3200]\n",
            "loss: 1.186834  [ 2880/ 3200]\n",
            "loss: 1.246518  [ 2896/ 3200]\n",
            "loss: 1.222947  [ 2912/ 3200]\n",
            "loss: 1.108512  [ 2928/ 3200]\n",
            "loss: 0.993540  [ 2944/ 3200]\n",
            "loss: 1.244551  [ 2960/ 3200]\n",
            "loss: 1.209074  [ 2976/ 3200]\n",
            "loss: 1.256139  [ 2992/ 3200]\n",
            "loss: 1.182388  [ 3008/ 3200]\n",
            "loss: 1.125149  [ 3024/ 3200]\n",
            "loss: 1.263350  [ 3040/ 3200]\n",
            "loss: 1.142306  [ 3056/ 3200]\n",
            "loss: 1.154954  [ 3072/ 3200]\n",
            "loss: 1.131815  [ 3088/ 3200]\n",
            "loss: 1.108063  [ 3104/ 3200]\n",
            "loss: 1.179133  [ 3120/ 3200]\n",
            "loss: 1.105868  [ 3136/ 3200]\n",
            "loss: 1.200091  [ 3152/ 3200]\n",
            "loss: 1.270549  [ 3168/ 3200]\n",
            "loss: 1.241538  [ 3184/ 3200]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.141048  [    0/ 3200]\n",
            "loss: 1.124175  [   16/ 3200]\n",
            "loss: 1.217010  [   32/ 3200]\n",
            "loss: 1.163122  [   48/ 3200]\n",
            "loss: 1.213570  [   64/ 3200]\n",
            "loss: 1.167585  [   80/ 3200]\n",
            "loss: 1.194693  [   96/ 3200]\n",
            "loss: 1.161739  [  112/ 3200]\n",
            "loss: 1.201107  [  128/ 3200]\n",
            "loss: 1.101891  [  144/ 3200]\n",
            "loss: 1.221172  [  160/ 3200]\n",
            "loss: 1.059446  [  176/ 3200]\n",
            "loss: 1.204348  [  192/ 3200]\n",
            "loss: 1.167424  [  208/ 3200]\n",
            "loss: 1.074066  [  224/ 3200]\n",
            "loss: 1.151011  [  240/ 3200]\n",
            "loss: 1.181451  [  256/ 3200]\n",
            "loss: 1.233220  [  272/ 3200]\n",
            "loss: 0.972625  [  288/ 3200]\n",
            "loss: 1.275959  [  304/ 3200]\n",
            "loss: 1.308017  [  320/ 3200]\n",
            "loss: 1.209734  [  336/ 3200]\n",
            "loss: 1.160653  [  352/ 3200]\n",
            "loss: 1.252825  [  368/ 3200]\n",
            "loss: 1.219732  [  384/ 3200]\n",
            "loss: 1.224557  [  400/ 3200]\n",
            "loss: 1.213638  [  416/ 3200]\n",
            "loss: 1.054541  [  432/ 3200]\n",
            "loss: 1.081247  [  448/ 3200]\n",
            "loss: 1.207466  [  464/ 3200]\n",
            "loss: 1.040757  [  480/ 3200]\n",
            "loss: 1.181014  [  496/ 3200]\n",
            "loss: 1.207837  [  512/ 3200]\n",
            "loss: 1.137232  [  528/ 3200]\n",
            "loss: 1.268803  [  544/ 3200]\n",
            "loss: 1.247172  [  560/ 3200]\n",
            "loss: 1.062856  [  576/ 3200]\n",
            "loss: 1.168303  [  592/ 3200]\n",
            "loss: 1.227595  [  608/ 3200]\n",
            "loss: 1.362994  [  624/ 3200]\n",
            "loss: 1.248937  [  640/ 3200]\n",
            "loss: 1.284150  [  656/ 3200]\n",
            "loss: 1.158130  [  672/ 3200]\n",
            "loss: 1.180675  [  688/ 3200]\n",
            "loss: 1.162279  [  704/ 3200]\n",
            "loss: 1.136638  [  720/ 3200]\n",
            "loss: 1.211015  [  736/ 3200]\n",
            "loss: 1.099575  [  752/ 3200]\n",
            "loss: 1.064800  [  768/ 3200]\n",
            "loss: 1.159283  [  784/ 3200]\n",
            "loss: 1.250488  [  800/ 3200]\n",
            "loss: 1.106534  [  816/ 3200]\n",
            "loss: 1.119236  [  832/ 3200]\n",
            "loss: 1.147896  [  848/ 3200]\n",
            "loss: 1.166784  [  864/ 3200]\n",
            "loss: 1.192100  [  880/ 3200]\n",
            "loss: 1.085459  [  896/ 3200]\n",
            "loss: 1.281396  [  912/ 3200]\n",
            "loss: 1.124210  [  928/ 3200]\n",
            "loss: 1.165862  [  944/ 3200]\n",
            "loss: 1.184312  [  960/ 3200]\n",
            "loss: 1.312411  [  976/ 3200]\n",
            "loss: 1.186596  [  992/ 3200]\n",
            "loss: 1.174121  [ 1008/ 3200]\n",
            "loss: 1.164328  [ 1024/ 3200]\n",
            "loss: 1.142019  [ 1040/ 3200]\n",
            "loss: 1.171802  [ 1056/ 3200]\n",
            "loss: 1.171862  [ 1072/ 3200]\n",
            "loss: 1.200098  [ 1088/ 3200]\n",
            "loss: 1.292402  [ 1104/ 3200]\n",
            "loss: 1.298642  [ 1120/ 3200]\n",
            "loss: 1.217280  [ 1136/ 3200]\n",
            "loss: 1.122360  [ 1152/ 3200]\n",
            "loss: 1.211239  [ 1168/ 3200]\n",
            "loss: 1.173209  [ 1184/ 3200]\n",
            "loss: 1.224215  [ 1200/ 3200]\n",
            "loss: 1.131522  [ 1216/ 3200]\n",
            "loss: 1.105658  [ 1232/ 3200]\n",
            "loss: 1.185537  [ 1248/ 3200]\n",
            "loss: 1.168686  [ 1264/ 3200]\n",
            "loss: 1.177469  [ 1280/ 3200]\n",
            "loss: 1.206007  [ 1296/ 3200]\n",
            "loss: 1.211000  [ 1312/ 3200]\n",
            "loss: 1.161018  [ 1328/ 3200]\n",
            "loss: 1.180390  [ 1344/ 3200]\n",
            "loss: 1.194301  [ 1360/ 3200]\n",
            "loss: 1.124015  [ 1376/ 3200]\n",
            "loss: 1.149188  [ 1392/ 3200]\n",
            "loss: 1.161472  [ 1408/ 3200]\n",
            "loss: 1.246214  [ 1424/ 3200]\n",
            "loss: 1.102609  [ 1440/ 3200]\n",
            "loss: 1.155598  [ 1456/ 3200]\n",
            "loss: 1.453532  [ 1472/ 3200]\n",
            "loss: 1.160348  [ 1488/ 3200]\n",
            "loss: 1.082727  [ 1504/ 3200]\n",
            "loss: 1.275933  [ 1520/ 3200]\n",
            "loss: 1.092439  [ 1536/ 3200]\n",
            "loss: 1.072675  [ 1552/ 3200]\n",
            "loss: 1.194509  [ 1568/ 3200]\n",
            "loss: 1.251069  [ 1584/ 3200]\n",
            "loss: 1.151088  [ 1600/ 3200]\n",
            "loss: 1.297907  [ 1616/ 3200]\n",
            "loss: 1.284288  [ 1632/ 3200]\n",
            "loss: 1.330874  [ 1648/ 3200]\n",
            "loss: 1.071748  [ 1664/ 3200]\n",
            "loss: 1.129815  [ 1680/ 3200]\n",
            "loss: 1.074962  [ 1696/ 3200]\n",
            "loss: 1.119737  [ 1712/ 3200]\n",
            "loss: 1.307572  [ 1728/ 3200]\n",
            "loss: 1.034798  [ 1744/ 3200]\n",
            "loss: 1.203292  [ 1760/ 3200]\n",
            "loss: 1.132862  [ 1776/ 3200]\n",
            "loss: 1.119038  [ 1792/ 3200]\n",
            "loss: 1.128230  [ 1808/ 3200]\n",
            "loss: 1.107789  [ 1824/ 3200]\n",
            "loss: 1.090183  [ 1840/ 3200]\n",
            "loss: 1.222550  [ 1856/ 3200]\n",
            "loss: 1.285205  [ 1872/ 3200]\n",
            "loss: 1.055369  [ 1888/ 3200]\n",
            "loss: 1.217825  [ 1904/ 3200]\n",
            "loss: 1.111159  [ 1920/ 3200]\n",
            "loss: 1.134393  [ 1936/ 3200]\n",
            "loss: 1.121541  [ 1952/ 3200]\n",
            "loss: 1.141577  [ 1968/ 3200]\n",
            "loss: 1.101420  [ 1984/ 3200]\n",
            "loss: 1.181844  [ 2000/ 3200]\n",
            "loss: 1.207286  [ 2016/ 3200]\n",
            "loss: 1.023286  [ 2032/ 3200]\n",
            "loss: 1.180353  [ 2048/ 3200]\n",
            "loss: 1.176755  [ 2064/ 3200]\n",
            "loss: 1.070063  [ 2080/ 3200]\n",
            "loss: 1.159740  [ 2096/ 3200]\n",
            "loss: 1.352959  [ 2112/ 3200]\n",
            "loss: 1.180657  [ 2128/ 3200]\n",
            "loss: 1.282185  [ 2144/ 3200]\n",
            "loss: 1.135893  [ 2160/ 3200]\n",
            "loss: 1.093425  [ 2176/ 3200]\n",
            "loss: 0.980831  [ 2192/ 3200]\n",
            "loss: 1.154655  [ 2208/ 3200]\n",
            "loss: 1.148319  [ 2224/ 3200]\n",
            "loss: 1.241764  [ 2240/ 3200]\n",
            "loss: 1.064265  [ 2256/ 3200]\n",
            "loss: 1.262367  [ 2272/ 3200]\n",
            "loss: 1.183069  [ 2288/ 3200]\n",
            "loss: 1.134938  [ 2304/ 3200]\n",
            "loss: 1.020533  [ 2320/ 3200]\n",
            "loss: 1.064610  [ 2336/ 3200]\n",
            "loss: 1.197068  [ 2352/ 3200]\n",
            "loss: 1.193840  [ 2368/ 3200]\n",
            "loss: 1.221427  [ 2384/ 3200]\n",
            "loss: 1.237372  [ 2400/ 3200]\n",
            "loss: 1.280337  [ 2416/ 3200]\n",
            "loss: 1.311991  [ 2432/ 3200]\n",
            "loss: 1.136623  [ 2448/ 3200]\n",
            "loss: 1.136056  [ 2464/ 3200]\n",
            "loss: 1.087515  [ 2480/ 3200]\n",
            "loss: 1.186440  [ 2496/ 3200]\n",
            "loss: 1.093515  [ 2512/ 3200]\n",
            "loss: 1.043026  [ 2528/ 3200]\n",
            "loss: 1.041361  [ 2544/ 3200]\n",
            "loss: 1.229167  [ 2560/ 3200]\n",
            "loss: 1.231519  [ 2576/ 3200]\n",
            "loss: 1.055828  [ 2592/ 3200]\n",
            "loss: 1.229373  [ 2608/ 3200]\n",
            "loss: 1.063439  [ 2624/ 3200]\n",
            "loss: 1.259465  [ 2640/ 3200]\n",
            "loss: 1.172496  [ 2656/ 3200]\n",
            "loss: 1.159341  [ 2672/ 3200]\n",
            "loss: 1.140681  [ 2688/ 3200]\n",
            "loss: 1.076163  [ 2704/ 3200]\n",
            "loss: 1.156963  [ 2720/ 3200]\n",
            "loss: 1.231194  [ 2736/ 3200]\n",
            "loss: 1.212815  [ 2752/ 3200]\n",
            "loss: 1.062779  [ 2768/ 3200]\n",
            "loss: 1.252748  [ 2784/ 3200]\n",
            "loss: 1.161787  [ 2800/ 3200]\n",
            "loss: 1.151708  [ 2816/ 3200]\n",
            "loss: 1.051727  [ 2832/ 3200]\n",
            "loss: 1.138672  [ 2848/ 3200]\n",
            "loss: 1.227022  [ 2864/ 3200]\n",
            "loss: 1.121858  [ 2880/ 3200]\n",
            "loss: 1.154640  [ 2896/ 3200]\n",
            "loss: 1.173337  [ 2912/ 3200]\n",
            "loss: 1.062013  [ 2928/ 3200]\n",
            "loss: 1.124408  [ 2944/ 3200]\n",
            "loss: 1.052022  [ 2960/ 3200]\n",
            "loss: 1.047306  [ 2976/ 3200]\n",
            "loss: 1.127761  [ 2992/ 3200]\n",
            "loss: 1.086175  [ 3008/ 3200]\n",
            "loss: 1.000358  [ 3024/ 3200]\n",
            "loss: 0.946992  [ 3040/ 3200]\n",
            "loss: 0.993185  [ 3056/ 3200]\n",
            "loss: 1.347804  [ 3072/ 3200]\n",
            "loss: 1.155968  [ 3088/ 3200]\n",
            "loss: 1.383861  [ 3104/ 3200]\n",
            "loss: 1.178110  [ 3120/ 3200]\n",
            "loss: 1.344080  [ 3136/ 3200]\n",
            "loss: 1.167035  [ 3152/ 3200]\n",
            "loss: 1.076954  [ 3168/ 3200]\n",
            "loss: 1.100782  [ 3184/ 3200]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.195302  [    0/ 3200]\n",
            "loss: 1.161655  [   16/ 3200]\n",
            "loss: 1.133830  [   32/ 3200]\n",
            "loss: 1.054027  [   48/ 3200]\n",
            "loss: 1.256806  [   64/ 3200]\n",
            "loss: 1.157798  [   80/ 3200]\n",
            "loss: 1.142599  [   96/ 3200]\n",
            "loss: 1.141603  [  112/ 3200]\n",
            "loss: 1.098355  [  128/ 3200]\n",
            "loss: 1.068149  [  144/ 3200]\n",
            "loss: 1.170220  [  160/ 3200]\n",
            "loss: 1.187457  [  176/ 3200]\n",
            "loss: 1.060679  [  192/ 3200]\n",
            "loss: 1.169878  [  208/ 3200]\n",
            "loss: 1.088121  [  224/ 3200]\n",
            "loss: 1.139414  [  240/ 3200]\n",
            "loss: 1.188950  [  256/ 3200]\n",
            "loss: 1.214766  [  272/ 3200]\n",
            "loss: 1.049989  [  288/ 3200]\n",
            "loss: 1.041566  [  304/ 3200]\n",
            "loss: 1.231536  [  320/ 3200]\n",
            "loss: 1.172962  [  336/ 3200]\n",
            "loss: 1.213795  [  352/ 3200]\n",
            "loss: 1.038260  [  368/ 3200]\n",
            "loss: 1.199601  [  384/ 3200]\n",
            "loss: 1.166547  [  400/ 3200]\n",
            "loss: 1.266520  [  416/ 3200]\n",
            "loss: 1.213152  [  432/ 3200]\n",
            "loss: 1.243256  [  448/ 3200]\n",
            "loss: 1.132572  [  464/ 3200]\n",
            "loss: 1.290749  [  480/ 3200]\n",
            "loss: 1.168596  [  496/ 3200]\n",
            "loss: 1.189654  [  512/ 3200]\n",
            "loss: 1.084972  [  528/ 3200]\n",
            "loss: 1.095424  [  544/ 3200]\n",
            "loss: 1.209900  [  560/ 3200]\n",
            "loss: 1.047744  [  576/ 3200]\n",
            "loss: 1.141200  [  592/ 3200]\n",
            "loss: 1.211568  [  608/ 3200]\n",
            "loss: 1.077102  [  624/ 3200]\n",
            "loss: 1.305285  [  640/ 3200]\n",
            "loss: 1.186296  [  656/ 3200]\n",
            "loss: 1.307938  [  672/ 3200]\n",
            "loss: 1.093045  [  688/ 3200]\n",
            "loss: 1.140447  [  704/ 3200]\n",
            "loss: 1.126401  [  720/ 3200]\n",
            "loss: 1.278001  [  736/ 3200]\n",
            "loss: 1.249201  [  752/ 3200]\n",
            "loss: 1.102633  [  768/ 3200]\n",
            "loss: 0.893087  [  784/ 3200]\n",
            "loss: 1.185015  [  800/ 3200]\n",
            "loss: 1.149846  [  816/ 3200]\n",
            "loss: 1.054109  [  832/ 3200]\n",
            "loss: 1.080432  [  848/ 3200]\n",
            "loss: 1.362317  [  864/ 3200]\n",
            "loss: 1.092948  [  880/ 3200]\n",
            "loss: 1.150733  [  896/ 3200]\n",
            "loss: 1.234967  [  912/ 3200]\n",
            "loss: 1.023193  [  928/ 3200]\n",
            "loss: 1.202445  [  944/ 3200]\n",
            "loss: 1.016540  [  960/ 3200]\n",
            "loss: 1.200266  [  976/ 3200]\n",
            "loss: 1.215473  [  992/ 3200]\n",
            "loss: 1.173728  [ 1008/ 3200]\n",
            "loss: 1.133625  [ 1024/ 3200]\n",
            "loss: 1.181647  [ 1040/ 3200]\n",
            "loss: 1.161363  [ 1056/ 3200]\n",
            "loss: 1.224286  [ 1072/ 3200]\n",
            "loss: 1.018084  [ 1088/ 3200]\n",
            "loss: 1.008113  [ 1104/ 3200]\n",
            "loss: 1.131510  [ 1120/ 3200]\n",
            "loss: 1.300801  [ 1136/ 3200]\n",
            "loss: 1.122814  [ 1152/ 3200]\n",
            "loss: 1.131693  [ 1168/ 3200]\n",
            "loss: 1.331224  [ 1184/ 3200]\n",
            "loss: 1.305612  [ 1200/ 3200]\n",
            "loss: 1.179741  [ 1216/ 3200]\n",
            "loss: 1.076228  [ 1232/ 3200]\n",
            "loss: 1.089399  [ 1248/ 3200]\n",
            "loss: 1.280614  [ 1264/ 3200]\n",
            "loss: 1.214001  [ 1280/ 3200]\n",
            "loss: 1.240830  [ 1296/ 3200]\n",
            "loss: 0.960711  [ 1312/ 3200]\n",
            "loss: 1.157786  [ 1328/ 3200]\n",
            "loss: 1.063338  [ 1344/ 3200]\n",
            "loss: 1.137924  [ 1360/ 3200]\n",
            "loss: 0.991026  [ 1376/ 3200]\n",
            "loss: 1.246332  [ 1392/ 3200]\n",
            "loss: 1.190683  [ 1408/ 3200]\n",
            "loss: 1.250945  [ 1424/ 3200]\n",
            "loss: 1.224185  [ 1440/ 3200]\n",
            "loss: 1.061367  [ 1456/ 3200]\n",
            "loss: 1.124035  [ 1472/ 3200]\n",
            "loss: 1.127834  [ 1488/ 3200]\n",
            "loss: 1.135028  [ 1504/ 3200]\n",
            "loss: 1.203774  [ 1520/ 3200]\n",
            "loss: 1.165294  [ 1536/ 3200]\n",
            "loss: 1.287377  [ 1552/ 3200]\n",
            "loss: 1.177667  [ 1568/ 3200]\n",
            "loss: 1.179163  [ 1584/ 3200]\n",
            "loss: 1.200122  [ 1600/ 3200]\n",
            "loss: 1.141773  [ 1616/ 3200]\n",
            "loss: 1.106975  [ 1632/ 3200]\n",
            "loss: 1.138572  [ 1648/ 3200]\n",
            "loss: 1.312048  [ 1664/ 3200]\n",
            "loss: 1.105613  [ 1680/ 3200]\n",
            "loss: 1.135020  [ 1696/ 3200]\n",
            "loss: 1.210000  [ 1712/ 3200]\n",
            "loss: 1.253042  [ 1728/ 3200]\n",
            "loss: 1.119110  [ 1744/ 3200]\n",
            "loss: 1.096802  [ 1760/ 3200]\n",
            "loss: 1.103406  [ 1776/ 3200]\n",
            "loss: 1.205781  [ 1792/ 3200]\n",
            "loss: 1.050726  [ 1808/ 3200]\n",
            "loss: 1.249677  [ 1824/ 3200]\n",
            "loss: 1.120136  [ 1840/ 3200]\n",
            "loss: 1.115381  [ 1856/ 3200]\n",
            "loss: 1.116471  [ 1872/ 3200]\n",
            "loss: 1.162663  [ 1888/ 3200]\n",
            "loss: 1.241049  [ 1904/ 3200]\n",
            "loss: 1.119586  [ 1920/ 3200]\n",
            "loss: 1.096836  [ 1936/ 3200]\n",
            "loss: 1.053028  [ 1952/ 3200]\n",
            "loss: 1.248285  [ 1968/ 3200]\n",
            "loss: 1.171050  [ 1984/ 3200]\n",
            "loss: 1.187322  [ 2000/ 3200]\n",
            "loss: 1.184605  [ 2016/ 3200]\n",
            "loss: 1.064966  [ 2032/ 3200]\n",
            "loss: 1.098151  [ 2048/ 3200]\n",
            "loss: 1.003850  [ 2064/ 3200]\n",
            "loss: 1.146117  [ 2080/ 3200]\n",
            "loss: 1.081721  [ 2096/ 3200]\n",
            "loss: 1.106578  [ 2112/ 3200]\n",
            "loss: 1.013284  [ 2128/ 3200]\n",
            "loss: 1.219350  [ 2144/ 3200]\n",
            "loss: 1.126106  [ 2160/ 3200]\n",
            "loss: 1.181816  [ 2176/ 3200]\n",
            "loss: 0.976437  [ 2192/ 3200]\n",
            "loss: 1.171764  [ 2208/ 3200]\n",
            "loss: 1.130832  [ 2224/ 3200]\n",
            "loss: 1.130017  [ 2240/ 3200]\n",
            "loss: 1.208567  [ 2256/ 3200]\n",
            "loss: 1.241883  [ 2272/ 3200]\n",
            "loss: 1.142455  [ 2288/ 3200]\n",
            "loss: 1.098804  [ 2304/ 3200]\n",
            "loss: 1.103886  [ 2320/ 3200]\n",
            "loss: 1.138002  [ 2336/ 3200]\n",
            "loss: 1.046643  [ 2352/ 3200]\n",
            "loss: 1.132479  [ 2368/ 3200]\n",
            "loss: 1.114679  [ 2384/ 3200]\n",
            "loss: 1.177966  [ 2400/ 3200]\n",
            "loss: 1.198268  [ 2416/ 3200]\n",
            "loss: 1.142605  [ 2432/ 3200]\n",
            "loss: 1.091006  [ 2448/ 3200]\n",
            "loss: 1.084493  [ 2464/ 3200]\n",
            "loss: 1.237531  [ 2480/ 3200]\n",
            "loss: 1.115200  [ 2496/ 3200]\n",
            "loss: 0.992033  [ 2512/ 3200]\n",
            "loss: 0.857957  [ 2528/ 3200]\n",
            "loss: 1.066508  [ 2544/ 3200]\n",
            "loss: 1.146680  [ 2560/ 3200]\n",
            "loss: 1.010897  [ 2576/ 3200]\n",
            "loss: 1.058156  [ 2592/ 3200]\n",
            "loss: 1.245753  [ 2608/ 3200]\n",
            "loss: 1.210438  [ 2624/ 3200]\n",
            "loss: 1.194031  [ 2640/ 3200]\n",
            "loss: 0.993849  [ 2656/ 3200]\n",
            "loss: 1.226017  [ 2672/ 3200]\n",
            "loss: 1.207655  [ 2688/ 3200]\n",
            "loss: 1.227936  [ 2704/ 3200]\n",
            "loss: 1.328734  [ 2720/ 3200]\n",
            "loss: 1.054890  [ 2736/ 3200]\n",
            "loss: 1.009394  [ 2752/ 3200]\n",
            "loss: 1.277816  [ 2768/ 3200]\n",
            "loss: 1.118927  [ 2784/ 3200]\n",
            "loss: 1.094767  [ 2800/ 3200]\n",
            "loss: 1.154876  [ 2816/ 3200]\n",
            "loss: 1.028024  [ 2832/ 3200]\n",
            "loss: 1.177796  [ 2848/ 3200]\n",
            "loss: 1.225615  [ 2864/ 3200]\n",
            "loss: 1.315159  [ 2880/ 3200]\n",
            "loss: 1.196357  [ 2896/ 3200]\n",
            "loss: 1.278216  [ 2912/ 3200]\n",
            "loss: 1.089644  [ 2928/ 3200]\n",
            "loss: 1.163406  [ 2944/ 3200]\n",
            "loss: 1.007692  [ 2960/ 3200]\n",
            "loss: 1.227572  [ 2976/ 3200]\n",
            "loss: 1.052556  [ 2992/ 3200]\n",
            "loss: 1.120660  [ 3008/ 3200]\n",
            "loss: 1.063684  [ 3024/ 3200]\n",
            "loss: 0.981119  [ 3040/ 3200]\n",
            "loss: 1.085400  [ 3056/ 3200]\n",
            "loss: 1.249143  [ 3072/ 3200]\n",
            "loss: 1.117498  [ 3088/ 3200]\n",
            "loss: 0.989657  [ 3104/ 3200]\n",
            "loss: 1.132057  [ 3120/ 3200]\n",
            "loss: 1.075148  [ 3136/ 3200]\n",
            "loss: 1.154691  [ 3152/ 3200]\n",
            "loss: 0.989086  [ 3168/ 3200]\n",
            "loss: 1.186296  [ 3184/ 3200]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.200572  [    0/ 3200]\n",
            "loss: 1.260585  [   16/ 3200]\n",
            "loss: 1.102100  [   32/ 3200]\n",
            "loss: 1.170459  [   48/ 3200]\n",
            "loss: 1.300431  [   64/ 3200]\n",
            "loss: 1.059408  [   80/ 3200]\n",
            "loss: 0.982764  [   96/ 3200]\n",
            "loss: 0.972626  [  112/ 3200]\n",
            "loss: 1.154003  [  128/ 3200]\n",
            "loss: 1.169061  [  144/ 3200]\n",
            "loss: 1.226859  [  160/ 3200]\n",
            "loss: 1.028020  [  176/ 3200]\n",
            "loss: 1.051392  [  192/ 3200]\n",
            "loss: 1.115373  [  208/ 3200]\n",
            "loss: 1.096153  [  224/ 3200]\n",
            "loss: 1.180590  [  240/ 3200]\n",
            "loss: 1.168236  [  256/ 3200]\n",
            "loss: 1.025824  [  272/ 3200]\n",
            "loss: 1.136375  [  288/ 3200]\n",
            "loss: 1.204511  [  304/ 3200]\n",
            "loss: 1.122428  [  320/ 3200]\n",
            "loss: 1.010984  [  336/ 3200]\n",
            "loss: 1.186610  [  352/ 3200]\n",
            "loss: 1.020227  [  368/ 3200]\n",
            "loss: 0.987978  [  384/ 3200]\n",
            "loss: 1.178158  [  400/ 3200]\n",
            "loss: 1.382913  [  416/ 3200]\n",
            "loss: 1.113879  [  432/ 3200]\n",
            "loss: 1.083167  [  448/ 3200]\n",
            "loss: 1.220679  [  464/ 3200]\n",
            "loss: 1.267079  [  480/ 3200]\n",
            "loss: 1.182890  [  496/ 3200]\n",
            "loss: 1.170713  [  512/ 3200]\n",
            "loss: 1.060584  [  528/ 3200]\n",
            "loss: 1.035438  [  544/ 3200]\n",
            "loss: 1.155660  [  560/ 3200]\n",
            "loss: 1.219807  [  576/ 3200]\n",
            "loss: 1.012836  [  592/ 3200]\n",
            "loss: 1.216545  [  608/ 3200]\n",
            "loss: 1.113853  [  624/ 3200]\n",
            "loss: 1.092421  [  640/ 3200]\n",
            "loss: 1.084900  [  656/ 3200]\n",
            "loss: 1.096987  [  672/ 3200]\n",
            "loss: 1.170865  [  688/ 3200]\n",
            "loss: 1.174159  [  704/ 3200]\n",
            "loss: 1.128520  [  720/ 3200]\n",
            "loss: 1.158070  [  736/ 3200]\n",
            "loss: 1.096975  [  752/ 3200]\n",
            "loss: 1.054361  [  768/ 3200]\n",
            "loss: 1.074568  [  784/ 3200]\n",
            "loss: 1.124546  [  800/ 3200]\n",
            "loss: 1.222563  [  816/ 3200]\n",
            "loss: 1.367522  [  832/ 3200]\n",
            "loss: 1.142912  [  848/ 3200]\n",
            "loss: 1.216373  [  864/ 3200]\n",
            "loss: 1.054299  [  880/ 3200]\n",
            "loss: 1.067957  [  896/ 3200]\n",
            "loss: 1.142271  [  912/ 3200]\n",
            "loss: 1.186598  [  928/ 3200]\n",
            "loss: 1.075732  [  944/ 3200]\n",
            "loss: 1.112399  [  960/ 3200]\n",
            "loss: 1.117503  [  976/ 3200]\n",
            "loss: 1.108422  [  992/ 3200]\n",
            "loss: 1.094029  [ 1008/ 3200]\n",
            "loss: 1.087113  [ 1024/ 3200]\n",
            "loss: 1.046565  [ 1040/ 3200]\n",
            "loss: 1.046288  [ 1056/ 3200]\n",
            "loss: 1.119926  [ 1072/ 3200]\n",
            "loss: 1.110502  [ 1088/ 3200]\n",
            "loss: 1.096455  [ 1104/ 3200]\n",
            "loss: 1.100050  [ 1120/ 3200]\n",
            "loss: 1.180577  [ 1136/ 3200]\n",
            "loss: 1.192721  [ 1152/ 3200]\n",
            "loss: 1.107764  [ 1168/ 3200]\n",
            "loss: 1.149815  [ 1184/ 3200]\n",
            "loss: 1.163462  [ 1200/ 3200]\n",
            "loss: 1.242184  [ 1216/ 3200]\n",
            "loss: 1.185556  [ 1232/ 3200]\n",
            "loss: 1.086743  [ 1248/ 3200]\n",
            "loss: 1.202646  [ 1264/ 3200]\n",
            "loss: 1.127324  [ 1280/ 3200]\n",
            "loss: 1.062382  [ 1296/ 3200]\n",
            "loss: 1.053176  [ 1312/ 3200]\n",
            "loss: 1.171385  [ 1328/ 3200]\n",
            "loss: 1.158852  [ 1344/ 3200]\n",
            "loss: 1.183003  [ 1360/ 3200]\n",
            "loss: 1.259332  [ 1376/ 3200]\n",
            "loss: 1.186330  [ 1392/ 3200]\n",
            "loss: 0.981513  [ 1408/ 3200]\n",
            "loss: 1.227723  [ 1424/ 3200]\n",
            "loss: 1.119492  [ 1440/ 3200]\n",
            "loss: 1.134170  [ 1456/ 3200]\n",
            "loss: 1.206688  [ 1472/ 3200]\n",
            "loss: 1.084713  [ 1488/ 3200]\n",
            "loss: 1.090161  [ 1504/ 3200]\n",
            "loss: 1.153261  [ 1520/ 3200]\n",
            "loss: 1.214979  [ 1536/ 3200]\n",
            "loss: 1.139074  [ 1552/ 3200]\n",
            "loss: 1.128498  [ 1568/ 3200]\n",
            "loss: 1.084968  [ 1584/ 3200]\n",
            "loss: 1.083379  [ 1600/ 3200]\n",
            "loss: 1.133254  [ 1616/ 3200]\n",
            "loss: 1.060790  [ 1632/ 3200]\n",
            "loss: 1.048428  [ 1648/ 3200]\n",
            "loss: 1.144024  [ 1664/ 3200]\n",
            "loss: 1.146119  [ 1680/ 3200]\n",
            "loss: 1.101928  [ 1696/ 3200]\n",
            "loss: 1.158673  [ 1712/ 3200]\n",
            "loss: 1.059537  [ 1728/ 3200]\n",
            "loss: 1.039681  [ 1744/ 3200]\n",
            "loss: 0.993492  [ 1760/ 3200]\n",
            "loss: 1.025471  [ 1776/ 3200]\n",
            "loss: 0.909470  [ 1792/ 3200]\n",
            "loss: 1.153539  [ 1808/ 3200]\n",
            "loss: 1.084483  [ 1824/ 3200]\n",
            "loss: 1.094632  [ 1840/ 3200]\n",
            "loss: 1.153253  [ 1856/ 3200]\n",
            "loss: 1.089455  [ 1872/ 3200]\n",
            "loss: 1.130615  [ 1888/ 3200]\n",
            "loss: 1.179877  [ 1904/ 3200]\n",
            "loss: 1.231984  [ 1920/ 3200]\n",
            "loss: 0.854301  [ 1936/ 3200]\n",
            "loss: 1.193664  [ 1952/ 3200]\n",
            "loss: 1.321709  [ 1968/ 3200]\n",
            "loss: 1.084781  [ 1984/ 3200]\n",
            "loss: 1.176864  [ 2000/ 3200]\n",
            "loss: 1.098757  [ 2016/ 3200]\n",
            "loss: 1.020808  [ 2032/ 3200]\n",
            "loss: 1.392421  [ 2048/ 3200]\n",
            "loss: 1.064947  [ 2064/ 3200]\n",
            "loss: 1.065080  [ 2080/ 3200]\n",
            "loss: 1.324411  [ 2096/ 3200]\n",
            "loss: 1.232515  [ 2112/ 3200]\n",
            "loss: 0.990748  [ 2128/ 3200]\n",
            "loss: 1.273381  [ 2144/ 3200]\n",
            "loss: 1.148692  [ 2160/ 3200]\n",
            "loss: 1.329507  [ 2176/ 3200]\n",
            "loss: 1.177606  [ 2192/ 3200]\n",
            "loss: 1.094804  [ 2208/ 3200]\n",
            "loss: 1.274216  [ 2224/ 3200]\n",
            "loss: 1.208692  [ 2240/ 3200]\n",
            "loss: 1.084563  [ 2256/ 3200]\n",
            "loss: 0.942268  [ 2272/ 3200]\n",
            "loss: 1.055299  [ 2288/ 3200]\n",
            "loss: 1.074625  [ 2304/ 3200]\n",
            "loss: 1.094206  [ 2320/ 3200]\n",
            "loss: 1.046454  [ 2336/ 3200]\n",
            "loss: 1.172975  [ 2352/ 3200]\n",
            "loss: 0.999288  [ 2368/ 3200]\n",
            "loss: 1.083506  [ 2384/ 3200]\n",
            "loss: 1.099597  [ 2400/ 3200]\n",
            "loss: 1.151170  [ 2416/ 3200]\n",
            "loss: 1.070814  [ 2432/ 3200]\n",
            "loss: 1.090970  [ 2448/ 3200]\n",
            "loss: 1.103710  [ 2464/ 3200]\n",
            "loss: 1.015277  [ 2480/ 3200]\n",
            "loss: 1.150422  [ 2496/ 3200]\n",
            "loss: 1.057105  [ 2512/ 3200]\n",
            "loss: 0.978935  [ 2528/ 3200]\n",
            "loss: 1.092044  [ 2544/ 3200]\n",
            "loss: 1.206400  [ 2560/ 3200]\n",
            "loss: 1.130824  [ 2576/ 3200]\n",
            "loss: 1.104339  [ 2592/ 3200]\n",
            "loss: 1.111808  [ 2608/ 3200]\n",
            "loss: 0.920042  [ 2624/ 3200]\n",
            "loss: 1.123403  [ 2640/ 3200]\n",
            "loss: 1.209255  [ 2656/ 3200]\n",
            "loss: 1.050889  [ 2672/ 3200]\n",
            "loss: 1.100857  [ 2688/ 3200]\n",
            "loss: 1.075696  [ 2704/ 3200]\n",
            "loss: 1.109240  [ 2720/ 3200]\n",
            "loss: 1.080688  [ 2736/ 3200]\n",
            "loss: 1.164790  [ 2752/ 3200]\n",
            "loss: 1.014389  [ 2768/ 3200]\n",
            "loss: 0.953836  [ 2784/ 3200]\n",
            "loss: 1.228942  [ 2800/ 3200]\n",
            "loss: 1.104792  [ 2816/ 3200]\n",
            "loss: 1.047602  [ 2832/ 3200]\n",
            "loss: 1.108142  [ 2848/ 3200]\n",
            "loss: 1.034249  [ 2864/ 3200]\n",
            "loss: 1.189538  [ 2880/ 3200]\n",
            "loss: 1.056015  [ 2896/ 3200]\n",
            "loss: 1.170542  [ 2912/ 3200]\n",
            "loss: 1.200883  [ 2928/ 3200]\n",
            "loss: 1.037548  [ 2944/ 3200]\n",
            "loss: 1.255834  [ 2960/ 3200]\n",
            "loss: 1.033955  [ 2976/ 3200]\n",
            "loss: 1.055130  [ 2992/ 3200]\n",
            "loss: 0.942359  [ 3008/ 3200]\n",
            "loss: 1.097253  [ 3024/ 3200]\n",
            "loss: 1.240537  [ 3040/ 3200]\n",
            "loss: 1.135280  [ 3056/ 3200]\n",
            "loss: 1.144992  [ 3072/ 3200]\n",
            "loss: 1.089125  [ 3088/ 3200]\n",
            "loss: 1.240492  [ 3104/ 3200]\n",
            "loss: 1.107035  [ 3120/ 3200]\n",
            "loss: 1.172974  [ 3136/ 3200]\n",
            "loss: 1.119908  [ 3152/ 3200]\n",
            "loss: 1.086279  [ 3168/ 3200]\n",
            "loss: 1.066374  [ 3184/ 3200]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.019863  [    0/ 3200]\n",
            "loss: 1.112481  [   16/ 3200]\n",
            "loss: 1.138940  [   32/ 3200]\n",
            "loss: 1.197846  [   48/ 3200]\n",
            "loss: 1.064078  [   64/ 3200]\n",
            "loss: 1.169992  [   80/ 3200]\n",
            "loss: 1.040042  [   96/ 3200]\n",
            "loss: 1.243945  [  112/ 3200]\n",
            "loss: 1.180510  [  128/ 3200]\n",
            "loss: 1.068621  [  144/ 3200]\n",
            "loss: 1.081500  [  160/ 3200]\n",
            "loss: 0.962638  [  176/ 3200]\n",
            "loss: 1.009602  [  192/ 3200]\n",
            "loss: 0.948225  [  208/ 3200]\n",
            "loss: 1.130819  [  224/ 3200]\n",
            "loss: 1.144826  [  240/ 3200]\n",
            "loss: 1.210967  [  256/ 3200]\n",
            "loss: 1.006292  [  272/ 3200]\n",
            "loss: 1.126192  [  288/ 3200]\n",
            "loss: 1.069504  [  304/ 3200]\n",
            "loss: 1.158028  [  320/ 3200]\n",
            "loss: 0.910497  [  336/ 3200]\n",
            "loss: 1.017718  [  352/ 3200]\n",
            "loss: 0.956201  [  368/ 3200]\n",
            "loss: 1.150209  [  384/ 3200]\n",
            "loss: 1.180412  [  400/ 3200]\n",
            "loss: 1.035211  [  416/ 3200]\n",
            "loss: 1.091731  [  432/ 3200]\n",
            "loss: 1.029784  [  448/ 3200]\n",
            "loss: 1.019837  [  464/ 3200]\n",
            "loss: 1.204876  [  480/ 3200]\n",
            "loss: 1.203018  [  496/ 3200]\n",
            "loss: 1.031189  [  512/ 3200]\n",
            "loss: 0.945484  [  528/ 3200]\n",
            "loss: 1.203415  [  544/ 3200]\n",
            "loss: 1.044389  [  560/ 3200]\n",
            "loss: 1.146645  [  576/ 3200]\n",
            "loss: 1.202816  [  592/ 3200]\n",
            "loss: 1.168844  [  608/ 3200]\n",
            "loss: 1.067847  [  624/ 3200]\n",
            "loss: 1.101962  [  640/ 3200]\n",
            "loss: 1.175828  [  656/ 3200]\n",
            "loss: 1.201593  [  672/ 3200]\n",
            "loss: 0.780111  [  688/ 3200]\n",
            "loss: 1.194488  [  704/ 3200]\n",
            "loss: 1.005327  [  720/ 3200]\n",
            "loss: 1.067793  [  736/ 3200]\n",
            "loss: 1.201868  [  752/ 3200]\n",
            "loss: 1.265045  [  768/ 3200]\n",
            "loss: 1.105788  [  784/ 3200]\n",
            "loss: 1.071190  [  800/ 3200]\n",
            "loss: 1.182032  [  816/ 3200]\n",
            "loss: 1.184705  [  832/ 3200]\n",
            "loss: 1.072382  [  848/ 3200]\n",
            "loss: 1.066883  [  864/ 3200]\n",
            "loss: 1.235918  [  880/ 3200]\n",
            "loss: 1.118331  [  896/ 3200]\n",
            "loss: 1.279464  [  912/ 3200]\n",
            "loss: 1.106737  [  928/ 3200]\n",
            "loss: 1.114856  [  944/ 3200]\n",
            "loss: 1.069523  [  960/ 3200]\n",
            "loss: 1.154325  [  976/ 3200]\n",
            "loss: 1.156781  [  992/ 3200]\n",
            "loss: 1.073074  [ 1008/ 3200]\n",
            "loss: 1.090974  [ 1024/ 3200]\n",
            "loss: 1.139143  [ 1040/ 3200]\n",
            "loss: 1.239604  [ 1056/ 3200]\n",
            "loss: 1.282633  [ 1072/ 3200]\n",
            "loss: 1.186057  [ 1088/ 3200]\n",
            "loss: 1.108366  [ 1104/ 3200]\n",
            "loss: 0.993150  [ 1120/ 3200]\n",
            "loss: 1.221729  [ 1136/ 3200]\n",
            "loss: 1.148990  [ 1152/ 3200]\n",
            "loss: 1.189963  [ 1168/ 3200]\n",
            "loss: 1.108710  [ 1184/ 3200]\n",
            "loss: 1.006713  [ 1200/ 3200]\n",
            "loss: 0.960935  [ 1216/ 3200]\n",
            "loss: 1.119730  [ 1232/ 3200]\n",
            "loss: 1.036990  [ 1248/ 3200]\n",
            "loss: 1.211489  [ 1264/ 3200]\n",
            "loss: 1.196042  [ 1280/ 3200]\n",
            "loss: 1.021468  [ 1296/ 3200]\n",
            "loss: 1.018481  [ 1312/ 3200]\n",
            "loss: 1.065710  [ 1328/ 3200]\n",
            "loss: 0.949316  [ 1344/ 3200]\n",
            "loss: 1.061615  [ 1360/ 3200]\n",
            "loss: 1.131399  [ 1376/ 3200]\n",
            "loss: 1.167650  [ 1392/ 3200]\n",
            "loss: 1.068387  [ 1408/ 3200]\n",
            "loss: 1.079350  [ 1424/ 3200]\n",
            "loss: 1.164718  [ 1440/ 3200]\n",
            "loss: 1.107359  [ 1456/ 3200]\n",
            "loss: 1.084352  [ 1472/ 3200]\n",
            "loss: 1.000618  [ 1488/ 3200]\n",
            "loss: 1.229965  [ 1504/ 3200]\n",
            "loss: 1.147298  [ 1520/ 3200]\n",
            "loss: 0.976016  [ 1536/ 3200]\n",
            "loss: 1.049447  [ 1552/ 3200]\n",
            "loss: 1.167661  [ 1568/ 3200]\n",
            "loss: 1.179944  [ 1584/ 3200]\n",
            "loss: 1.331362  [ 1600/ 3200]\n",
            "loss: 1.104912  [ 1616/ 3200]\n",
            "loss: 0.852652  [ 1632/ 3200]\n",
            "loss: 1.031332  [ 1648/ 3200]\n",
            "loss: 1.092690  [ 1664/ 3200]\n",
            "loss: 1.102422  [ 1680/ 3200]\n",
            "loss: 0.949730  [ 1696/ 3200]\n",
            "loss: 1.200716  [ 1712/ 3200]\n",
            "loss: 1.003075  [ 1728/ 3200]\n",
            "loss: 1.070893  [ 1744/ 3200]\n",
            "loss: 1.055081  [ 1760/ 3200]\n",
            "loss: 1.344613  [ 1776/ 3200]\n",
            "loss: 1.189320  [ 1792/ 3200]\n",
            "loss: 1.023748  [ 1808/ 3200]\n",
            "loss: 1.220158  [ 1824/ 3200]\n",
            "loss: 0.957777  [ 1840/ 3200]\n",
            "loss: 1.044774  [ 1856/ 3200]\n",
            "loss: 1.122518  [ 1872/ 3200]\n",
            "loss: 1.007700  [ 1888/ 3200]\n",
            "loss: 1.204366  [ 1904/ 3200]\n",
            "loss: 1.166116  [ 1920/ 3200]\n",
            "loss: 1.051500  [ 1936/ 3200]\n",
            "loss: 1.032082  [ 1952/ 3200]\n",
            "loss: 1.137338  [ 1968/ 3200]\n",
            "loss: 1.037704  [ 1984/ 3200]\n",
            "loss: 1.089015  [ 2000/ 3200]\n",
            "loss: 1.243055  [ 2016/ 3200]\n",
            "loss: 0.908688  [ 2032/ 3200]\n",
            "loss: 0.981694  [ 2048/ 3200]\n",
            "loss: 0.887274  [ 2064/ 3200]\n",
            "loss: 1.046736  [ 2080/ 3200]\n",
            "loss: 1.115334  [ 2096/ 3200]\n",
            "loss: 1.299905  [ 2112/ 3200]\n",
            "loss: 1.087553  [ 2128/ 3200]\n",
            "loss: 1.165530  [ 2144/ 3200]\n",
            "loss: 0.996743  [ 2160/ 3200]\n",
            "loss: 1.120808  [ 2176/ 3200]\n",
            "loss: 0.918455  [ 2192/ 3200]\n",
            "loss: 1.193038  [ 2208/ 3200]\n",
            "loss: 1.136243  [ 2224/ 3200]\n",
            "loss: 1.194953  [ 2240/ 3200]\n",
            "loss: 1.116850  [ 2256/ 3200]\n",
            "loss: 1.116788  [ 2272/ 3200]\n",
            "loss: 1.070919  [ 2288/ 3200]\n",
            "loss: 1.087494  [ 2304/ 3200]\n",
            "loss: 1.448492  [ 2320/ 3200]\n",
            "loss: 1.015483  [ 2336/ 3200]\n",
            "loss: 1.185439  [ 2352/ 3200]\n",
            "loss: 1.147657  [ 2368/ 3200]\n",
            "loss: 1.021492  [ 2384/ 3200]\n",
            "loss: 1.138198  [ 2400/ 3200]\n",
            "loss: 0.993502  [ 2416/ 3200]\n",
            "loss: 0.997495  [ 2432/ 3200]\n",
            "loss: 1.056448  [ 2448/ 3200]\n",
            "loss: 1.132490  [ 2464/ 3200]\n",
            "loss: 1.091819  [ 2480/ 3200]\n",
            "loss: 1.222672  [ 2496/ 3200]\n",
            "loss: 1.094518  [ 2512/ 3200]\n",
            "loss: 0.993281  [ 2528/ 3200]\n",
            "loss: 1.035976  [ 2544/ 3200]\n",
            "loss: 1.129042  [ 2560/ 3200]\n",
            "loss: 1.096292  [ 2576/ 3200]\n",
            "loss: 1.055048  [ 2592/ 3200]\n",
            "loss: 1.270749  [ 2608/ 3200]\n",
            "loss: 1.085843  [ 2624/ 3200]\n",
            "loss: 1.261271  [ 2640/ 3200]\n",
            "loss: 1.045862  [ 2656/ 3200]\n",
            "loss: 0.981330  [ 2672/ 3200]\n",
            "loss: 1.064644  [ 2688/ 3200]\n",
            "loss: 1.219189  [ 2704/ 3200]\n",
            "loss: 1.098900  [ 2720/ 3200]\n",
            "loss: 1.300494  [ 2736/ 3200]\n",
            "loss: 1.053365  [ 2752/ 3200]\n",
            "loss: 1.048792  [ 2768/ 3200]\n",
            "loss: 1.136701  [ 2784/ 3200]\n",
            "loss: 1.183893  [ 2800/ 3200]\n",
            "loss: 1.006976  [ 2816/ 3200]\n",
            "loss: 1.129975  [ 2832/ 3200]\n",
            "loss: 1.073189  [ 2848/ 3200]\n",
            "loss: 1.062573  [ 2864/ 3200]\n",
            "loss: 0.977103  [ 2880/ 3200]\n",
            "loss: 0.943385  [ 2896/ 3200]\n",
            "loss: 1.194985  [ 2912/ 3200]\n",
            "loss: 1.274163  [ 2928/ 3200]\n",
            "loss: 1.124856  [ 2944/ 3200]\n",
            "loss: 1.098889  [ 2960/ 3200]\n",
            "loss: 1.123547  [ 2976/ 3200]\n",
            "loss: 1.062874  [ 2992/ 3200]\n",
            "loss: 1.035984  [ 3008/ 3200]\n",
            "loss: 0.911899  [ 3024/ 3200]\n",
            "loss: 1.039344  [ 3040/ 3200]\n",
            "loss: 1.084249  [ 3056/ 3200]\n",
            "loss: 1.021832  [ 3072/ 3200]\n",
            "loss: 1.276199  [ 3088/ 3200]\n",
            "loss: 1.045105  [ 3104/ 3200]\n",
            "loss: 1.057863  [ 3120/ 3200]\n",
            "loss: 1.012106  [ 3136/ 3200]\n",
            "loss: 1.075068  [ 3152/ 3200]\n",
            "loss: 1.051146  [ 3168/ 3200]\n",
            "loss: 1.402968  [ 3184/ 3200]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.013112  [    0/ 3200]\n",
            "loss: 1.116221  [   16/ 3200]\n",
            "loss: 1.075602  [   32/ 3200]\n",
            "loss: 1.102807  [   48/ 3200]\n",
            "loss: 1.087221  [   64/ 3200]\n",
            "loss: 1.093920  [   80/ 3200]\n",
            "loss: 1.145894  [   96/ 3200]\n",
            "loss: 1.191362  [  112/ 3200]\n",
            "loss: 1.281903  [  128/ 3200]\n",
            "loss: 1.154825  [  144/ 3200]\n",
            "loss: 1.008563  [  160/ 3200]\n",
            "loss: 1.131367  [  176/ 3200]\n",
            "loss: 1.036896  [  192/ 3200]\n",
            "loss: 0.976826  [  208/ 3200]\n",
            "loss: 0.875909  [  224/ 3200]\n",
            "loss: 1.135444  [  240/ 3200]\n",
            "loss: 1.105914  [  256/ 3200]\n",
            "loss: 1.031405  [  272/ 3200]\n",
            "loss: 1.189201  [  288/ 3200]\n",
            "loss: 1.266685  [  304/ 3200]\n",
            "loss: 1.150789  [  320/ 3200]\n",
            "loss: 0.965088  [  336/ 3200]\n",
            "loss: 1.065699  [  352/ 3200]\n",
            "loss: 0.962626  [  368/ 3200]\n",
            "loss: 1.070126  [  384/ 3200]\n",
            "loss: 0.996428  [  400/ 3200]\n",
            "loss: 1.206413  [  416/ 3200]\n",
            "loss: 1.115417  [  432/ 3200]\n",
            "loss: 1.125901  [  448/ 3200]\n",
            "loss: 0.898698  [  464/ 3200]\n",
            "loss: 1.177557  [  480/ 3200]\n",
            "loss: 1.146481  [  496/ 3200]\n",
            "loss: 1.081964  [  512/ 3200]\n",
            "loss: 1.013177  [  528/ 3200]\n",
            "loss: 1.070388  [  544/ 3200]\n",
            "loss: 1.257636  [  560/ 3200]\n",
            "loss: 0.996933  [  576/ 3200]\n",
            "loss: 1.083780  [  592/ 3200]\n",
            "loss: 1.032276  [  608/ 3200]\n",
            "loss: 1.091862  [  624/ 3200]\n",
            "loss: 1.035487  [  640/ 3200]\n",
            "loss: 1.094674  [  656/ 3200]\n",
            "loss: 1.168567  [  672/ 3200]\n",
            "loss: 1.042151  [  688/ 3200]\n",
            "loss: 1.133358  [  704/ 3200]\n",
            "loss: 1.138919  [  720/ 3200]\n",
            "loss: 1.033417  [  736/ 3200]\n",
            "loss: 1.160656  [  752/ 3200]\n",
            "loss: 1.196160  [  768/ 3200]\n",
            "loss: 1.037874  [  784/ 3200]\n",
            "loss: 1.147885  [  800/ 3200]\n",
            "loss: 1.133053  [  816/ 3200]\n",
            "loss: 1.032914  [  832/ 3200]\n",
            "loss: 1.319449  [  848/ 3200]\n",
            "loss: 0.930711  [  864/ 3200]\n",
            "loss: 1.159131  [  880/ 3200]\n",
            "loss: 1.164816  [  896/ 3200]\n",
            "loss: 0.853075  [  912/ 3200]\n",
            "loss: 1.076575  [  928/ 3200]\n",
            "loss: 1.061971  [  944/ 3200]\n",
            "loss: 1.115836  [  960/ 3200]\n",
            "loss: 0.996575  [  976/ 3200]\n",
            "loss: 0.960541  [  992/ 3200]\n",
            "loss: 1.149679  [ 1008/ 3200]\n",
            "loss: 1.210304  [ 1024/ 3200]\n",
            "loss: 1.089867  [ 1040/ 3200]\n",
            "loss: 1.063842  [ 1056/ 3200]\n",
            "loss: 1.135904  [ 1072/ 3200]\n",
            "loss: 1.107306  [ 1088/ 3200]\n",
            "loss: 1.393389  [ 1104/ 3200]\n",
            "loss: 1.054850  [ 1120/ 3200]\n",
            "loss: 1.052251  [ 1136/ 3200]\n",
            "loss: 1.096955  [ 1152/ 3200]\n",
            "loss: 1.251570  [ 1168/ 3200]\n",
            "loss: 0.733225  [ 1184/ 3200]\n",
            "loss: 1.021207  [ 1200/ 3200]\n",
            "loss: 1.135112  [ 1216/ 3200]\n",
            "loss: 0.852328  [ 1232/ 3200]\n",
            "loss: 1.036061  [ 1248/ 3200]\n",
            "loss: 1.185357  [ 1264/ 3200]\n",
            "loss: 1.127055  [ 1280/ 3200]\n",
            "loss: 0.822543  [ 1296/ 3200]\n",
            "loss: 1.024443  [ 1312/ 3200]\n",
            "loss: 1.057359  [ 1328/ 3200]\n",
            "loss: 0.983107  [ 1344/ 3200]\n",
            "loss: 1.093929  [ 1360/ 3200]\n",
            "loss: 1.154940  [ 1376/ 3200]\n",
            "loss: 1.116869  [ 1392/ 3200]\n",
            "loss: 1.079209  [ 1408/ 3200]\n",
            "loss: 1.154674  [ 1424/ 3200]\n",
            "loss: 1.163185  [ 1440/ 3200]\n",
            "loss: 1.161023  [ 1456/ 3200]\n",
            "loss: 1.378164  [ 1472/ 3200]\n",
            "loss: 1.010999  [ 1488/ 3200]\n",
            "loss: 1.187357  [ 1504/ 3200]\n",
            "loss: 0.971095  [ 1520/ 3200]\n",
            "loss: 0.932603  [ 1536/ 3200]\n",
            "loss: 1.157640  [ 1552/ 3200]\n",
            "loss: 1.069602  [ 1568/ 3200]\n",
            "loss: 1.162733  [ 1584/ 3200]\n",
            "loss: 1.261024  [ 1600/ 3200]\n",
            "loss: 1.005042  [ 1616/ 3200]\n",
            "loss: 0.962905  [ 1632/ 3200]\n",
            "loss: 1.025434  [ 1648/ 3200]\n",
            "loss: 1.111200  [ 1664/ 3200]\n",
            "loss: 1.134148  [ 1680/ 3200]\n",
            "loss: 1.247677  [ 1696/ 3200]\n",
            "loss: 0.963683  [ 1712/ 3200]\n",
            "loss: 0.972919  [ 1728/ 3200]\n",
            "loss: 1.113808  [ 1744/ 3200]\n",
            "loss: 0.854731  [ 1760/ 3200]\n",
            "loss: 1.237712  [ 1776/ 3200]\n",
            "loss: 1.094666  [ 1792/ 3200]\n",
            "loss: 1.149652  [ 1808/ 3200]\n",
            "loss: 1.084272  [ 1824/ 3200]\n",
            "loss: 1.043844  [ 1840/ 3200]\n",
            "loss: 1.018188  [ 1856/ 3200]\n",
            "loss: 0.949932  [ 1872/ 3200]\n",
            "loss: 1.324960  [ 1888/ 3200]\n",
            "loss: 0.952596  [ 1904/ 3200]\n",
            "loss: 1.071752  [ 1920/ 3200]\n",
            "loss: 0.860289  [ 1936/ 3200]\n",
            "loss: 1.156378  [ 1952/ 3200]\n",
            "loss: 0.980870  [ 1968/ 3200]\n",
            "loss: 0.974074  [ 1984/ 3200]\n",
            "loss: 1.047843  [ 2000/ 3200]\n",
            "loss: 1.184121  [ 2016/ 3200]\n",
            "loss: 1.311856  [ 2032/ 3200]\n",
            "loss: 1.043103  [ 2048/ 3200]\n",
            "loss: 1.036840  [ 2064/ 3200]\n",
            "loss: 1.162746  [ 2080/ 3200]\n",
            "loss: 1.280591  [ 2096/ 3200]\n",
            "loss: 1.202722  [ 2112/ 3200]\n",
            "loss: 1.041953  [ 2128/ 3200]\n",
            "loss: 0.975775  [ 2144/ 3200]\n",
            "loss: 1.012907  [ 2160/ 3200]\n",
            "loss: 1.063003  [ 2176/ 3200]\n",
            "loss: 1.016769  [ 2192/ 3200]\n",
            "loss: 1.123592  [ 2208/ 3200]\n",
            "loss: 1.225387  [ 2224/ 3200]\n",
            "loss: 1.091011  [ 2240/ 3200]\n",
            "loss: 1.046819  [ 2256/ 3200]\n",
            "loss: 1.123878  [ 2272/ 3200]\n",
            "loss: 0.855581  [ 2288/ 3200]\n",
            "loss: 1.053666  [ 2304/ 3200]\n",
            "loss: 1.045244  [ 2320/ 3200]\n",
            "loss: 1.483117  [ 2336/ 3200]\n",
            "loss: 1.077103  [ 2352/ 3200]\n",
            "loss: 1.148842  [ 2368/ 3200]\n",
            "loss: 0.995885  [ 2384/ 3200]\n",
            "loss: 1.091973  [ 2400/ 3200]\n",
            "loss: 1.043515  [ 2416/ 3200]\n",
            "loss: 1.106653  [ 2432/ 3200]\n",
            "loss: 1.078547  [ 2448/ 3200]\n",
            "loss: 0.985863  [ 2464/ 3200]\n",
            "loss: 1.067160  [ 2480/ 3200]\n",
            "loss: 0.886668  [ 2496/ 3200]\n",
            "loss: 1.178109  [ 2512/ 3200]\n",
            "loss: 0.923539  [ 2528/ 3200]\n",
            "loss: 1.085739  [ 2544/ 3200]\n",
            "loss: 1.244660  [ 2560/ 3200]\n",
            "loss: 1.001480  [ 2576/ 3200]\n",
            "loss: 0.961511  [ 2592/ 3200]\n",
            "loss: 1.182052  [ 2608/ 3200]\n",
            "loss: 1.151509  [ 2624/ 3200]\n",
            "loss: 0.995570  [ 2640/ 3200]\n",
            "loss: 1.054429  [ 2656/ 3200]\n",
            "loss: 0.988386  [ 2672/ 3200]\n",
            "loss: 1.125476  [ 2688/ 3200]\n",
            "loss: 0.926127  [ 2704/ 3200]\n",
            "loss: 1.124543  [ 2720/ 3200]\n",
            "loss: 1.098511  [ 2736/ 3200]\n",
            "loss: 1.207112  [ 2752/ 3200]\n",
            "loss: 1.125098  [ 2768/ 3200]\n",
            "loss: 0.929893  [ 2784/ 3200]\n",
            "loss: 1.049060  [ 2800/ 3200]\n",
            "loss: 0.918184  [ 2816/ 3200]\n",
            "loss: 1.153529  [ 2832/ 3200]\n",
            "loss: 1.018167  [ 2848/ 3200]\n",
            "loss: 1.009099  [ 2864/ 3200]\n",
            "loss: 0.885282  [ 2880/ 3200]\n",
            "loss: 1.123251  [ 2896/ 3200]\n",
            "loss: 1.013099  [ 2912/ 3200]\n",
            "loss: 1.074612  [ 2928/ 3200]\n",
            "loss: 1.131705  [ 2944/ 3200]\n",
            "loss: 1.241994  [ 2960/ 3200]\n",
            "loss: 1.127888  [ 2976/ 3200]\n",
            "loss: 1.088375  [ 2992/ 3200]\n",
            "loss: 1.210934  [ 3008/ 3200]\n",
            "loss: 1.102465  [ 3024/ 3200]\n",
            "loss: 1.013595  [ 3040/ 3200]\n",
            "loss: 0.940226  [ 3056/ 3200]\n",
            "loss: 1.118835  [ 3072/ 3200]\n",
            "loss: 1.214587  [ 3088/ 3200]\n",
            "loss: 1.211550  [ 3104/ 3200]\n",
            "loss: 1.312621  [ 3120/ 3200]\n",
            "loss: 1.097541  [ 3136/ 3200]\n",
            "loss: 1.049763  [ 3152/ 3200]\n",
            "loss: 0.983261  [ 3168/ 3200]\n",
            "loss: 0.952585  [ 3184/ 3200]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.044557  [    0/ 3200]\n",
            "loss: 1.033075  [   16/ 3200]\n",
            "loss: 1.132763  [   32/ 3200]\n",
            "loss: 1.143737  [   48/ 3200]\n",
            "loss: 1.146737  [   64/ 3200]\n",
            "loss: 1.089640  [   80/ 3200]\n",
            "loss: 1.223176  [   96/ 3200]\n",
            "loss: 1.102386  [  112/ 3200]\n",
            "loss: 1.074281  [  128/ 3200]\n",
            "loss: 1.276625  [  144/ 3200]\n",
            "loss: 1.111402  [  160/ 3200]\n",
            "loss: 1.184093  [  176/ 3200]\n",
            "loss: 1.054818  [  192/ 3200]\n",
            "loss: 0.984916  [  208/ 3200]\n",
            "loss: 1.099386  [  224/ 3200]\n",
            "loss: 1.041556  [  240/ 3200]\n",
            "loss: 1.235155  [  256/ 3200]\n",
            "loss: 0.808529  [  272/ 3200]\n",
            "loss: 1.282275  [  288/ 3200]\n",
            "loss: 1.004112  [  304/ 3200]\n",
            "loss: 0.970917  [  320/ 3200]\n",
            "loss: 1.121950  [  336/ 3200]\n",
            "loss: 1.121785  [  352/ 3200]\n",
            "loss: 1.182125  [  368/ 3200]\n",
            "loss: 0.913217  [  384/ 3200]\n",
            "loss: 1.040498  [  400/ 3200]\n",
            "loss: 1.153680  [  416/ 3200]\n",
            "loss: 1.118404  [  432/ 3200]\n",
            "loss: 1.261465  [  448/ 3200]\n",
            "loss: 1.291316  [  464/ 3200]\n",
            "loss: 1.217355  [  480/ 3200]\n",
            "loss: 0.984193  [  496/ 3200]\n",
            "loss: 1.079367  [  512/ 3200]\n",
            "loss: 1.126203  [  528/ 3200]\n",
            "loss: 1.260926  [  544/ 3200]\n",
            "loss: 1.057498  [  560/ 3200]\n",
            "loss: 1.199827  [  576/ 3200]\n",
            "loss: 0.895793  [  592/ 3200]\n",
            "loss: 1.169928  [  608/ 3200]\n",
            "loss: 1.143837  [  624/ 3200]\n",
            "loss: 1.001682  [  640/ 3200]\n",
            "loss: 0.867891  [  656/ 3200]\n",
            "loss: 1.029974  [  672/ 3200]\n",
            "loss: 0.981007  [  688/ 3200]\n",
            "loss: 1.068730  [  704/ 3200]\n",
            "loss: 0.966040  [  720/ 3200]\n",
            "loss: 1.060861  [  736/ 3200]\n",
            "loss: 1.282539  [  752/ 3200]\n",
            "loss: 1.445954  [  768/ 3200]\n",
            "loss: 1.017200  [  784/ 3200]\n",
            "loss: 1.081791  [  800/ 3200]\n",
            "loss: 1.040660  [  816/ 3200]\n",
            "loss: 1.120981  [  832/ 3200]\n",
            "loss: 1.208503  [  848/ 3200]\n",
            "loss: 0.948150  [  864/ 3200]\n",
            "loss: 1.089379  [  880/ 3200]\n",
            "loss: 0.989948  [  896/ 3200]\n",
            "loss: 1.080860  [  912/ 3200]\n",
            "loss: 1.169681  [  928/ 3200]\n",
            "loss: 1.076626  [  944/ 3200]\n",
            "loss: 0.968759  [  960/ 3200]\n",
            "loss: 1.230685  [  976/ 3200]\n",
            "loss: 1.077665  [  992/ 3200]\n",
            "loss: 1.118958  [ 1008/ 3200]\n",
            "loss: 1.035126  [ 1024/ 3200]\n",
            "loss: 1.122584  [ 1040/ 3200]\n",
            "loss: 1.143911  [ 1056/ 3200]\n",
            "loss: 0.985731  [ 1072/ 3200]\n",
            "loss: 1.004557  [ 1088/ 3200]\n",
            "loss: 0.903968  [ 1104/ 3200]\n",
            "loss: 1.144346  [ 1120/ 3200]\n",
            "loss: 1.279954  [ 1136/ 3200]\n",
            "loss: 0.922134  [ 1152/ 3200]\n",
            "loss: 1.042191  [ 1168/ 3200]\n",
            "loss: 0.891094  [ 1184/ 3200]\n",
            "loss: 1.020863  [ 1200/ 3200]\n",
            "loss: 0.949781  [ 1216/ 3200]\n",
            "loss: 1.052952  [ 1232/ 3200]\n",
            "loss: 1.071983  [ 1248/ 3200]\n",
            "loss: 1.020198  [ 1264/ 3200]\n",
            "loss: 0.974174  [ 1280/ 3200]\n",
            "loss: 0.986329  [ 1296/ 3200]\n",
            "loss: 0.935574  [ 1312/ 3200]\n",
            "loss: 1.206332  [ 1328/ 3200]\n",
            "loss: 1.121405  [ 1344/ 3200]\n",
            "loss: 1.116768  [ 1360/ 3200]\n",
            "loss: 1.170271  [ 1376/ 3200]\n",
            "loss: 0.958262  [ 1392/ 3200]\n",
            "loss: 1.062839  [ 1408/ 3200]\n",
            "loss: 0.941407  [ 1424/ 3200]\n",
            "loss: 1.070331  [ 1440/ 3200]\n",
            "loss: 1.113685  [ 1456/ 3200]\n",
            "loss: 1.063714  [ 1472/ 3200]\n",
            "loss: 1.237724  [ 1488/ 3200]\n",
            "loss: 0.888504  [ 1504/ 3200]\n",
            "loss: 1.257711  [ 1520/ 3200]\n",
            "loss: 0.924404  [ 1536/ 3200]\n",
            "loss: 0.962789  [ 1552/ 3200]\n",
            "loss: 1.008884  [ 1568/ 3200]\n",
            "loss: 1.054842  [ 1584/ 3200]\n",
            "loss: 1.115219  [ 1600/ 3200]\n",
            "loss: 0.947984  [ 1616/ 3200]\n",
            "loss: 1.120504  [ 1632/ 3200]\n",
            "loss: 0.975968  [ 1648/ 3200]\n",
            "loss: 1.122447  [ 1664/ 3200]\n",
            "loss: 1.121701  [ 1680/ 3200]\n",
            "loss: 0.963159  [ 1696/ 3200]\n",
            "loss: 1.015934  [ 1712/ 3200]\n",
            "loss: 1.080925  [ 1728/ 3200]\n",
            "loss: 1.219569  [ 1744/ 3200]\n",
            "loss: 1.014936  [ 1760/ 3200]\n",
            "loss: 0.989226  [ 1776/ 3200]\n",
            "loss: 1.189820  [ 1792/ 3200]\n",
            "loss: 0.950854  [ 1808/ 3200]\n",
            "loss: 0.973886  [ 1824/ 3200]\n",
            "loss: 1.358929  [ 1840/ 3200]\n",
            "loss: 0.940505  [ 1856/ 3200]\n",
            "loss: 1.128358  [ 1872/ 3200]\n",
            "loss: 1.119532  [ 1888/ 3200]\n",
            "loss: 1.155008  [ 1904/ 3200]\n",
            "loss: 0.962466  [ 1920/ 3200]\n",
            "loss: 1.298754  [ 1936/ 3200]\n",
            "loss: 0.983994  [ 1952/ 3200]\n",
            "loss: 0.892289  [ 1968/ 3200]\n",
            "loss: 0.996158  [ 1984/ 3200]\n",
            "loss: 0.979350  [ 2000/ 3200]\n",
            "loss: 1.111047  [ 2016/ 3200]\n",
            "loss: 1.017959  [ 2032/ 3200]\n",
            "loss: 1.175489  [ 2048/ 3200]\n",
            "loss: 1.134695  [ 2064/ 3200]\n",
            "loss: 0.950950  [ 2080/ 3200]\n",
            "loss: 0.933679  [ 2096/ 3200]\n",
            "loss: 1.012198  [ 2112/ 3200]\n",
            "loss: 1.042487  [ 2128/ 3200]\n",
            "loss: 1.102230  [ 2144/ 3200]\n",
            "loss: 1.060556  [ 2160/ 3200]\n",
            "loss: 1.240020  [ 2176/ 3200]\n",
            "loss: 1.097904  [ 2192/ 3200]\n",
            "loss: 1.081749  [ 2208/ 3200]\n",
            "loss: 1.079590  [ 2224/ 3200]\n",
            "loss: 1.042778  [ 2240/ 3200]\n",
            "loss: 1.033081  [ 2256/ 3200]\n",
            "loss: 1.120598  [ 2272/ 3200]\n",
            "loss: 1.088221  [ 2288/ 3200]\n",
            "loss: 1.201424  [ 2304/ 3200]\n",
            "loss: 1.245694  [ 2320/ 3200]\n",
            "loss: 0.991876  [ 2336/ 3200]\n",
            "loss: 0.928760  [ 2352/ 3200]\n",
            "loss: 1.134382  [ 2368/ 3200]\n",
            "loss: 1.121872  [ 2384/ 3200]\n",
            "loss: 1.246623  [ 2400/ 3200]\n",
            "loss: 1.008324  [ 2416/ 3200]\n",
            "loss: 1.164410  [ 2432/ 3200]\n",
            "loss: 0.822344  [ 2448/ 3200]\n",
            "loss: 0.972175  [ 2464/ 3200]\n",
            "loss: 1.088841  [ 2480/ 3200]\n",
            "loss: 1.001836  [ 2496/ 3200]\n",
            "loss: 1.177507  [ 2512/ 3200]\n",
            "loss: 1.070415  [ 2528/ 3200]\n",
            "loss: 1.051213  [ 2544/ 3200]\n",
            "loss: 1.121247  [ 2560/ 3200]\n",
            "loss: 1.101660  [ 2576/ 3200]\n",
            "loss: 1.108870  [ 2592/ 3200]\n",
            "loss: 0.937351  [ 2608/ 3200]\n",
            "loss: 0.940597  [ 2624/ 3200]\n",
            "loss: 1.003522  [ 2640/ 3200]\n",
            "loss: 1.156176  [ 2656/ 3200]\n",
            "loss: 0.957724  [ 2672/ 3200]\n",
            "loss: 0.807474  [ 2688/ 3200]\n",
            "loss: 1.091074  [ 2704/ 3200]\n",
            "loss: 0.915221  [ 2720/ 3200]\n",
            "loss: 1.105261  [ 2736/ 3200]\n",
            "loss: 1.150358  [ 2752/ 3200]\n",
            "loss: 1.168536  [ 2768/ 3200]\n",
            "loss: 1.177844  [ 2784/ 3200]\n",
            "loss: 1.054367  [ 2800/ 3200]\n",
            "loss: 1.234568  [ 2816/ 3200]\n",
            "loss: 1.114587  [ 2832/ 3200]\n",
            "loss: 1.009257  [ 2848/ 3200]\n",
            "loss: 1.134849  [ 2864/ 3200]\n",
            "loss: 0.903831  [ 2880/ 3200]\n",
            "loss: 0.881071  [ 2896/ 3200]\n",
            "loss: 1.055357  [ 2912/ 3200]\n",
            "loss: 1.067349  [ 2928/ 3200]\n",
            "loss: 1.073040  [ 2944/ 3200]\n",
            "loss: 0.758419  [ 2960/ 3200]\n",
            "loss: 0.982957  [ 2976/ 3200]\n",
            "loss: 0.991053  [ 2992/ 3200]\n",
            "loss: 0.919523  [ 3008/ 3200]\n",
            "loss: 0.931028  [ 3024/ 3200]\n",
            "loss: 1.242454  [ 3040/ 3200]\n",
            "loss: 0.822605  [ 3056/ 3200]\n",
            "loss: 0.966480  [ 3072/ 3200]\n",
            "loss: 1.171410  [ 3088/ 3200]\n",
            "loss: 1.254980  [ 3104/ 3200]\n",
            "loss: 0.981724  [ 3120/ 3200]\n",
            "loss: 1.079941  [ 3136/ 3200]\n",
            "loss: 1.078569  [ 3152/ 3200]\n",
            "loss: 0.949845  [ 3168/ 3200]\n",
            "loss: 1.350166  [ 3184/ 3200]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.039988  [    0/ 3200]\n",
            "loss: 1.265711  [   16/ 3200]\n",
            "loss: 1.013668  [   32/ 3200]\n",
            "loss: 1.030146  [   48/ 3200]\n",
            "loss: 0.990233  [   64/ 3200]\n",
            "loss: 1.068472  [   80/ 3200]\n",
            "loss: 0.873214  [   96/ 3200]\n",
            "loss: 1.103090  [  112/ 3200]\n",
            "loss: 0.838094  [  128/ 3200]\n",
            "loss: 1.101550  [  144/ 3200]\n",
            "loss: 1.001595  [  160/ 3200]\n",
            "loss: 0.883429  [  176/ 3200]\n",
            "loss: 1.155964  [  192/ 3200]\n",
            "loss: 0.997375  [  208/ 3200]\n",
            "loss: 0.919184  [  224/ 3200]\n",
            "loss: 1.024101  [  240/ 3200]\n",
            "loss: 1.097348  [  256/ 3200]\n",
            "loss: 1.079416  [  272/ 3200]\n",
            "loss: 1.067174  [  288/ 3200]\n",
            "loss: 1.235693  [  304/ 3200]\n",
            "loss: 0.781491  [  320/ 3200]\n",
            "loss: 1.156389  [  336/ 3200]\n",
            "loss: 0.946819  [  352/ 3200]\n",
            "loss: 0.916247  [  368/ 3200]\n",
            "loss: 0.935959  [  384/ 3200]\n",
            "loss: 1.011942  [  400/ 3200]\n",
            "loss: 1.088953  [  416/ 3200]\n",
            "loss: 1.092077  [  432/ 3200]\n",
            "loss: 1.140137  [  448/ 3200]\n",
            "loss: 0.888072  [  464/ 3200]\n",
            "loss: 0.888335  [  480/ 3200]\n",
            "loss: 0.938513  [  496/ 3200]\n",
            "loss: 1.006108  [  512/ 3200]\n",
            "loss: 1.160173  [  528/ 3200]\n",
            "loss: 1.110296  [  544/ 3200]\n",
            "loss: 1.080226  [  560/ 3200]\n",
            "loss: 1.132318  [  576/ 3200]\n",
            "loss: 1.087012  [  592/ 3200]\n",
            "loss: 1.058000  [  608/ 3200]\n",
            "loss: 1.167142  [  624/ 3200]\n",
            "loss: 1.083817  [  640/ 3200]\n",
            "loss: 1.015186  [  656/ 3200]\n",
            "loss: 0.868972  [  672/ 3200]\n",
            "loss: 1.061538  [  688/ 3200]\n",
            "loss: 1.062703  [  704/ 3200]\n",
            "loss: 1.237806  [  720/ 3200]\n",
            "loss: 1.120987  [  736/ 3200]\n",
            "loss: 1.285504  [  752/ 3200]\n",
            "loss: 1.087969  [  768/ 3200]\n",
            "loss: 1.049298  [  784/ 3200]\n",
            "loss: 1.026698  [  800/ 3200]\n",
            "loss: 0.953243  [  816/ 3200]\n",
            "loss: 1.006044  [  832/ 3200]\n",
            "loss: 1.146967  [  848/ 3200]\n",
            "loss: 0.989077  [  864/ 3200]\n",
            "loss: 1.085447  [  880/ 3200]\n",
            "loss: 0.948899  [  896/ 3200]\n",
            "loss: 1.572881  [  912/ 3200]\n",
            "loss: 1.148853  [  928/ 3200]\n",
            "loss: 1.003565  [  944/ 3200]\n",
            "loss: 1.194431  [  960/ 3200]\n",
            "loss: 0.771375  [  976/ 3200]\n",
            "loss: 0.941518  [  992/ 3200]\n",
            "loss: 1.162986  [ 1008/ 3200]\n",
            "loss: 0.961394  [ 1024/ 3200]\n",
            "loss: 1.015811  [ 1040/ 3200]\n",
            "loss: 0.946420  [ 1056/ 3200]\n",
            "loss: 1.099312  [ 1072/ 3200]\n",
            "loss: 1.000472  [ 1088/ 3200]\n",
            "loss: 1.068081  [ 1104/ 3200]\n",
            "loss: 1.025659  [ 1120/ 3200]\n",
            "loss: 1.009346  [ 1136/ 3200]\n",
            "loss: 0.999767  [ 1152/ 3200]\n",
            "loss: 1.066433  [ 1168/ 3200]\n",
            "loss: 1.099612  [ 1184/ 3200]\n",
            "loss: 1.242794  [ 1200/ 3200]\n",
            "loss: 1.142754  [ 1216/ 3200]\n",
            "loss: 1.226334  [ 1232/ 3200]\n",
            "loss: 1.237450  [ 1248/ 3200]\n",
            "loss: 1.120393  [ 1264/ 3200]\n",
            "loss: 0.854820  [ 1280/ 3200]\n",
            "loss: 0.950331  [ 1296/ 3200]\n",
            "loss: 0.811165  [ 1312/ 3200]\n",
            "loss: 0.934980  [ 1328/ 3200]\n",
            "loss: 0.995986  [ 1344/ 3200]\n",
            "loss: 1.233760  [ 1360/ 3200]\n",
            "loss: 1.078469  [ 1376/ 3200]\n",
            "loss: 1.056175  [ 1392/ 3200]\n",
            "loss: 1.203158  [ 1408/ 3200]\n",
            "loss: 0.937927  [ 1424/ 3200]\n",
            "loss: 1.144591  [ 1440/ 3200]\n",
            "loss: 1.365671  [ 1456/ 3200]\n",
            "loss: 1.016411  [ 1472/ 3200]\n",
            "loss: 1.117992  [ 1488/ 3200]\n",
            "loss: 1.048306  [ 1504/ 3200]\n",
            "loss: 1.107111  [ 1520/ 3200]\n",
            "loss: 1.187070  [ 1536/ 3200]\n",
            "loss: 1.068885  [ 1552/ 3200]\n",
            "loss: 0.821653  [ 1568/ 3200]\n",
            "loss: 0.938994  [ 1584/ 3200]\n",
            "loss: 1.050159  [ 1600/ 3200]\n",
            "loss: 1.108952  [ 1616/ 3200]\n",
            "loss: 0.778026  [ 1632/ 3200]\n",
            "loss: 1.001655  [ 1648/ 3200]\n",
            "loss: 1.040868  [ 1664/ 3200]\n",
            "loss: 1.173846  [ 1680/ 3200]\n",
            "loss: 1.087880  [ 1696/ 3200]\n",
            "loss: 0.981572  [ 1712/ 3200]\n",
            "loss: 1.083019  [ 1728/ 3200]\n",
            "loss: 1.082176  [ 1744/ 3200]\n",
            "loss: 1.071149  [ 1760/ 3200]\n",
            "loss: 1.107026  [ 1776/ 3200]\n",
            "loss: 0.902849  [ 1792/ 3200]\n",
            "loss: 1.294353  [ 1808/ 3200]\n",
            "loss: 0.919047  [ 1824/ 3200]\n",
            "loss: 1.185628  [ 1840/ 3200]\n",
            "loss: 1.131228  [ 1856/ 3200]\n",
            "loss: 0.951276  [ 1872/ 3200]\n",
            "loss: 0.972885  [ 1888/ 3200]\n",
            "loss: 0.947393  [ 1904/ 3200]\n",
            "loss: 1.032028  [ 1920/ 3200]\n",
            "loss: 1.080502  [ 1936/ 3200]\n",
            "loss: 0.998987  [ 1952/ 3200]\n",
            "loss: 0.829065  [ 1968/ 3200]\n",
            "loss: 0.975356  [ 1984/ 3200]\n",
            "loss: 1.091168  [ 2000/ 3200]\n",
            "loss: 1.149195  [ 2016/ 3200]\n",
            "loss: 1.053108  [ 2032/ 3200]\n",
            "loss: 1.290695  [ 2048/ 3200]\n",
            "loss: 0.921757  [ 2064/ 3200]\n",
            "loss: 1.038555  [ 2080/ 3200]\n",
            "loss: 1.045631  [ 2096/ 3200]\n",
            "loss: 1.062949  [ 2112/ 3200]\n",
            "loss: 0.891919  [ 2128/ 3200]\n",
            "loss: 1.122777  [ 2144/ 3200]\n",
            "loss: 1.016272  [ 2160/ 3200]\n",
            "loss: 1.048564  [ 2176/ 3200]\n",
            "loss: 0.973652  [ 2192/ 3200]\n",
            "loss: 0.883307  [ 2208/ 3200]\n",
            "loss: 1.094240  [ 2224/ 3200]\n",
            "loss: 1.043275  [ 2240/ 3200]\n",
            "loss: 0.973399  [ 2256/ 3200]\n",
            "loss: 1.335675  [ 2272/ 3200]\n",
            "loss: 1.114674  [ 2288/ 3200]\n",
            "loss: 0.984704  [ 2304/ 3200]\n",
            "loss: 1.066556  [ 2320/ 3200]\n",
            "loss: 1.108020  [ 2336/ 3200]\n",
            "loss: 1.298640  [ 2352/ 3200]\n",
            "loss: 0.996309  [ 2368/ 3200]\n",
            "loss: 0.850470  [ 2384/ 3200]\n",
            "loss: 0.906598  [ 2400/ 3200]\n",
            "loss: 1.009112  [ 2416/ 3200]\n",
            "loss: 0.883887  [ 2432/ 3200]\n",
            "loss: 1.044239  [ 2448/ 3200]\n",
            "loss: 1.122316  [ 2464/ 3200]\n",
            "loss: 1.068011  [ 2480/ 3200]\n",
            "loss: 0.914100  [ 2496/ 3200]\n",
            "loss: 1.256832  [ 2512/ 3200]\n",
            "loss: 0.962417  [ 2528/ 3200]\n",
            "loss: 1.364329  [ 2544/ 3200]\n",
            "loss: 1.085753  [ 2560/ 3200]\n",
            "loss: 0.982621  [ 2576/ 3200]\n",
            "loss: 1.432209  [ 2592/ 3200]\n",
            "loss: 1.040991  [ 2608/ 3200]\n",
            "loss: 0.941627  [ 2624/ 3200]\n",
            "loss: 1.294224  [ 2640/ 3200]\n",
            "loss: 1.320741  [ 2656/ 3200]\n",
            "loss: 1.206166  [ 2672/ 3200]\n",
            "loss: 1.050990  [ 2688/ 3200]\n",
            "loss: 1.240254  [ 2704/ 3200]\n",
            "loss: 1.046954  [ 2720/ 3200]\n",
            "loss: 1.077348  [ 2736/ 3200]\n",
            "loss: 0.877537  [ 2752/ 3200]\n",
            "loss: 1.142389  [ 2768/ 3200]\n",
            "loss: 0.964920  [ 2784/ 3200]\n",
            "loss: 1.221015  [ 2800/ 3200]\n",
            "loss: 1.069100  [ 2816/ 3200]\n",
            "loss: 1.121099  [ 2832/ 3200]\n",
            "loss: 0.850305  [ 2848/ 3200]\n",
            "loss: 0.990836  [ 2864/ 3200]\n",
            "loss: 1.011304  [ 2880/ 3200]\n",
            "loss: 1.103027  [ 2896/ 3200]\n",
            "loss: 1.012244  [ 2912/ 3200]\n",
            "loss: 1.355560  [ 2928/ 3200]\n",
            "loss: 1.130130  [ 2944/ 3200]\n",
            "loss: 1.038279  [ 2960/ 3200]\n",
            "loss: 0.949686  [ 2976/ 3200]\n",
            "loss: 1.041858  [ 2992/ 3200]\n",
            "loss: 1.045994  [ 3008/ 3200]\n",
            "loss: 1.003550  [ 3024/ 3200]\n",
            "loss: 0.995125  [ 3040/ 3200]\n",
            "loss: 0.899129  [ 3056/ 3200]\n",
            "loss: 1.027743  [ 3072/ 3200]\n",
            "loss: 0.988513  [ 3088/ 3200]\n",
            "loss: 1.141805  [ 3104/ 3200]\n",
            "loss: 1.061816  [ 3120/ 3200]\n",
            "loss: 1.161069  [ 3136/ 3200]\n",
            "loss: 1.210568  [ 3152/ 3200]\n",
            "loss: 0.959085  [ 3168/ 3200]\n",
            "loss: 1.031096  [ 3184/ 3200]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.008831  [    0/ 3200]\n",
            "loss: 1.045390  [   16/ 3200]\n",
            "loss: 1.103075  [   32/ 3200]\n",
            "loss: 1.272665  [   48/ 3200]\n",
            "loss: 1.265433  [   64/ 3200]\n",
            "loss: 0.955372  [   80/ 3200]\n",
            "loss: 0.943994  [   96/ 3200]\n",
            "loss: 1.187549  [  112/ 3200]\n",
            "loss: 1.038068  [  128/ 3200]\n",
            "loss: 1.071245  [  144/ 3200]\n",
            "loss: 1.101389  [  160/ 3200]\n",
            "loss: 0.874455  [  176/ 3200]\n",
            "loss: 0.942729  [  192/ 3200]\n",
            "loss: 1.141840  [  208/ 3200]\n",
            "loss: 0.996810  [  224/ 3200]\n",
            "loss: 0.944009  [  240/ 3200]\n",
            "loss: 0.973107  [  256/ 3200]\n",
            "loss: 0.936710  [  272/ 3200]\n",
            "loss: 1.089166  [  288/ 3200]\n",
            "loss: 1.470393  [  304/ 3200]\n",
            "loss: 0.980987  [  320/ 3200]\n",
            "loss: 1.085401  [  336/ 3200]\n",
            "loss: 1.180902  [  352/ 3200]\n",
            "loss: 1.031727  [  368/ 3200]\n",
            "loss: 0.916069  [  384/ 3200]\n",
            "loss: 0.946692  [  400/ 3200]\n",
            "loss: 0.989652  [  416/ 3200]\n",
            "loss: 1.155161  [  432/ 3200]\n",
            "loss: 0.955021  [  448/ 3200]\n",
            "loss: 0.875094  [  464/ 3200]\n",
            "loss: 0.758842  [  480/ 3200]\n",
            "loss: 0.975013  [  496/ 3200]\n",
            "loss: 1.167950  [  512/ 3200]\n",
            "loss: 1.128907  [  528/ 3200]\n",
            "loss: 1.189858  [  544/ 3200]\n",
            "loss: 1.247453  [  560/ 3200]\n",
            "loss: 1.099158  [  576/ 3200]\n",
            "loss: 0.935503  [  592/ 3200]\n",
            "loss: 1.077541  [  608/ 3200]\n",
            "loss: 1.133711  [  624/ 3200]\n",
            "loss: 1.251864  [  640/ 3200]\n",
            "loss: 1.037498  [  656/ 3200]\n",
            "loss: 0.894360  [  672/ 3200]\n",
            "loss: 1.253791  [  688/ 3200]\n",
            "loss: 1.035433  [  704/ 3200]\n",
            "loss: 1.096823  [  720/ 3200]\n",
            "loss: 1.242732  [  736/ 3200]\n",
            "loss: 0.999871  [  752/ 3200]\n",
            "loss: 1.198863  [  768/ 3200]\n",
            "loss: 0.899313  [  784/ 3200]\n",
            "loss: 0.979626  [  800/ 3200]\n",
            "loss: 1.000569  [  816/ 3200]\n",
            "loss: 0.898068  [  832/ 3200]\n",
            "loss: 1.110737  [  848/ 3200]\n",
            "loss: 0.964090  [  864/ 3200]\n",
            "loss: 0.971036  [  880/ 3200]\n",
            "loss: 1.017329  [  896/ 3200]\n",
            "loss: 1.152445  [  912/ 3200]\n",
            "loss: 1.090428  [  928/ 3200]\n",
            "loss: 1.196664  [  944/ 3200]\n",
            "loss: 0.891941  [  960/ 3200]\n",
            "loss: 1.063597  [  976/ 3200]\n",
            "loss: 0.817686  [  992/ 3200]\n",
            "loss: 0.958043  [ 1008/ 3200]\n",
            "loss: 0.976728  [ 1024/ 3200]\n",
            "loss: 1.168632  [ 1040/ 3200]\n",
            "loss: 1.044999  [ 1056/ 3200]\n",
            "loss: 0.956160  [ 1072/ 3200]\n",
            "loss: 1.183345  [ 1088/ 3200]\n",
            "loss: 0.887528  [ 1104/ 3200]\n",
            "loss: 1.073828  [ 1120/ 3200]\n",
            "loss: 1.122051  [ 1136/ 3200]\n",
            "loss: 0.976109  [ 1152/ 3200]\n",
            "loss: 1.104029  [ 1168/ 3200]\n",
            "loss: 1.107113  [ 1184/ 3200]\n",
            "loss: 1.133579  [ 1200/ 3200]\n",
            "loss: 0.961678  [ 1216/ 3200]\n",
            "loss: 0.807840  [ 1232/ 3200]\n",
            "loss: 1.198232  [ 1248/ 3200]\n",
            "loss: 1.115121  [ 1264/ 3200]\n",
            "loss: 0.960626  [ 1280/ 3200]\n",
            "loss: 1.109171  [ 1296/ 3200]\n",
            "loss: 0.880861  [ 1312/ 3200]\n",
            "loss: 1.011853  [ 1328/ 3200]\n",
            "loss: 1.013497  [ 1344/ 3200]\n",
            "loss: 0.912111  [ 1360/ 3200]\n",
            "loss: 0.951148  [ 1376/ 3200]\n",
            "loss: 1.018728  [ 1392/ 3200]\n",
            "loss: 0.972955  [ 1408/ 3200]\n",
            "loss: 1.262055  [ 1424/ 3200]\n",
            "loss: 1.117455  [ 1440/ 3200]\n",
            "loss: 0.888239  [ 1456/ 3200]\n",
            "loss: 1.019189  [ 1472/ 3200]\n",
            "loss: 0.867423  [ 1488/ 3200]\n",
            "loss: 1.169709  [ 1504/ 3200]\n",
            "loss: 1.050843  [ 1520/ 3200]\n",
            "loss: 0.948246  [ 1536/ 3200]\n",
            "loss: 0.978322  [ 1552/ 3200]\n",
            "loss: 0.960081  [ 1568/ 3200]\n",
            "loss: 1.001047  [ 1584/ 3200]\n",
            "loss: 0.914272  [ 1600/ 3200]\n",
            "loss: 1.051440  [ 1616/ 3200]\n",
            "loss: 1.216525  [ 1632/ 3200]\n",
            "loss: 1.014232  [ 1648/ 3200]\n",
            "loss: 1.174968  [ 1664/ 3200]\n",
            "loss: 1.021941  [ 1680/ 3200]\n",
            "loss: 0.857128  [ 1696/ 3200]\n",
            "loss: 1.213696  [ 1712/ 3200]\n",
            "loss: 1.129528  [ 1728/ 3200]\n",
            "loss: 1.287338  [ 1744/ 3200]\n",
            "loss: 0.911673  [ 1760/ 3200]\n",
            "loss: 1.093347  [ 1776/ 3200]\n",
            "loss: 0.846714  [ 1792/ 3200]\n",
            "loss: 0.952566  [ 1808/ 3200]\n",
            "loss: 1.015260  [ 1824/ 3200]\n",
            "loss: 1.048108  [ 1840/ 3200]\n",
            "loss: 1.388927  [ 1856/ 3200]\n",
            "loss: 1.004999  [ 1872/ 3200]\n",
            "loss: 1.121325  [ 1888/ 3200]\n",
            "loss: 1.151708  [ 1904/ 3200]\n",
            "loss: 0.898398  [ 1920/ 3200]\n",
            "loss: 1.062570  [ 1936/ 3200]\n",
            "loss: 0.940808  [ 1952/ 3200]\n",
            "loss: 0.905217  [ 1968/ 3200]\n",
            "loss: 0.877652  [ 1984/ 3200]\n",
            "loss: 0.895427  [ 2000/ 3200]\n",
            "loss: 0.877925  [ 2016/ 3200]\n",
            "loss: 1.142562  [ 2032/ 3200]\n",
            "loss: 1.076608  [ 2048/ 3200]\n",
            "loss: 1.213582  [ 2064/ 3200]\n",
            "loss: 1.005528  [ 2080/ 3200]\n",
            "loss: 1.100855  [ 2096/ 3200]\n",
            "loss: 1.000983  [ 2112/ 3200]\n",
            "loss: 1.151721  [ 2128/ 3200]\n",
            "loss: 1.039992  [ 2144/ 3200]\n",
            "loss: 1.168020  [ 2160/ 3200]\n",
            "loss: 0.909296  [ 2176/ 3200]\n",
            "loss: 1.069561  [ 2192/ 3200]\n",
            "loss: 0.832793  [ 2208/ 3200]\n",
            "loss: 1.279418  [ 2224/ 3200]\n",
            "loss: 1.136831  [ 2240/ 3200]\n",
            "loss: 0.993153  [ 2256/ 3200]\n",
            "loss: 1.030994  [ 2272/ 3200]\n",
            "loss: 1.063462  [ 2288/ 3200]\n",
            "loss: 1.105625  [ 2304/ 3200]\n",
            "loss: 1.052462  [ 2320/ 3200]\n",
            "loss: 1.018166  [ 2336/ 3200]\n",
            "loss: 0.883853  [ 2352/ 3200]\n",
            "loss: 1.017893  [ 2368/ 3200]\n",
            "loss: 0.940165  [ 2384/ 3200]\n",
            "loss: 0.976134  [ 2400/ 3200]\n",
            "loss: 1.045037  [ 2416/ 3200]\n",
            "loss: 1.457178  [ 2432/ 3200]\n",
            "loss: 0.992379  [ 2448/ 3200]\n",
            "loss: 0.874833  [ 2464/ 3200]\n",
            "loss: 0.848043  [ 2480/ 3200]\n",
            "loss: 1.064446  [ 2496/ 3200]\n",
            "loss: 1.037471  [ 2512/ 3200]\n",
            "loss: 0.874718  [ 2528/ 3200]\n",
            "loss: 1.210406  [ 2544/ 3200]\n",
            "loss: 0.934389  [ 2560/ 3200]\n",
            "loss: 0.943051  [ 2576/ 3200]\n",
            "loss: 1.070375  [ 2592/ 3200]\n",
            "loss: 0.973763  [ 2608/ 3200]\n",
            "loss: 0.867030  [ 2624/ 3200]\n",
            "loss: 1.117450  [ 2640/ 3200]\n",
            "loss: 1.245156  [ 2656/ 3200]\n",
            "loss: 1.022951  [ 2672/ 3200]\n",
            "loss: 0.965283  [ 2688/ 3200]\n",
            "loss: 0.857240  [ 2704/ 3200]\n",
            "loss: 1.090689  [ 2720/ 3200]\n",
            "loss: 0.895784  [ 2736/ 3200]\n",
            "loss: 0.975085  [ 2752/ 3200]\n",
            "loss: 1.309202  [ 2768/ 3200]\n",
            "loss: 0.940524  [ 2784/ 3200]\n",
            "loss: 0.868643  [ 2800/ 3200]\n",
            "loss: 1.187111  [ 2816/ 3200]\n",
            "loss: 1.218564  [ 2832/ 3200]\n",
            "loss: 1.183536  [ 2848/ 3200]\n",
            "loss: 0.850044  [ 2864/ 3200]\n",
            "loss: 0.878973  [ 2880/ 3200]\n",
            "loss: 1.013657  [ 2896/ 3200]\n",
            "loss: 0.901560  [ 2912/ 3200]\n",
            "loss: 0.819394  [ 2928/ 3200]\n",
            "loss: 1.057343  [ 2944/ 3200]\n",
            "loss: 1.068930  [ 2960/ 3200]\n",
            "loss: 1.160807  [ 2976/ 3200]\n",
            "loss: 1.090076  [ 2992/ 3200]\n",
            "loss: 1.170880  [ 3008/ 3200]\n",
            "loss: 1.012474  [ 3024/ 3200]\n",
            "loss: 1.035388  [ 3040/ 3200]\n",
            "loss: 0.953694  [ 3056/ 3200]\n",
            "loss: 1.089436  [ 3072/ 3200]\n",
            "loss: 0.900603  [ 3088/ 3200]\n",
            "loss: 0.898372  [ 3104/ 3200]\n",
            "loss: 0.945519  [ 3120/ 3200]\n",
            "loss: 0.896942  [ 3136/ 3200]\n",
            "loss: 1.168807  [ 3152/ 3200]\n",
            "loss: 0.945181  [ 3168/ 3200]\n",
            "loss: 1.361451  [ 3184/ 3200]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.096501  [    0/ 3200]\n",
            "loss: 0.863786  [   16/ 3200]\n",
            "loss: 1.019032  [   32/ 3200]\n",
            "loss: 1.036560  [   48/ 3200]\n",
            "loss: 0.825203  [   64/ 3200]\n",
            "loss: 1.053866  [   80/ 3200]\n",
            "loss: 1.019350  [   96/ 3200]\n",
            "loss: 0.993221  [  112/ 3200]\n",
            "loss: 1.063737  [  128/ 3200]\n",
            "loss: 1.035593  [  144/ 3200]\n",
            "loss: 0.926656  [  160/ 3200]\n",
            "loss: 1.103514  [  176/ 3200]\n",
            "loss: 1.151586  [  192/ 3200]\n",
            "loss: 0.939899  [  208/ 3200]\n",
            "loss: 0.921242  [  224/ 3200]\n",
            "loss: 0.957797  [  240/ 3200]\n",
            "loss: 0.890620  [  256/ 3200]\n",
            "loss: 1.174802  [  272/ 3200]\n",
            "loss: 0.988577  [  288/ 3200]\n",
            "loss: 1.458019  [  304/ 3200]\n",
            "loss: 1.190501  [  320/ 3200]\n",
            "loss: 1.046120  [  336/ 3200]\n",
            "loss: 1.055272  [  352/ 3200]\n",
            "loss: 1.172758  [  368/ 3200]\n",
            "loss: 0.946378  [  384/ 3200]\n",
            "loss: 1.315918  [  400/ 3200]\n",
            "loss: 0.950699  [  416/ 3200]\n",
            "loss: 0.890115  [  432/ 3200]\n",
            "loss: 1.127080  [  448/ 3200]\n",
            "loss: 0.880889  [  464/ 3200]\n",
            "loss: 1.127859  [  480/ 3200]\n",
            "loss: 0.995169  [  496/ 3200]\n",
            "loss: 1.130560  [  512/ 3200]\n",
            "loss: 0.930816  [  528/ 3200]\n",
            "loss: 1.326725  [  544/ 3200]\n",
            "loss: 1.013502  [  560/ 3200]\n",
            "loss: 0.978260  [  576/ 3200]\n",
            "loss: 1.131490  [  592/ 3200]\n",
            "loss: 1.191213  [  608/ 3200]\n",
            "loss: 0.971355  [  624/ 3200]\n",
            "loss: 1.127489  [  640/ 3200]\n",
            "loss: 0.900577  [  656/ 3200]\n",
            "loss: 1.108632  [  672/ 3200]\n",
            "loss: 1.215120  [  688/ 3200]\n",
            "loss: 0.882378  [  704/ 3200]\n",
            "loss: 1.106819  [  720/ 3200]\n",
            "loss: 0.805845  [  736/ 3200]\n",
            "loss: 1.164195  [  752/ 3200]\n",
            "loss: 0.906301  [  768/ 3200]\n",
            "loss: 0.944229  [  784/ 3200]\n",
            "loss: 0.982410  [  800/ 3200]\n",
            "loss: 1.027457  [  816/ 3200]\n",
            "loss: 0.959746  [  832/ 3200]\n",
            "loss: 1.149354  [  848/ 3200]\n",
            "loss: 0.952845  [  864/ 3200]\n",
            "loss: 1.149154  [  880/ 3200]\n",
            "loss: 0.710326  [  896/ 3200]\n",
            "loss: 0.920821  [  912/ 3200]\n",
            "loss: 1.113918  [  928/ 3200]\n",
            "loss: 1.026728  [  944/ 3200]\n",
            "loss: 1.012750  [  960/ 3200]\n",
            "loss: 0.912605  [  976/ 3200]\n",
            "loss: 0.876084  [  992/ 3200]\n",
            "loss: 0.939692  [ 1008/ 3200]\n",
            "loss: 1.070600  [ 1024/ 3200]\n",
            "loss: 0.817523  [ 1040/ 3200]\n",
            "loss: 1.036986  [ 1056/ 3200]\n",
            "loss: 1.061831  [ 1072/ 3200]\n",
            "loss: 1.073003  [ 1088/ 3200]\n",
            "loss: 1.006031  [ 1104/ 3200]\n",
            "loss: 1.082114  [ 1120/ 3200]\n",
            "loss: 0.995920  [ 1136/ 3200]\n",
            "loss: 1.144249  [ 1152/ 3200]\n",
            "loss: 0.831034  [ 1168/ 3200]\n",
            "loss: 0.995706  [ 1184/ 3200]\n",
            "loss: 0.993070  [ 1200/ 3200]\n",
            "loss: 0.904451  [ 1216/ 3200]\n",
            "loss: 0.990293  [ 1232/ 3200]\n",
            "loss: 1.052880  [ 1248/ 3200]\n",
            "loss: 1.248014  [ 1264/ 3200]\n",
            "loss: 1.139489  [ 1280/ 3200]\n",
            "loss: 1.061571  [ 1296/ 3200]\n",
            "loss: 0.987328  [ 1312/ 3200]\n",
            "loss: 1.114750  [ 1328/ 3200]\n",
            "loss: 1.011688  [ 1344/ 3200]\n",
            "loss: 1.014528  [ 1360/ 3200]\n",
            "loss: 1.025311  [ 1376/ 3200]\n",
            "loss: 1.040457  [ 1392/ 3200]\n",
            "loss: 1.077137  [ 1408/ 3200]\n",
            "loss: 1.044612  [ 1424/ 3200]\n",
            "loss: 1.022142  [ 1440/ 3200]\n",
            "loss: 1.029766  [ 1456/ 3200]\n",
            "loss: 1.237107  [ 1472/ 3200]\n",
            "loss: 0.979917  [ 1488/ 3200]\n",
            "loss: 0.964370  [ 1504/ 3200]\n",
            "loss: 1.052034  [ 1520/ 3200]\n",
            "loss: 1.070977  [ 1536/ 3200]\n",
            "loss: 0.898761  [ 1552/ 3200]\n",
            "loss: 0.957507  [ 1568/ 3200]\n",
            "loss: 1.001511  [ 1584/ 3200]\n",
            "loss: 1.086523  [ 1600/ 3200]\n",
            "loss: 1.318979  [ 1616/ 3200]\n",
            "loss: 1.093753  [ 1632/ 3200]\n",
            "loss: 1.043844  [ 1648/ 3200]\n",
            "loss: 1.058960  [ 1664/ 3200]\n",
            "loss: 1.022890  [ 1680/ 3200]\n",
            "loss: 0.928632  [ 1696/ 3200]\n",
            "loss: 1.071242  [ 1712/ 3200]\n",
            "loss: 1.194698  [ 1728/ 3200]\n",
            "loss: 1.187562  [ 1744/ 3200]\n",
            "loss: 1.013862  [ 1760/ 3200]\n",
            "loss: 1.010029  [ 1776/ 3200]\n",
            "loss: 1.113692  [ 1792/ 3200]\n",
            "loss: 0.888964  [ 1808/ 3200]\n",
            "loss: 1.041944  [ 1824/ 3200]\n",
            "loss: 0.964666  [ 1840/ 3200]\n",
            "loss: 0.879410  [ 1856/ 3200]\n",
            "loss: 1.123613  [ 1872/ 3200]\n",
            "loss: 1.060626  [ 1888/ 3200]\n",
            "loss: 0.949527  [ 1904/ 3200]\n",
            "loss: 1.216273  [ 1920/ 3200]\n",
            "loss: 0.962215  [ 1936/ 3200]\n",
            "loss: 0.982243  [ 1952/ 3200]\n",
            "loss: 1.015556  [ 1968/ 3200]\n",
            "loss: 1.092733  [ 1984/ 3200]\n",
            "loss: 1.144529  [ 2000/ 3200]\n",
            "loss: 0.897901  [ 2016/ 3200]\n",
            "loss: 0.849259  [ 2032/ 3200]\n",
            "loss: 1.165953  [ 2048/ 3200]\n",
            "loss: 1.154461  [ 2064/ 3200]\n",
            "loss: 0.924941  [ 2080/ 3200]\n",
            "loss: 0.962645  [ 2096/ 3200]\n",
            "loss: 1.168601  [ 2112/ 3200]\n",
            "loss: 1.129243  [ 2128/ 3200]\n",
            "loss: 1.243477  [ 2144/ 3200]\n",
            "loss: 1.069435  [ 2160/ 3200]\n",
            "loss: 0.796565  [ 2176/ 3200]\n",
            "loss: 0.994507  [ 2192/ 3200]\n",
            "loss: 1.028806  [ 2208/ 3200]\n",
            "loss: 0.936934  [ 2224/ 3200]\n",
            "loss: 0.965057  [ 2240/ 3200]\n",
            "loss: 1.003384  [ 2256/ 3200]\n",
            "loss: 0.996603  [ 2272/ 3200]\n",
            "loss: 0.916607  [ 2288/ 3200]\n",
            "loss: 0.750228  [ 2304/ 3200]\n",
            "loss: 1.044122  [ 2320/ 3200]\n",
            "loss: 0.894921  [ 2336/ 3200]\n",
            "loss: 1.142139  [ 2352/ 3200]\n",
            "loss: 0.851686  [ 2368/ 3200]\n",
            "loss: 0.960035  [ 2384/ 3200]\n",
            "loss: 0.947299  [ 2400/ 3200]\n",
            "loss: 0.941612  [ 2416/ 3200]\n",
            "loss: 1.003204  [ 2432/ 3200]\n",
            "loss: 1.044029  [ 2448/ 3200]\n",
            "loss: 1.055013  [ 2464/ 3200]\n",
            "loss: 0.965845  [ 2480/ 3200]\n",
            "loss: 1.096977  [ 2496/ 3200]\n",
            "loss: 1.123868  [ 2512/ 3200]\n",
            "loss: 0.982635  [ 2528/ 3200]\n",
            "loss: 1.066642  [ 2544/ 3200]\n",
            "loss: 1.113491  [ 2560/ 3200]\n",
            "loss: 1.287338  [ 2576/ 3200]\n",
            "loss: 1.090626  [ 2592/ 3200]\n",
            "loss: 0.842857  [ 2608/ 3200]\n",
            "loss: 1.017319  [ 2624/ 3200]\n",
            "loss: 1.064044  [ 2640/ 3200]\n",
            "loss: 0.884144  [ 2656/ 3200]\n",
            "loss: 0.978199  [ 2672/ 3200]\n",
            "loss: 0.802809  [ 2688/ 3200]\n",
            "loss: 1.206212  [ 2704/ 3200]\n",
            "loss: 0.884082  [ 2720/ 3200]\n",
            "loss: 1.151559  [ 2736/ 3200]\n",
            "loss: 1.091451  [ 2752/ 3200]\n",
            "loss: 1.026972  [ 2768/ 3200]\n",
            "loss: 1.313766  [ 2784/ 3200]\n",
            "loss: 0.848112  [ 2800/ 3200]\n",
            "loss: 0.985728  [ 2816/ 3200]\n",
            "loss: 0.967461  [ 2832/ 3200]\n",
            "loss: 1.008321  [ 2848/ 3200]\n",
            "loss: 0.810986  [ 2864/ 3200]\n",
            "loss: 0.925502  [ 2880/ 3200]\n",
            "loss: 0.911573  [ 2896/ 3200]\n",
            "loss: 1.094314  [ 2912/ 3200]\n",
            "loss: 0.895151  [ 2928/ 3200]\n",
            "loss: 1.047517  [ 2944/ 3200]\n",
            "loss: 0.975510  [ 2960/ 3200]\n",
            "loss: 0.905822  [ 2976/ 3200]\n",
            "loss: 1.019543  [ 2992/ 3200]\n",
            "loss: 1.201868  [ 3008/ 3200]\n",
            "loss: 1.087081  [ 3024/ 3200]\n",
            "loss: 1.092542  [ 3040/ 3200]\n",
            "loss: 1.086756  [ 3056/ 3200]\n",
            "loss: 1.088753  [ 3072/ 3200]\n",
            "loss: 0.902315  [ 3088/ 3200]\n",
            "loss: 1.098700  [ 3104/ 3200]\n",
            "loss: 0.948470  [ 3120/ 3200]\n",
            "loss: 1.152104  [ 3136/ 3200]\n",
            "loss: 0.858609  [ 3152/ 3200]\n",
            "loss: 1.099443  [ 3168/ 3200]\n",
            "loss: 0.852068  [ 3184/ 3200]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.946304  [    0/ 3200]\n",
            "loss: 1.067714  [   16/ 3200]\n",
            "loss: 0.857296  [   32/ 3200]\n",
            "loss: 1.241443  [   48/ 3200]\n",
            "loss: 0.887582  [   64/ 3200]\n",
            "loss: 0.842789  [   80/ 3200]\n",
            "loss: 1.002910  [   96/ 3200]\n",
            "loss: 0.921239  [  112/ 3200]\n",
            "loss: 0.845950  [  128/ 3200]\n",
            "loss: 1.129344  [  144/ 3200]\n",
            "loss: 1.095313  [  160/ 3200]\n",
            "loss: 1.255589  [  176/ 3200]\n",
            "loss: 0.990128  [  192/ 3200]\n",
            "loss: 0.824193  [  208/ 3200]\n",
            "loss: 0.934326  [  224/ 3200]\n",
            "loss: 1.050430  [  240/ 3200]\n",
            "loss: 1.032912  [  256/ 3200]\n",
            "loss: 0.881980  [  272/ 3200]\n",
            "loss: 0.957265  [  288/ 3200]\n",
            "loss: 1.104995  [  304/ 3200]\n",
            "loss: 0.948283  [  320/ 3200]\n",
            "loss: 0.886361  [  336/ 3200]\n",
            "loss: 0.997124  [  352/ 3200]\n",
            "loss: 0.978974  [  368/ 3200]\n",
            "loss: 0.844488  [  384/ 3200]\n",
            "loss: 0.943943  [  400/ 3200]\n",
            "loss: 0.989388  [  416/ 3200]\n",
            "loss: 0.813743  [  432/ 3200]\n",
            "loss: 1.219781  [  448/ 3200]\n",
            "loss: 1.091458  [  464/ 3200]\n",
            "loss: 0.821395  [  480/ 3200]\n",
            "loss: 1.078357  [  496/ 3200]\n",
            "loss: 1.010369  [  512/ 3200]\n",
            "loss: 1.042543  [  528/ 3200]\n",
            "loss: 1.059439  [  544/ 3200]\n",
            "loss: 1.010954  [  560/ 3200]\n",
            "loss: 1.017931  [  576/ 3200]\n",
            "loss: 1.127142  [  592/ 3200]\n",
            "loss: 1.047645  [  608/ 3200]\n",
            "loss: 0.851905  [  624/ 3200]\n",
            "loss: 0.912579  [  640/ 3200]\n",
            "loss: 0.955662  [  656/ 3200]\n",
            "loss: 0.908307  [  672/ 3200]\n",
            "loss: 1.090426  [  688/ 3200]\n",
            "loss: 1.279327  [  704/ 3200]\n",
            "loss: 1.040676  [  720/ 3200]\n",
            "loss: 1.294234  [  736/ 3200]\n",
            "loss: 1.245422  [  752/ 3200]\n",
            "loss: 0.932652  [  768/ 3200]\n",
            "loss: 1.023834  [  784/ 3200]\n",
            "loss: 0.961844  [  800/ 3200]\n",
            "loss: 1.075744  [  816/ 3200]\n",
            "loss: 1.171701  [  832/ 3200]\n",
            "loss: 0.818300  [  848/ 3200]\n",
            "loss: 0.858970  [  864/ 3200]\n",
            "loss: 1.159461  [  880/ 3200]\n",
            "loss: 0.927414  [  896/ 3200]\n",
            "loss: 0.918498  [  912/ 3200]\n",
            "loss: 0.918687  [  928/ 3200]\n",
            "loss: 0.896231  [  944/ 3200]\n",
            "loss: 0.986978  [  960/ 3200]\n",
            "loss: 1.233309  [  976/ 3200]\n",
            "loss: 1.046060  [  992/ 3200]\n",
            "loss: 0.683651  [ 1008/ 3200]\n",
            "loss: 1.042219  [ 1024/ 3200]\n",
            "loss: 1.025712  [ 1040/ 3200]\n",
            "loss: 1.298467  [ 1056/ 3200]\n",
            "loss: 0.866709  [ 1072/ 3200]\n",
            "loss: 1.052837  [ 1088/ 3200]\n",
            "loss: 1.028243  [ 1104/ 3200]\n",
            "loss: 0.996242  [ 1120/ 3200]\n",
            "loss: 1.088513  [ 1136/ 3200]\n",
            "loss: 1.140651  [ 1152/ 3200]\n",
            "loss: 0.979551  [ 1168/ 3200]\n",
            "loss: 0.991302  [ 1184/ 3200]\n",
            "loss: 1.112855  [ 1200/ 3200]\n",
            "loss: 1.207289  [ 1216/ 3200]\n",
            "loss: 1.181170  [ 1232/ 3200]\n",
            "loss: 0.832325  [ 1248/ 3200]\n",
            "loss: 0.871122  [ 1264/ 3200]\n",
            "loss: 1.161551  [ 1280/ 3200]\n",
            "loss: 0.807196  [ 1296/ 3200]\n",
            "loss: 1.084839  [ 1312/ 3200]\n",
            "loss: 1.005687  [ 1328/ 3200]\n",
            "loss: 1.121574  [ 1344/ 3200]\n",
            "loss: 0.878053  [ 1360/ 3200]\n",
            "loss: 1.182804  [ 1376/ 3200]\n",
            "loss: 0.884049  [ 1392/ 3200]\n",
            "loss: 0.960038  [ 1408/ 3200]\n",
            "loss: 0.972659  [ 1424/ 3200]\n",
            "loss: 0.922287  [ 1440/ 3200]\n",
            "loss: 0.886935  [ 1456/ 3200]\n",
            "loss: 0.959685  [ 1472/ 3200]\n",
            "loss: 0.932563  [ 1488/ 3200]\n",
            "loss: 1.161381  [ 1504/ 3200]\n",
            "loss: 0.718001  [ 1520/ 3200]\n",
            "loss: 0.863062  [ 1536/ 3200]\n",
            "loss: 1.159476  [ 1552/ 3200]\n",
            "loss: 1.092248  [ 1568/ 3200]\n",
            "loss: 0.979760  [ 1584/ 3200]\n",
            "loss: 0.937928  [ 1600/ 3200]\n",
            "loss: 0.980888  [ 1616/ 3200]\n",
            "loss: 1.003205  [ 1632/ 3200]\n",
            "loss: 0.931424  [ 1648/ 3200]\n",
            "loss: 1.195964  [ 1664/ 3200]\n",
            "loss: 1.299755  [ 1680/ 3200]\n",
            "loss: 1.119136  [ 1696/ 3200]\n",
            "loss: 0.887606  [ 1712/ 3200]\n",
            "loss: 1.120629  [ 1728/ 3200]\n",
            "loss: 0.982270  [ 1744/ 3200]\n",
            "loss: 0.812198  [ 1760/ 3200]\n",
            "loss: 1.029549  [ 1776/ 3200]\n",
            "loss: 1.067354  [ 1792/ 3200]\n",
            "loss: 1.073890  [ 1808/ 3200]\n",
            "loss: 1.042697  [ 1824/ 3200]\n",
            "loss: 1.006237  [ 1840/ 3200]\n",
            "loss: 1.174588  [ 1856/ 3200]\n",
            "loss: 1.205211  [ 1872/ 3200]\n",
            "loss: 0.863524  [ 1888/ 3200]\n",
            "loss: 1.020848  [ 1904/ 3200]\n",
            "loss: 1.010772  [ 1920/ 3200]\n",
            "loss: 1.166070  [ 1936/ 3200]\n",
            "loss: 0.943410  [ 1952/ 3200]\n",
            "loss: 0.941058  [ 1968/ 3200]\n",
            "loss: 0.938751  [ 1984/ 3200]\n",
            "loss: 0.916487  [ 2000/ 3200]\n",
            "loss: 1.173977  [ 2016/ 3200]\n",
            "loss: 0.878368  [ 2032/ 3200]\n",
            "loss: 1.008543  [ 2048/ 3200]\n",
            "loss: 0.846847  [ 2064/ 3200]\n",
            "loss: 1.133871  [ 2080/ 3200]\n",
            "loss: 0.978559  [ 2096/ 3200]\n",
            "loss: 1.081822  [ 2112/ 3200]\n",
            "loss: 1.133001  [ 2128/ 3200]\n",
            "loss: 0.977324  [ 2144/ 3200]\n",
            "loss: 1.143158  [ 2160/ 3200]\n",
            "loss: 0.994639  [ 2176/ 3200]\n",
            "loss: 1.050216  [ 2192/ 3200]\n",
            "loss: 1.147184  [ 2208/ 3200]\n",
            "loss: 0.996773  [ 2224/ 3200]\n",
            "loss: 1.036490  [ 2240/ 3200]\n",
            "loss: 1.188888  [ 2256/ 3200]\n",
            "loss: 1.075543  [ 2272/ 3200]\n",
            "loss: 1.004457  [ 2288/ 3200]\n",
            "loss: 1.082168  [ 2304/ 3200]\n",
            "loss: 0.925008  [ 2320/ 3200]\n",
            "loss: 1.288962  [ 2336/ 3200]\n",
            "loss: 0.872323  [ 2352/ 3200]\n",
            "loss: 1.229208  [ 2368/ 3200]\n",
            "loss: 1.139720  [ 2384/ 3200]\n",
            "loss: 0.795034  [ 2400/ 3200]\n",
            "loss: 0.975387  [ 2416/ 3200]\n",
            "loss: 1.003772  [ 2432/ 3200]\n",
            "loss: 1.028043  [ 2448/ 3200]\n",
            "loss: 1.218850  [ 2464/ 3200]\n",
            "loss: 1.125076  [ 2480/ 3200]\n",
            "loss: 0.982125  [ 2496/ 3200]\n",
            "loss: 1.044965  [ 2512/ 3200]\n",
            "loss: 1.095134  [ 2528/ 3200]\n",
            "loss: 0.829298  [ 2544/ 3200]\n",
            "loss: 1.220273  [ 2560/ 3200]\n",
            "loss: 1.038042  [ 2576/ 3200]\n",
            "loss: 0.805293  [ 2592/ 3200]\n",
            "loss: 1.032284  [ 2608/ 3200]\n",
            "loss: 1.080428  [ 2624/ 3200]\n",
            "loss: 1.090439  [ 2640/ 3200]\n",
            "loss: 1.209834  [ 2656/ 3200]\n",
            "loss: 0.822020  [ 2672/ 3200]\n",
            "loss: 0.739301  [ 2688/ 3200]\n",
            "loss: 1.002726  [ 2704/ 3200]\n",
            "loss: 0.890552  [ 2720/ 3200]\n",
            "loss: 0.760213  [ 2736/ 3200]\n",
            "loss: 0.904130  [ 2752/ 3200]\n",
            "loss: 1.042485  [ 2768/ 3200]\n",
            "loss: 0.925423  [ 2784/ 3200]\n",
            "loss: 1.020167  [ 2800/ 3200]\n",
            "loss: 0.873917  [ 2816/ 3200]\n",
            "loss: 0.893421  [ 2832/ 3200]\n",
            "loss: 0.988889  [ 2848/ 3200]\n",
            "loss: 1.128154  [ 2864/ 3200]\n",
            "loss: 0.838047  [ 2880/ 3200]\n",
            "loss: 1.130433  [ 2896/ 3200]\n",
            "loss: 1.062981  [ 2912/ 3200]\n",
            "loss: 1.155229  [ 2928/ 3200]\n",
            "loss: 1.054283  [ 2944/ 3200]\n",
            "loss: 1.072899  [ 2960/ 3200]\n",
            "loss: 0.986014  [ 2976/ 3200]\n",
            "loss: 1.084494  [ 2992/ 3200]\n",
            "loss: 1.302775  [ 3008/ 3200]\n",
            "loss: 0.988627  [ 3024/ 3200]\n",
            "loss: 0.786261  [ 3040/ 3200]\n",
            "loss: 0.719207  [ 3056/ 3200]\n",
            "loss: 0.873769  [ 3072/ 3200]\n",
            "loss: 1.073639  [ 3088/ 3200]\n",
            "loss: 0.926608  [ 3104/ 3200]\n",
            "loss: 1.161695  [ 3120/ 3200]\n",
            "loss: 0.940361  [ 3136/ 3200]\n",
            "loss: 1.036610  [ 3152/ 3200]\n",
            "loss: 1.074665  [ 3168/ 3200]\n",
            "loss: 1.405807  [ 3184/ 3200]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.038982  [    0/ 3200]\n",
            "loss: 0.867663  [   16/ 3200]\n",
            "loss: 1.055218  [   32/ 3200]\n",
            "loss: 0.821390  [   48/ 3200]\n",
            "loss: 0.964040  [   64/ 3200]\n",
            "loss: 0.899278  [   80/ 3200]\n",
            "loss: 1.215717  [   96/ 3200]\n",
            "loss: 1.087273  [  112/ 3200]\n",
            "loss: 0.856660  [  128/ 3200]\n",
            "loss: 0.840182  [  144/ 3200]\n",
            "loss: 0.905222  [  160/ 3200]\n",
            "loss: 1.175009  [  176/ 3200]\n",
            "loss: 1.023198  [  192/ 3200]\n",
            "loss: 1.152795  [  208/ 3200]\n",
            "loss: 0.982198  [  224/ 3200]\n",
            "loss: 0.754038  [  240/ 3200]\n",
            "loss: 0.932855  [  256/ 3200]\n",
            "loss: 0.927532  [  272/ 3200]\n",
            "loss: 1.000391  [  288/ 3200]\n",
            "loss: 1.088667  [  304/ 3200]\n",
            "loss: 0.959462  [  320/ 3200]\n",
            "loss: 0.886916  [  336/ 3200]\n",
            "loss: 0.835100  [  352/ 3200]\n",
            "loss: 0.958282  [  368/ 3200]\n",
            "loss: 1.035396  [  384/ 3200]\n",
            "loss: 0.832877  [  400/ 3200]\n",
            "loss: 1.284090  [  416/ 3200]\n",
            "loss: 0.848705  [  432/ 3200]\n",
            "loss: 1.315228  [  448/ 3200]\n",
            "loss: 1.019986  [  464/ 3200]\n",
            "loss: 1.166474  [  480/ 3200]\n",
            "loss: 1.019909  [  496/ 3200]\n",
            "loss: 0.918638  [  512/ 3200]\n",
            "loss: 0.943664  [  528/ 3200]\n",
            "loss: 0.958136  [  544/ 3200]\n",
            "loss: 1.020562  [  560/ 3200]\n",
            "loss: 0.793025  [  576/ 3200]\n",
            "loss: 1.019031  [  592/ 3200]\n",
            "loss: 0.880228  [  608/ 3200]\n",
            "loss: 0.896295  [  624/ 3200]\n",
            "loss: 0.888023  [  640/ 3200]\n",
            "loss: 1.110797  [  656/ 3200]\n",
            "loss: 1.094836  [  672/ 3200]\n",
            "loss: 0.939855  [  688/ 3200]\n",
            "loss: 0.865064  [  704/ 3200]\n",
            "loss: 1.023265  [  720/ 3200]\n",
            "loss: 1.183573  [  736/ 3200]\n",
            "loss: 0.787705  [  752/ 3200]\n",
            "loss: 0.974778  [  768/ 3200]\n",
            "loss: 0.934406  [  784/ 3200]\n",
            "loss: 1.192970  [  800/ 3200]\n",
            "loss: 1.010434  [  816/ 3200]\n",
            "loss: 1.091758  [  832/ 3200]\n",
            "loss: 1.145435  [  848/ 3200]\n",
            "loss: 0.967801  [  864/ 3200]\n",
            "loss: 1.013785  [  880/ 3200]\n",
            "loss: 1.039209  [  896/ 3200]\n",
            "loss: 0.909828  [  912/ 3200]\n",
            "loss: 1.048034  [  928/ 3200]\n",
            "loss: 0.917042  [  944/ 3200]\n",
            "loss: 1.158532  [  960/ 3200]\n",
            "loss: 1.114380  [  976/ 3200]\n",
            "loss: 1.048962  [  992/ 3200]\n",
            "loss: 1.019928  [ 1008/ 3200]\n",
            "loss: 0.893338  [ 1024/ 3200]\n",
            "loss: 0.681930  [ 1040/ 3200]\n",
            "loss: 0.961256  [ 1056/ 3200]\n",
            "loss: 0.909198  [ 1072/ 3200]\n",
            "loss: 0.813713  [ 1088/ 3200]\n",
            "loss: 0.789295  [ 1104/ 3200]\n",
            "loss: 0.819350  [ 1120/ 3200]\n",
            "loss: 0.956254  [ 1136/ 3200]\n",
            "loss: 1.052544  [ 1152/ 3200]\n",
            "loss: 0.948197  [ 1168/ 3200]\n",
            "loss: 0.953112  [ 1184/ 3200]\n",
            "loss: 0.864419  [ 1200/ 3200]\n",
            "loss: 1.173714  [ 1216/ 3200]\n",
            "loss: 1.098400  [ 1232/ 3200]\n",
            "loss: 1.107353  [ 1248/ 3200]\n",
            "loss: 1.068313  [ 1264/ 3200]\n",
            "loss: 1.121586  [ 1280/ 3200]\n",
            "loss: 0.983324  [ 1296/ 3200]\n",
            "loss: 1.170343  [ 1312/ 3200]\n",
            "loss: 0.739768  [ 1328/ 3200]\n",
            "loss: 1.017936  [ 1344/ 3200]\n",
            "loss: 0.944963  [ 1360/ 3200]\n",
            "loss: 1.107495  [ 1376/ 3200]\n",
            "loss: 1.127210  [ 1392/ 3200]\n",
            "loss: 1.028501  [ 1408/ 3200]\n",
            "loss: 1.086189  [ 1424/ 3200]\n",
            "loss: 1.212538  [ 1440/ 3200]\n",
            "loss: 1.103168  [ 1456/ 3200]\n",
            "loss: 1.428505  [ 1472/ 3200]\n",
            "loss: 0.999864  [ 1488/ 3200]\n",
            "loss: 1.134418  [ 1504/ 3200]\n",
            "loss: 0.996962  [ 1520/ 3200]\n",
            "loss: 0.851314  [ 1536/ 3200]\n",
            "loss: 1.002081  [ 1552/ 3200]\n",
            "loss: 0.746767  [ 1568/ 3200]\n",
            "loss: 1.356229  [ 1584/ 3200]\n",
            "loss: 1.349961  [ 1600/ 3200]\n",
            "loss: 0.801574  [ 1616/ 3200]\n",
            "loss: 0.969342  [ 1632/ 3200]\n",
            "loss: 0.898388  [ 1648/ 3200]\n",
            "loss: 1.174361  [ 1664/ 3200]\n",
            "loss: 0.875663  [ 1680/ 3200]\n",
            "loss: 1.015835  [ 1696/ 3200]\n",
            "loss: 1.014765  [ 1712/ 3200]\n",
            "loss: 1.572598  [ 1728/ 3200]\n",
            "loss: 0.886140  [ 1744/ 3200]\n",
            "loss: 0.601810  [ 1760/ 3200]\n",
            "loss: 1.031530  [ 1776/ 3200]\n",
            "loss: 1.047738  [ 1792/ 3200]\n",
            "loss: 1.071945  [ 1808/ 3200]\n",
            "loss: 0.952426  [ 1824/ 3200]\n",
            "loss: 1.108393  [ 1840/ 3200]\n",
            "loss: 0.885516  [ 1856/ 3200]\n",
            "loss: 0.909214  [ 1872/ 3200]\n",
            "loss: 0.795357  [ 1888/ 3200]\n",
            "loss: 0.989911  [ 1904/ 3200]\n",
            "loss: 0.768969  [ 1920/ 3200]\n",
            "loss: 1.167269  [ 1936/ 3200]\n",
            "loss: 0.988399  [ 1952/ 3200]\n",
            "loss: 0.978033  [ 1968/ 3200]\n",
            "loss: 0.781131  [ 1984/ 3200]\n",
            "loss: 0.930646  [ 2000/ 3200]\n",
            "loss: 1.070184  [ 2016/ 3200]\n",
            "loss: 1.021912  [ 2032/ 3200]\n",
            "loss: 1.138343  [ 2048/ 3200]\n",
            "loss: 0.932981  [ 2064/ 3200]\n",
            "loss: 0.842608  [ 2080/ 3200]\n",
            "loss: 1.057200  [ 2096/ 3200]\n",
            "loss: 0.965972  [ 2112/ 3200]\n",
            "loss: 0.801716  [ 2128/ 3200]\n",
            "loss: 0.998755  [ 2144/ 3200]\n",
            "loss: 1.239833  [ 2160/ 3200]\n",
            "loss: 1.068687  [ 2176/ 3200]\n",
            "loss: 1.069934  [ 2192/ 3200]\n",
            "loss: 0.794385  [ 2208/ 3200]\n",
            "loss: 1.198850  [ 2224/ 3200]\n",
            "loss: 1.000870  [ 2240/ 3200]\n",
            "loss: 0.868541  [ 2256/ 3200]\n",
            "loss: 1.132642  [ 2272/ 3200]\n",
            "loss: 1.084709  [ 2288/ 3200]\n",
            "loss: 1.118328  [ 2304/ 3200]\n",
            "loss: 0.816888  [ 2320/ 3200]\n",
            "loss: 0.890401  [ 2336/ 3200]\n",
            "loss: 1.023565  [ 2352/ 3200]\n",
            "loss: 0.724025  [ 2368/ 3200]\n",
            "loss: 1.015265  [ 2384/ 3200]\n",
            "loss: 0.994244  [ 2400/ 3200]\n",
            "loss: 0.996815  [ 2416/ 3200]\n",
            "loss: 0.960768  [ 2432/ 3200]\n",
            "loss: 0.785717  [ 2448/ 3200]\n",
            "loss: 1.128656  [ 2464/ 3200]\n",
            "loss: 1.249575  [ 2480/ 3200]\n",
            "loss: 1.398710  [ 2496/ 3200]\n",
            "loss: 1.030881  [ 2512/ 3200]\n",
            "loss: 1.041403  [ 2528/ 3200]\n",
            "loss: 1.066427  [ 2544/ 3200]\n",
            "loss: 1.072351  [ 2560/ 3200]\n",
            "loss: 1.032101  [ 2576/ 3200]\n",
            "loss: 1.404374  [ 2592/ 3200]\n",
            "loss: 1.396866  [ 2608/ 3200]\n",
            "loss: 0.980646  [ 2624/ 3200]\n",
            "loss: 0.855802  [ 2640/ 3200]\n",
            "loss: 1.042068  [ 2656/ 3200]\n",
            "loss: 0.982707  [ 2672/ 3200]\n",
            "loss: 0.994672  [ 2688/ 3200]\n",
            "loss: 0.958970  [ 2704/ 3200]\n",
            "loss: 1.202361  [ 2720/ 3200]\n",
            "loss: 1.125051  [ 2736/ 3200]\n",
            "loss: 1.305155  [ 2752/ 3200]\n",
            "loss: 0.925641  [ 2768/ 3200]\n",
            "loss: 0.976798  [ 2784/ 3200]\n",
            "loss: 0.940927  [ 2800/ 3200]\n",
            "loss: 0.920342  [ 2816/ 3200]\n",
            "loss: 1.079447  [ 2832/ 3200]\n",
            "loss: 0.941305  [ 2848/ 3200]\n",
            "loss: 0.911258  [ 2864/ 3200]\n",
            "loss: 0.835276  [ 2880/ 3200]\n",
            "loss: 0.983994  [ 2896/ 3200]\n",
            "loss: 1.182775  [ 2912/ 3200]\n",
            "loss: 1.041848  [ 2928/ 3200]\n",
            "loss: 1.166635  [ 2944/ 3200]\n",
            "loss: 0.969836  [ 2960/ 3200]\n",
            "loss: 1.012415  [ 2976/ 3200]\n",
            "loss: 0.989405  [ 2992/ 3200]\n",
            "loss: 0.960358  [ 3008/ 3200]\n",
            "loss: 0.991452  [ 3024/ 3200]\n",
            "loss: 0.939382  [ 3040/ 3200]\n",
            "loss: 1.037897  [ 3056/ 3200]\n",
            "loss: 0.916609  [ 3072/ 3200]\n",
            "loss: 1.021583  [ 3088/ 3200]\n",
            "loss: 0.929887  [ 3104/ 3200]\n",
            "loss: 1.247635  [ 3120/ 3200]\n",
            "loss: 1.266930  [ 3136/ 3200]\n",
            "loss: 1.003016  [ 3152/ 3200]\n",
            "loss: 0.942367  [ 3168/ 3200]\n",
            "loss: 0.900015  [ 3184/ 3200]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1.062181  [    0/ 3200]\n",
            "loss: 0.922398  [   16/ 3200]\n",
            "loss: 1.123763  [   32/ 3200]\n",
            "loss: 0.862176  [   48/ 3200]\n",
            "loss: 0.936628  [   64/ 3200]\n",
            "loss: 1.186974  [   80/ 3200]\n",
            "loss: 1.040080  [   96/ 3200]\n",
            "loss: 1.235338  [  112/ 3200]\n",
            "loss: 0.814197  [  128/ 3200]\n",
            "loss: 0.918853  [  144/ 3200]\n",
            "loss: 1.122484  [  160/ 3200]\n",
            "loss: 0.935152  [  176/ 3200]\n",
            "loss: 1.219686  [  192/ 3200]\n",
            "loss: 0.891163  [  208/ 3200]\n",
            "loss: 0.974748  [  224/ 3200]\n",
            "loss: 0.933576  [  240/ 3200]\n",
            "loss: 1.048013  [  256/ 3200]\n",
            "loss: 0.949050  [  272/ 3200]\n",
            "loss: 1.152856  [  288/ 3200]\n",
            "loss: 0.914780  [  304/ 3200]\n",
            "loss: 0.836319  [  320/ 3200]\n",
            "loss: 0.829204  [  336/ 3200]\n",
            "loss: 1.297645  [  352/ 3200]\n",
            "loss: 0.754531  [  368/ 3200]\n",
            "loss: 1.058414  [  384/ 3200]\n",
            "loss: 0.876355  [  400/ 3200]\n",
            "loss: 0.979674  [  416/ 3200]\n",
            "loss: 1.150360  [  432/ 3200]\n",
            "loss: 1.214202  [  448/ 3200]\n",
            "loss: 0.888252  [  464/ 3200]\n",
            "loss: 0.838661  [  480/ 3200]\n",
            "loss: 0.983228  [  496/ 3200]\n",
            "loss: 1.025264  [  512/ 3200]\n",
            "loss: 1.439991  [  528/ 3200]\n",
            "loss: 0.955190  [  544/ 3200]\n",
            "loss: 0.863383  [  560/ 3200]\n",
            "loss: 1.023418  [  576/ 3200]\n",
            "loss: 1.191829  [  592/ 3200]\n",
            "loss: 1.504823  [  608/ 3200]\n",
            "loss: 0.968869  [  624/ 3200]\n",
            "loss: 1.062423  [  640/ 3200]\n",
            "loss: 1.029099  [  656/ 3200]\n",
            "loss: 0.942563  [  672/ 3200]\n",
            "loss: 1.276695  [  688/ 3200]\n",
            "loss: 0.997371  [  704/ 3200]\n",
            "loss: 0.985210  [  720/ 3200]\n",
            "loss: 0.831227  [  736/ 3200]\n",
            "loss: 0.875753  [  752/ 3200]\n",
            "loss: 0.855271  [  768/ 3200]\n",
            "loss: 1.052440  [  784/ 3200]\n",
            "loss: 1.099796  [  800/ 3200]\n",
            "loss: 0.863224  [  816/ 3200]\n",
            "loss: 1.144139  [  832/ 3200]\n",
            "loss: 1.059192  [  848/ 3200]\n",
            "loss: 1.016187  [  864/ 3200]\n",
            "loss: 1.171326  [  880/ 3200]\n",
            "loss: 0.972188  [  896/ 3200]\n",
            "loss: 0.951938  [  912/ 3200]\n",
            "loss: 0.966236  [  928/ 3200]\n",
            "loss: 1.148319  [  944/ 3200]\n",
            "loss: 0.911383  [  960/ 3200]\n",
            "loss: 1.063273  [  976/ 3200]\n",
            "loss: 1.104256  [  992/ 3200]\n",
            "loss: 0.983616  [ 1008/ 3200]\n",
            "loss: 0.947713  [ 1024/ 3200]\n",
            "loss: 1.150907  [ 1040/ 3200]\n",
            "loss: 0.938073  [ 1056/ 3200]\n",
            "loss: 0.792430  [ 1072/ 3200]\n",
            "loss: 1.150617  [ 1088/ 3200]\n",
            "loss: 1.090804  [ 1104/ 3200]\n",
            "loss: 0.838456  [ 1120/ 3200]\n",
            "loss: 0.979787  [ 1136/ 3200]\n",
            "loss: 1.095207  [ 1152/ 3200]\n",
            "loss: 0.882270  [ 1168/ 3200]\n",
            "loss: 1.155496  [ 1184/ 3200]\n",
            "loss: 1.050802  [ 1200/ 3200]\n",
            "loss: 1.055476  [ 1216/ 3200]\n",
            "loss: 1.089981  [ 1232/ 3200]\n",
            "loss: 1.109331  [ 1248/ 3200]\n",
            "loss: 0.895335  [ 1264/ 3200]\n",
            "loss: 0.735872  [ 1280/ 3200]\n",
            "loss: 0.786736  [ 1296/ 3200]\n",
            "loss: 0.757640  [ 1312/ 3200]\n",
            "loss: 1.014144  [ 1328/ 3200]\n",
            "loss: 0.953860  [ 1344/ 3200]\n",
            "loss: 0.834881  [ 1360/ 3200]\n",
            "loss: 0.939939  [ 1376/ 3200]\n",
            "loss: 0.966100  [ 1392/ 3200]\n",
            "loss: 1.157435  [ 1408/ 3200]\n",
            "loss: 0.814037  [ 1424/ 3200]\n",
            "loss: 1.183528  [ 1440/ 3200]\n",
            "loss: 0.935725  [ 1456/ 3200]\n",
            "loss: 0.816254  [ 1472/ 3200]\n",
            "loss: 0.971973  [ 1488/ 3200]\n",
            "loss: 0.942835  [ 1504/ 3200]\n",
            "loss: 1.163463  [ 1520/ 3200]\n",
            "loss: 1.208387  [ 1536/ 3200]\n",
            "loss: 0.906330  [ 1552/ 3200]\n",
            "loss: 1.658873  [ 1568/ 3200]\n",
            "loss: 1.085188  [ 1584/ 3200]\n",
            "loss: 1.107326  [ 1600/ 3200]\n",
            "loss: 0.868509  [ 1616/ 3200]\n",
            "loss: 1.165895  [ 1632/ 3200]\n",
            "loss: 0.897799  [ 1648/ 3200]\n",
            "loss: 1.030131  [ 1664/ 3200]\n",
            "loss: 1.014359  [ 1680/ 3200]\n",
            "loss: 1.013436  [ 1696/ 3200]\n",
            "loss: 0.853013  [ 1712/ 3200]\n",
            "loss: 0.822138  [ 1728/ 3200]\n",
            "loss: 0.872772  [ 1744/ 3200]\n",
            "loss: 0.809760  [ 1760/ 3200]\n",
            "loss: 0.570421  [ 1776/ 3200]\n",
            "loss: 0.986386  [ 1792/ 3200]\n",
            "loss: 1.072135  [ 1808/ 3200]\n",
            "loss: 0.829803  [ 1824/ 3200]\n",
            "loss: 1.083807  [ 1840/ 3200]\n",
            "loss: 0.897661  [ 1856/ 3200]\n",
            "loss: 1.295872  [ 1872/ 3200]\n",
            "loss: 0.857004  [ 1888/ 3200]\n",
            "loss: 0.934274  [ 1904/ 3200]\n",
            "loss: 1.013877  [ 1920/ 3200]\n",
            "loss: 1.154331  [ 1936/ 3200]\n",
            "loss: 0.899644  [ 1952/ 3200]\n",
            "loss: 1.088811  [ 1968/ 3200]\n",
            "loss: 1.101644  [ 1984/ 3200]\n",
            "loss: 1.005743  [ 2000/ 3200]\n",
            "loss: 0.892886  [ 2016/ 3200]\n",
            "loss: 1.047147  [ 2032/ 3200]\n",
            "loss: 0.907505  [ 2048/ 3200]\n",
            "loss: 0.928309  [ 2064/ 3200]\n",
            "loss: 0.958561  [ 2080/ 3200]\n",
            "loss: 0.926662  [ 2096/ 3200]\n",
            "loss: 0.970148  [ 2112/ 3200]\n",
            "loss: 0.945801  [ 2128/ 3200]\n",
            "loss: 1.072030  [ 2144/ 3200]\n",
            "loss: 1.244141  [ 2160/ 3200]\n",
            "loss: 1.119696  [ 2176/ 3200]\n",
            "loss: 0.806736  [ 2192/ 3200]\n",
            "loss: 0.628874  [ 2208/ 3200]\n",
            "loss: 1.012361  [ 2224/ 3200]\n",
            "loss: 0.891296  [ 2240/ 3200]\n",
            "loss: 0.923267  [ 2256/ 3200]\n",
            "loss: 0.827019  [ 2272/ 3200]\n",
            "loss: 0.997908  [ 2288/ 3200]\n",
            "loss: 0.930084  [ 2304/ 3200]\n",
            "loss: 1.025169  [ 2320/ 3200]\n",
            "loss: 0.902688  [ 2336/ 3200]\n",
            "loss: 1.062357  [ 2352/ 3200]\n",
            "loss: 0.884916  [ 2368/ 3200]\n",
            "loss: 0.967955  [ 2384/ 3200]\n",
            "loss: 0.820679  [ 2400/ 3200]\n",
            "loss: 0.956558  [ 2416/ 3200]\n",
            "loss: 0.899133  [ 2432/ 3200]\n",
            "loss: 0.814219  [ 2448/ 3200]\n",
            "loss: 1.163723  [ 2464/ 3200]\n",
            "loss: 0.804234  [ 2480/ 3200]\n",
            "loss: 1.324997  [ 2496/ 3200]\n",
            "loss: 0.721662  [ 2512/ 3200]\n",
            "loss: 1.042977  [ 2528/ 3200]\n",
            "loss: 0.870089  [ 2544/ 3200]\n",
            "loss: 0.791521  [ 2560/ 3200]\n",
            "loss: 0.919572  [ 2576/ 3200]\n",
            "loss: 1.117859  [ 2592/ 3200]\n",
            "loss: 0.795796  [ 2608/ 3200]\n",
            "loss: 0.998938  [ 2624/ 3200]\n",
            "loss: 1.037155  [ 2640/ 3200]\n",
            "loss: 0.901123  [ 2656/ 3200]\n",
            "loss: 1.086243  [ 2672/ 3200]\n",
            "loss: 0.692612  [ 2688/ 3200]\n",
            "loss: 0.981223  [ 2704/ 3200]\n",
            "loss: 0.801257  [ 2720/ 3200]\n",
            "loss: 0.783144  [ 2736/ 3200]\n",
            "loss: 1.020831  [ 2752/ 3200]\n",
            "loss: 1.080948  [ 2768/ 3200]\n",
            "loss: 1.062058  [ 2784/ 3200]\n",
            "loss: 1.176980  [ 2800/ 3200]\n",
            "loss: 1.353626  [ 2816/ 3200]\n",
            "loss: 0.928835  [ 2832/ 3200]\n",
            "loss: 0.904010  [ 2848/ 3200]\n",
            "loss: 0.772893  [ 2864/ 3200]\n",
            "loss: 1.031234  [ 2880/ 3200]\n",
            "loss: 1.118447  [ 2896/ 3200]\n",
            "loss: 1.078999  [ 2912/ 3200]\n",
            "loss: 1.136183  [ 2928/ 3200]\n",
            "loss: 1.118620  [ 2944/ 3200]\n",
            "loss: 0.919990  [ 2960/ 3200]\n",
            "loss: 0.881451  [ 2976/ 3200]\n",
            "loss: 1.235694  [ 2992/ 3200]\n",
            "loss: 0.980726  [ 3008/ 3200]\n",
            "loss: 1.033649  [ 3024/ 3200]\n",
            "loss: 1.162261  [ 3040/ 3200]\n",
            "loss: 0.978564  [ 3056/ 3200]\n",
            "loss: 0.969153  [ 3072/ 3200]\n",
            "loss: 0.813084  [ 3088/ 3200]\n",
            "loss: 0.982931  [ 3104/ 3200]\n",
            "loss: 0.860284  [ 3120/ 3200]\n",
            "loss: 1.191727  [ 3136/ 3200]\n",
            "loss: 0.949617  [ 3152/ 3200]\n",
            "loss: 1.129753  [ 3168/ 3200]\n",
            "loss: 1.120020  [ 3184/ 3200]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.801980  [    0/ 3200]\n",
            "loss: 1.044601  [   16/ 3200]\n",
            "loss: 0.870803  [   32/ 3200]\n",
            "loss: 0.943714  [   48/ 3200]\n",
            "loss: 0.940212  [   64/ 3200]\n",
            "loss: 0.745412  [   80/ 3200]\n",
            "loss: 0.863983  [   96/ 3200]\n",
            "loss: 0.973717  [  112/ 3200]\n",
            "loss: 0.913940  [  128/ 3200]\n",
            "loss: 0.993122  [  144/ 3200]\n",
            "loss: 1.141262  [  160/ 3200]\n",
            "loss: 1.046753  [  176/ 3200]\n",
            "loss: 1.187433  [  192/ 3200]\n",
            "loss: 0.769008  [  208/ 3200]\n",
            "loss: 1.072174  [  224/ 3200]\n",
            "loss: 1.086463  [  240/ 3200]\n",
            "loss: 0.768786  [  256/ 3200]\n",
            "loss: 1.177135  [  272/ 3200]\n",
            "loss: 1.124823  [  288/ 3200]\n",
            "loss: 1.103190  [  304/ 3200]\n",
            "loss: 1.024545  [  320/ 3200]\n",
            "loss: 1.202964  [  336/ 3200]\n",
            "loss: 1.240462  [  352/ 3200]\n",
            "loss: 1.015601  [  368/ 3200]\n",
            "loss: 1.030714  [  384/ 3200]\n",
            "loss: 1.283000  [  400/ 3200]\n",
            "loss: 1.075268  [  416/ 3200]\n",
            "loss: 1.061695  [  432/ 3200]\n",
            "loss: 1.150190  [  448/ 3200]\n",
            "loss: 1.235607  [  464/ 3200]\n",
            "loss: 0.977594  [  480/ 3200]\n",
            "loss: 0.839062  [  496/ 3200]\n",
            "loss: 1.153522  [  512/ 3200]\n",
            "loss: 0.999047  [  528/ 3200]\n",
            "loss: 0.845647  [  544/ 3200]\n",
            "loss: 0.994621  [  560/ 3200]\n",
            "loss: 0.904074  [  576/ 3200]\n",
            "loss: 1.073858  [  592/ 3200]\n",
            "loss: 0.860303  [  608/ 3200]\n",
            "loss: 0.984602  [  624/ 3200]\n",
            "loss: 0.884005  [  640/ 3200]\n",
            "loss: 0.972596  [  656/ 3200]\n",
            "loss: 1.150728  [  672/ 3200]\n",
            "loss: 0.923953  [  688/ 3200]\n",
            "loss: 0.748361  [  704/ 3200]\n",
            "loss: 1.192397  [  720/ 3200]\n",
            "loss: 0.915738  [  736/ 3200]\n",
            "loss: 0.901809  [  752/ 3200]\n",
            "loss: 1.243106  [  768/ 3200]\n",
            "loss: 0.982630  [  784/ 3200]\n",
            "loss: 0.929578  [  800/ 3200]\n",
            "loss: 0.933368  [  816/ 3200]\n",
            "loss: 0.919470  [  832/ 3200]\n",
            "loss: 0.820091  [  848/ 3200]\n",
            "loss: 1.060431  [  864/ 3200]\n",
            "loss: 1.200852  [  880/ 3200]\n",
            "loss: 1.081505  [  896/ 3200]\n",
            "loss: 1.108846  [  912/ 3200]\n",
            "loss: 0.967291  [  928/ 3200]\n",
            "loss: 1.156536  [  944/ 3200]\n",
            "loss: 0.973643  [  960/ 3200]\n",
            "loss: 1.088231  [  976/ 3200]\n",
            "loss: 0.930534  [  992/ 3200]\n",
            "loss: 0.886841  [ 1008/ 3200]\n",
            "loss: 1.122901  [ 1024/ 3200]\n",
            "loss: 0.992124  [ 1040/ 3200]\n",
            "loss: 0.920556  [ 1056/ 3200]\n",
            "loss: 0.915257  [ 1072/ 3200]\n",
            "loss: 0.851375  [ 1088/ 3200]\n",
            "loss: 0.962310  [ 1104/ 3200]\n",
            "loss: 0.984693  [ 1120/ 3200]\n",
            "loss: 0.938147  [ 1136/ 3200]\n",
            "loss: 1.200957  [ 1152/ 3200]\n",
            "loss: 0.986904  [ 1168/ 3200]\n",
            "loss: 1.014971  [ 1184/ 3200]\n",
            "loss: 1.009659  [ 1200/ 3200]\n",
            "loss: 0.869930  [ 1216/ 3200]\n",
            "loss: 0.931508  [ 1232/ 3200]\n",
            "loss: 0.915781  [ 1248/ 3200]\n",
            "loss: 0.785692  [ 1264/ 3200]\n",
            "loss: 1.109895  [ 1280/ 3200]\n",
            "loss: 0.896679  [ 1296/ 3200]\n",
            "loss: 0.818829  [ 1312/ 3200]\n",
            "loss: 1.087015  [ 1328/ 3200]\n",
            "loss: 0.980698  [ 1344/ 3200]\n",
            "loss: 0.991953  [ 1360/ 3200]\n",
            "loss: 0.900656  [ 1376/ 3200]\n",
            "loss: 1.050133  [ 1392/ 3200]\n",
            "loss: 1.007135  [ 1408/ 3200]\n",
            "loss: 0.896026  [ 1424/ 3200]\n",
            "loss: 1.081993  [ 1440/ 3200]\n",
            "loss: 1.164622  [ 1456/ 3200]\n",
            "loss: 0.941236  [ 1472/ 3200]\n",
            "loss: 0.915884  [ 1488/ 3200]\n",
            "loss: 0.952795  [ 1504/ 3200]\n",
            "loss: 0.989252  [ 1520/ 3200]\n",
            "loss: 0.799772  [ 1536/ 3200]\n",
            "loss: 1.079550  [ 1552/ 3200]\n",
            "loss: 1.122746  [ 1568/ 3200]\n",
            "loss: 1.200843  [ 1584/ 3200]\n",
            "loss: 1.089495  [ 1600/ 3200]\n",
            "loss: 0.916925  [ 1616/ 3200]\n",
            "loss: 1.042121  [ 1632/ 3200]\n",
            "loss: 0.976476  [ 1648/ 3200]\n",
            "loss: 1.329850  [ 1664/ 3200]\n",
            "loss: 0.939787  [ 1680/ 3200]\n",
            "loss: 0.875288  [ 1696/ 3200]\n",
            "loss: 1.131418  [ 1712/ 3200]\n",
            "loss: 0.894108  [ 1728/ 3200]\n",
            "loss: 1.054357  [ 1744/ 3200]\n",
            "loss: 0.949009  [ 1760/ 3200]\n",
            "loss: 0.871911  [ 1776/ 3200]\n",
            "loss: 0.997689  [ 1792/ 3200]\n",
            "loss: 0.811374  [ 1808/ 3200]\n",
            "loss: 1.184060  [ 1824/ 3200]\n",
            "loss: 1.084352  [ 1840/ 3200]\n",
            "loss: 0.943278  [ 1856/ 3200]\n",
            "loss: 0.815702  [ 1872/ 3200]\n",
            "loss: 0.829218  [ 1888/ 3200]\n",
            "loss: 1.119591  [ 1904/ 3200]\n",
            "loss: 0.888332  [ 1920/ 3200]\n",
            "loss: 1.186526  [ 1936/ 3200]\n",
            "loss: 0.869331  [ 1952/ 3200]\n",
            "loss: 1.267245  [ 1968/ 3200]\n",
            "loss: 0.974687  [ 1984/ 3200]\n",
            "loss: 1.113390  [ 2000/ 3200]\n",
            "loss: 0.729716  [ 2016/ 3200]\n",
            "loss: 0.838679  [ 2032/ 3200]\n",
            "loss: 0.940664  [ 2048/ 3200]\n",
            "loss: 0.928724  [ 2064/ 3200]\n",
            "loss: 0.903230  [ 2080/ 3200]\n",
            "loss: 0.867207  [ 2096/ 3200]\n",
            "loss: 0.879272  [ 2112/ 3200]\n",
            "loss: 0.827259  [ 2128/ 3200]\n",
            "loss: 1.070213  [ 2144/ 3200]\n",
            "loss: 1.065597  [ 2160/ 3200]\n",
            "loss: 1.147097  [ 2176/ 3200]\n",
            "loss: 1.076409  [ 2192/ 3200]\n",
            "loss: 0.737777  [ 2208/ 3200]\n",
            "loss: 0.844854  [ 2224/ 3200]\n",
            "loss: 1.126161  [ 2240/ 3200]\n",
            "loss: 0.919212  [ 2256/ 3200]\n",
            "loss: 1.106324  [ 2272/ 3200]\n",
            "loss: 1.098279  [ 2288/ 3200]\n",
            "loss: 1.297437  [ 2304/ 3200]\n",
            "loss: 0.824816  [ 2320/ 3200]\n",
            "loss: 1.052860  [ 2336/ 3200]\n",
            "loss: 0.878294  [ 2352/ 3200]\n",
            "loss: 0.949395  [ 2368/ 3200]\n",
            "loss: 0.866964  [ 2384/ 3200]\n",
            "loss: 0.893616  [ 2400/ 3200]\n",
            "loss: 0.953939  [ 2416/ 3200]\n",
            "loss: 0.880225  [ 2432/ 3200]\n",
            "loss: 1.027867  [ 2448/ 3200]\n",
            "loss: 1.058432  [ 2464/ 3200]\n",
            "loss: 1.057073  [ 2480/ 3200]\n",
            "loss: 0.839048  [ 2496/ 3200]\n",
            "loss: 1.169175  [ 2512/ 3200]\n",
            "loss: 0.833196  [ 2528/ 3200]\n",
            "loss: 1.090973  [ 2544/ 3200]\n",
            "loss: 1.036669  [ 2560/ 3200]\n",
            "loss: 1.046187  [ 2576/ 3200]\n",
            "loss: 0.889647  [ 2592/ 3200]\n",
            "loss: 0.958342  [ 2608/ 3200]\n",
            "loss: 0.776956  [ 2624/ 3200]\n",
            "loss: 1.015460  [ 2640/ 3200]\n",
            "loss: 0.862094  [ 2656/ 3200]\n",
            "loss: 0.963897  [ 2672/ 3200]\n",
            "loss: 0.931068  [ 2688/ 3200]\n",
            "loss: 1.099766  [ 2704/ 3200]\n",
            "loss: 0.886587  [ 2720/ 3200]\n",
            "loss: 1.023971  [ 2736/ 3200]\n",
            "loss: 0.915769  [ 2752/ 3200]\n",
            "loss: 1.045823  [ 2768/ 3200]\n",
            "loss: 0.939062  [ 2784/ 3200]\n",
            "loss: 0.774372  [ 2800/ 3200]\n",
            "loss: 0.688163  [ 2816/ 3200]\n",
            "loss: 0.729563  [ 2832/ 3200]\n",
            "loss: 1.249641  [ 2848/ 3200]\n",
            "loss: 0.983015  [ 2864/ 3200]\n",
            "loss: 0.947246  [ 2880/ 3200]\n",
            "loss: 0.997810  [ 2896/ 3200]\n",
            "loss: 0.819873  [ 2912/ 3200]\n",
            "loss: 0.964492  [ 2928/ 3200]\n",
            "loss: 1.051786  [ 2944/ 3200]\n",
            "loss: 0.940978  [ 2960/ 3200]\n",
            "loss: 1.471475  [ 2976/ 3200]\n",
            "loss: 1.011308  [ 2992/ 3200]\n",
            "loss: 0.932143  [ 3008/ 3200]\n",
            "loss: 0.914809  [ 3024/ 3200]\n",
            "loss: 1.030538  [ 3040/ 3200]\n",
            "loss: 0.983056  [ 3056/ 3200]\n",
            "loss: 1.142486  [ 3072/ 3200]\n",
            "loss: 1.161006  [ 3088/ 3200]\n",
            "loss: 0.799711  [ 3104/ 3200]\n",
            "loss: 0.708104  [ 3120/ 3200]\n",
            "loss: 0.766458  [ 3136/ 3200]\n",
            "loss: 0.912009  [ 3152/ 3200]\n",
            "loss: 0.946107  [ 3168/ 3200]\n",
            "loss: 0.834829  [ 3184/ 3200]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.929710  [    0/ 3200]\n",
            "loss: 1.117033  [   16/ 3200]\n",
            "loss: 0.896330  [   32/ 3200]\n",
            "loss: 0.949642  [   48/ 3200]\n",
            "loss: 0.834515  [   64/ 3200]\n",
            "loss: 1.231525  [   80/ 3200]\n",
            "loss: 0.804620  [   96/ 3200]\n",
            "loss: 1.076821  [  112/ 3200]\n",
            "loss: 0.936674  [  128/ 3200]\n",
            "loss: 0.932963  [  144/ 3200]\n",
            "loss: 0.824805  [  160/ 3200]\n",
            "loss: 1.335520  [  176/ 3200]\n",
            "loss: 1.066458  [  192/ 3200]\n",
            "loss: 0.961685  [  208/ 3200]\n",
            "loss: 0.772617  [  224/ 3200]\n",
            "loss: 1.090356  [  240/ 3200]\n",
            "loss: 0.841689  [  256/ 3200]\n",
            "loss: 0.749480  [  272/ 3200]\n",
            "loss: 1.012098  [  288/ 3200]\n",
            "loss: 0.937738  [  304/ 3200]\n",
            "loss: 0.972512  [  320/ 3200]\n",
            "loss: 0.843287  [  336/ 3200]\n",
            "loss: 0.852433  [  352/ 3200]\n",
            "loss: 0.985480  [  368/ 3200]\n",
            "loss: 0.681904  [  384/ 3200]\n",
            "loss: 1.170181  [  400/ 3200]\n",
            "loss: 0.671064  [  416/ 3200]\n",
            "loss: 0.935401  [  432/ 3200]\n",
            "loss: 1.039895  [  448/ 3200]\n",
            "loss: 0.769270  [  464/ 3200]\n",
            "loss: 0.908159  [  480/ 3200]\n",
            "loss: 1.065278  [  496/ 3200]\n",
            "loss: 0.974964  [  512/ 3200]\n",
            "loss: 0.961170  [  528/ 3200]\n",
            "loss: 0.706125  [  544/ 3200]\n",
            "loss: 0.926641  [  560/ 3200]\n",
            "loss: 0.882279  [  576/ 3200]\n",
            "loss: 0.853993  [  592/ 3200]\n",
            "loss: 0.778072  [  608/ 3200]\n",
            "loss: 0.966123  [  624/ 3200]\n",
            "loss: 1.224637  [  640/ 3200]\n",
            "loss: 1.322685  [  656/ 3200]\n",
            "loss: 0.702136  [  672/ 3200]\n",
            "loss: 1.357434  [  688/ 3200]\n",
            "loss: 1.083032  [  704/ 3200]\n",
            "loss: 0.944692  [  720/ 3200]\n",
            "loss: 1.068086  [  736/ 3200]\n",
            "loss: 0.980553  [  752/ 3200]\n",
            "loss: 0.799593  [  768/ 3200]\n",
            "loss: 1.129410  [  784/ 3200]\n",
            "loss: 0.964835  [  800/ 3200]\n",
            "loss: 1.124830  [  816/ 3200]\n",
            "loss: 1.397925  [  832/ 3200]\n",
            "loss: 0.732891  [  848/ 3200]\n",
            "loss: 0.830683  [  864/ 3200]\n",
            "loss: 0.790871  [  880/ 3200]\n",
            "loss: 0.802721  [  896/ 3200]\n",
            "loss: 0.964602  [  912/ 3200]\n",
            "loss: 0.865746  [  928/ 3200]\n",
            "loss: 1.051496  [  944/ 3200]\n",
            "loss: 1.175122  [  960/ 3200]\n",
            "loss: 0.779852  [  976/ 3200]\n",
            "loss: 1.279391  [  992/ 3200]\n",
            "loss: 1.139809  [ 1008/ 3200]\n",
            "loss: 1.211723  [ 1024/ 3200]\n",
            "loss: 0.816230  [ 1040/ 3200]\n",
            "loss: 0.928617  [ 1056/ 3200]\n",
            "loss: 1.038679  [ 1072/ 3200]\n",
            "loss: 0.985712  [ 1088/ 3200]\n",
            "loss: 0.796775  [ 1104/ 3200]\n",
            "loss: 1.108999  [ 1120/ 3200]\n",
            "loss: 1.188303  [ 1136/ 3200]\n",
            "loss: 0.902659  [ 1152/ 3200]\n",
            "loss: 0.979155  [ 1168/ 3200]\n",
            "loss: 0.845658  [ 1184/ 3200]\n",
            "loss: 1.399172  [ 1200/ 3200]\n",
            "loss: 1.189981  [ 1216/ 3200]\n",
            "loss: 0.976328  [ 1232/ 3200]\n",
            "loss: 1.180794  [ 1248/ 3200]\n",
            "loss: 0.882690  [ 1264/ 3200]\n",
            "loss: 1.117549  [ 1280/ 3200]\n",
            "loss: 1.111585  [ 1296/ 3200]\n",
            "loss: 0.958695  [ 1312/ 3200]\n",
            "loss: 1.054025  [ 1328/ 3200]\n",
            "loss: 1.052439  [ 1344/ 3200]\n",
            "loss: 0.744463  [ 1360/ 3200]\n",
            "loss: 0.780364  [ 1376/ 3200]\n",
            "loss: 0.953017  [ 1392/ 3200]\n",
            "loss: 0.807900  [ 1408/ 3200]\n",
            "loss: 0.931231  [ 1424/ 3200]\n",
            "loss: 1.176340  [ 1440/ 3200]\n",
            "loss: 1.152516  [ 1456/ 3200]\n",
            "loss: 0.782903  [ 1472/ 3200]\n",
            "loss: 0.982746  [ 1488/ 3200]\n",
            "loss: 1.031757  [ 1504/ 3200]\n",
            "loss: 0.678088  [ 1520/ 3200]\n",
            "loss: 1.249084  [ 1536/ 3200]\n",
            "loss: 0.729964  [ 1552/ 3200]\n",
            "loss: 0.670255  [ 1568/ 3200]\n",
            "loss: 1.022091  [ 1584/ 3200]\n",
            "loss: 1.029048  [ 1600/ 3200]\n",
            "loss: 1.081758  [ 1616/ 3200]\n",
            "loss: 0.823726  [ 1632/ 3200]\n",
            "loss: 1.063460  [ 1648/ 3200]\n",
            "loss: 0.909866  [ 1664/ 3200]\n",
            "loss: 0.885861  [ 1680/ 3200]\n",
            "loss: 0.997302  [ 1696/ 3200]\n",
            "loss: 1.126065  [ 1712/ 3200]\n",
            "loss: 1.122625  [ 1728/ 3200]\n",
            "loss: 0.776669  [ 1744/ 3200]\n",
            "loss: 1.111873  [ 1760/ 3200]\n",
            "loss: 1.134453  [ 1776/ 3200]\n",
            "loss: 0.780808  [ 1792/ 3200]\n",
            "loss: 0.873955  [ 1808/ 3200]\n",
            "loss: 1.181594  [ 1824/ 3200]\n",
            "loss: 0.771165  [ 1840/ 3200]\n",
            "loss: 0.816340  [ 1856/ 3200]\n",
            "loss: 1.105968  [ 1872/ 3200]\n",
            "loss: 0.760119  [ 1888/ 3200]\n",
            "loss: 0.870250  [ 1904/ 3200]\n",
            "loss: 1.227633  [ 1920/ 3200]\n",
            "loss: 1.040663  [ 1936/ 3200]\n",
            "loss: 1.207392  [ 1952/ 3200]\n",
            "loss: 1.185893  [ 1968/ 3200]\n",
            "loss: 0.806614  [ 1984/ 3200]\n",
            "loss: 1.301914  [ 2000/ 3200]\n",
            "loss: 0.905219  [ 2016/ 3200]\n",
            "loss: 0.808681  [ 2032/ 3200]\n",
            "loss: 1.053149  [ 2048/ 3200]\n",
            "loss: 1.036229  [ 2064/ 3200]\n",
            "loss: 0.955441  [ 2080/ 3200]\n",
            "loss: 1.113749  [ 2096/ 3200]\n",
            "loss: 0.890996  [ 2112/ 3200]\n",
            "loss: 0.743980  [ 2128/ 3200]\n",
            "loss: 1.234671  [ 2144/ 3200]\n",
            "loss: 0.984657  [ 2160/ 3200]\n",
            "loss: 1.229173  [ 2176/ 3200]\n",
            "loss: 0.823907  [ 2192/ 3200]\n",
            "loss: 0.863742  [ 2208/ 3200]\n",
            "loss: 0.926056  [ 2224/ 3200]\n",
            "loss: 0.750435  [ 2240/ 3200]\n",
            "loss: 0.958539  [ 2256/ 3200]\n",
            "loss: 0.773605  [ 2272/ 3200]\n",
            "loss: 0.978325  [ 2288/ 3200]\n",
            "loss: 1.301848  [ 2304/ 3200]\n",
            "loss: 1.160865  [ 2320/ 3200]\n",
            "loss: 0.995455  [ 2336/ 3200]\n",
            "loss: 1.244024  [ 2352/ 3200]\n",
            "loss: 0.862699  [ 2368/ 3200]\n",
            "loss: 0.718056  [ 2384/ 3200]\n",
            "loss: 1.117237  [ 2400/ 3200]\n",
            "loss: 0.901274  [ 2416/ 3200]\n",
            "loss: 0.806916  [ 2432/ 3200]\n",
            "loss: 0.772755  [ 2448/ 3200]\n",
            "loss: 0.861506  [ 2464/ 3200]\n",
            "loss: 0.937883  [ 2480/ 3200]\n",
            "loss: 0.917501  [ 2496/ 3200]\n",
            "loss: 0.929684  [ 2512/ 3200]\n",
            "loss: 1.056041  [ 2528/ 3200]\n",
            "loss: 0.942657  [ 2544/ 3200]\n",
            "loss: 0.938302  [ 2560/ 3200]\n",
            "loss: 0.762926  [ 2576/ 3200]\n",
            "loss: 0.849362  [ 2592/ 3200]\n",
            "loss: 1.016816  [ 2608/ 3200]\n",
            "loss: 0.924655  [ 2624/ 3200]\n",
            "loss: 0.957276  [ 2640/ 3200]\n",
            "loss: 1.019740  [ 2656/ 3200]\n",
            "loss: 0.945329  [ 2672/ 3200]\n",
            "loss: 0.921411  [ 2688/ 3200]\n",
            "loss: 0.928078  [ 2704/ 3200]\n",
            "loss: 1.043729  [ 2720/ 3200]\n",
            "loss: 1.410478  [ 2736/ 3200]\n",
            "loss: 1.208609  [ 2752/ 3200]\n",
            "loss: 0.967433  [ 2768/ 3200]\n",
            "loss: 1.112133  [ 2784/ 3200]\n",
            "loss: 0.835795  [ 2800/ 3200]\n",
            "loss: 0.901257  [ 2816/ 3200]\n",
            "loss: 0.960619  [ 2832/ 3200]\n",
            "loss: 0.908544  [ 2848/ 3200]\n",
            "loss: 1.001944  [ 2864/ 3200]\n",
            "loss: 0.990328  [ 2880/ 3200]\n",
            "loss: 0.798766  [ 2896/ 3200]\n",
            "loss: 1.312690  [ 2912/ 3200]\n",
            "loss: 0.919040  [ 2928/ 3200]\n",
            "loss: 0.935215  [ 2944/ 3200]\n",
            "loss: 0.959682  [ 2960/ 3200]\n",
            "loss: 1.092193  [ 2976/ 3200]\n",
            "loss: 1.449664  [ 2992/ 3200]\n",
            "loss: 1.096411  [ 3008/ 3200]\n",
            "loss: 0.759608  [ 3024/ 3200]\n",
            "loss: 1.175769  [ 3040/ 3200]\n",
            "loss: 0.707729  [ 3056/ 3200]\n",
            "loss: 1.149006  [ 3072/ 3200]\n",
            "loss: 1.271237  [ 3088/ 3200]\n",
            "loss: 1.064560  [ 3104/ 3200]\n",
            "loss: 1.047472  [ 3120/ 3200]\n",
            "loss: 0.723565  [ 3136/ 3200]\n",
            "loss: 0.789176  [ 3152/ 3200]\n",
            "loss: 0.807775  [ 3168/ 3200]\n",
            "loss: 0.709074  [ 3184/ 3200]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.997490  [    0/ 3200]\n",
            "loss: 1.120528  [   16/ 3200]\n",
            "loss: 0.970643  [   32/ 3200]\n",
            "loss: 0.947058  [   48/ 3200]\n",
            "loss: 0.778153  [   64/ 3200]\n",
            "loss: 1.008817  [   80/ 3200]\n",
            "loss: 1.085096  [   96/ 3200]\n",
            "loss: 0.712499  [  112/ 3200]\n",
            "loss: 0.941743  [  128/ 3200]\n",
            "loss: 0.876923  [  144/ 3200]\n",
            "loss: 1.039963  [  160/ 3200]\n",
            "loss: 0.935136  [  176/ 3200]\n",
            "loss: 0.875162  [  192/ 3200]\n",
            "loss: 0.926913  [  208/ 3200]\n",
            "loss: 1.090068  [  224/ 3200]\n",
            "loss: 1.136827  [  240/ 3200]\n",
            "loss: 0.705740  [  256/ 3200]\n",
            "loss: 0.897883  [  272/ 3200]\n",
            "loss: 0.871752  [  288/ 3200]\n",
            "loss: 0.990125  [  304/ 3200]\n",
            "loss: 0.853994  [  320/ 3200]\n",
            "loss: 0.822874  [  336/ 3200]\n",
            "loss: 1.139832  [  352/ 3200]\n",
            "loss: 0.687259  [  368/ 3200]\n",
            "loss: 0.697342  [  384/ 3200]\n",
            "loss: 0.999078  [  400/ 3200]\n",
            "loss: 0.892206  [  416/ 3200]\n",
            "loss: 0.998871  [  432/ 3200]\n",
            "loss: 0.934312  [  448/ 3200]\n",
            "loss: 0.960861  [  464/ 3200]\n",
            "loss: 0.990561  [  480/ 3200]\n",
            "loss: 0.698694  [  496/ 3200]\n",
            "loss: 1.086442  [  512/ 3200]\n",
            "loss: 0.951182  [  528/ 3200]\n",
            "loss: 1.266515  [  544/ 3200]\n",
            "loss: 0.769514  [  560/ 3200]\n",
            "loss: 0.835954  [  576/ 3200]\n",
            "loss: 0.848344  [  592/ 3200]\n",
            "loss: 1.156874  [  608/ 3200]\n",
            "loss: 0.952857  [  624/ 3200]\n",
            "loss: 1.250678  [  640/ 3200]\n",
            "loss: 0.971154  [  656/ 3200]\n",
            "loss: 0.983776  [  672/ 3200]\n",
            "loss: 0.912125  [  688/ 3200]\n",
            "loss: 0.948407  [  704/ 3200]\n",
            "loss: 0.802556  [  720/ 3200]\n",
            "loss: 1.048884  [  736/ 3200]\n",
            "loss: 0.914638  [  752/ 3200]\n",
            "loss: 1.061588  [  768/ 3200]\n",
            "loss: 0.776654  [  784/ 3200]\n",
            "loss: 0.957609  [  800/ 3200]\n",
            "loss: 1.054345  [  816/ 3200]\n",
            "loss: 1.090220  [  832/ 3200]\n",
            "loss: 1.072207  [  848/ 3200]\n",
            "loss: 0.837143  [  864/ 3200]\n",
            "loss: 1.002454  [  880/ 3200]\n",
            "loss: 1.077539  [  896/ 3200]\n",
            "loss: 1.127932  [  912/ 3200]\n",
            "loss: 0.667140  [  928/ 3200]\n",
            "loss: 1.230549  [  944/ 3200]\n",
            "loss: 0.853088  [  960/ 3200]\n",
            "loss: 0.915662  [  976/ 3200]\n",
            "loss: 0.886913  [  992/ 3200]\n",
            "loss: 1.084013  [ 1008/ 3200]\n",
            "loss: 0.925118  [ 1024/ 3200]\n",
            "loss: 1.138711  [ 1040/ 3200]\n",
            "loss: 0.734630  [ 1056/ 3200]\n",
            "loss: 1.098833  [ 1072/ 3200]\n",
            "loss: 1.302125  [ 1088/ 3200]\n",
            "loss: 0.919110  [ 1104/ 3200]\n",
            "loss: 1.935517  [ 1120/ 3200]\n",
            "loss: 1.182190  [ 1136/ 3200]\n",
            "loss: 0.713003  [ 1152/ 3200]\n",
            "loss: 0.940948  [ 1168/ 3200]\n",
            "loss: 0.736027  [ 1184/ 3200]\n",
            "loss: 1.050345  [ 1200/ 3200]\n",
            "loss: 1.166109  [ 1216/ 3200]\n",
            "loss: 0.758221  [ 1232/ 3200]\n",
            "loss: 0.837523  [ 1248/ 3200]\n",
            "loss: 0.873896  [ 1264/ 3200]\n",
            "loss: 1.081894  [ 1280/ 3200]\n",
            "loss: 0.928354  [ 1296/ 3200]\n",
            "loss: 1.050341  [ 1312/ 3200]\n",
            "loss: 0.986454  [ 1328/ 3200]\n",
            "loss: 0.898895  [ 1344/ 3200]\n",
            "loss: 0.793572  [ 1360/ 3200]\n",
            "loss: 1.098411  [ 1376/ 3200]\n",
            "loss: 0.927337  [ 1392/ 3200]\n",
            "loss: 1.149683  [ 1408/ 3200]\n",
            "loss: 0.903310  [ 1424/ 3200]\n",
            "loss: 1.021548  [ 1440/ 3200]\n",
            "loss: 0.864696  [ 1456/ 3200]\n",
            "loss: 0.652547  [ 1472/ 3200]\n",
            "loss: 0.744642  [ 1488/ 3200]\n",
            "loss: 1.028944  [ 1504/ 3200]\n",
            "loss: 0.873548  [ 1520/ 3200]\n",
            "loss: 1.279385  [ 1536/ 3200]\n",
            "loss: 0.814829  [ 1552/ 3200]\n",
            "loss: 1.154550  [ 1568/ 3200]\n",
            "loss: 0.926898  [ 1584/ 3200]\n",
            "loss: 0.833219  [ 1600/ 3200]\n",
            "loss: 0.796884  [ 1616/ 3200]\n",
            "loss: 1.245753  [ 1632/ 3200]\n",
            "loss: 1.060515  [ 1648/ 3200]\n",
            "loss: 0.774925  [ 1664/ 3200]\n",
            "loss: 1.004541  [ 1680/ 3200]\n",
            "loss: 0.951859  [ 1696/ 3200]\n",
            "loss: 0.799835  [ 1712/ 3200]\n",
            "loss: 0.806445  [ 1728/ 3200]\n",
            "loss: 0.779187  [ 1744/ 3200]\n",
            "loss: 1.134336  [ 1760/ 3200]\n",
            "loss: 0.985686  [ 1776/ 3200]\n",
            "loss: 1.021663  [ 1792/ 3200]\n",
            "loss: 0.760576  [ 1808/ 3200]\n",
            "loss: 1.032436  [ 1824/ 3200]\n",
            "loss: 0.922865  [ 1840/ 3200]\n",
            "loss: 1.205845  [ 1856/ 3200]\n",
            "loss: 1.087992  [ 1872/ 3200]\n",
            "loss: 1.217742  [ 1888/ 3200]\n",
            "loss: 1.021068  [ 1904/ 3200]\n",
            "loss: 0.896041  [ 1920/ 3200]\n",
            "loss: 0.687545  [ 1936/ 3200]\n",
            "loss: 0.786514  [ 1952/ 3200]\n",
            "loss: 1.365849  [ 1968/ 3200]\n",
            "loss: 1.084703  [ 1984/ 3200]\n",
            "loss: 0.837069  [ 2000/ 3200]\n",
            "loss: 1.173775  [ 2016/ 3200]\n",
            "loss: 0.855150  [ 2032/ 3200]\n",
            "loss: 0.984785  [ 2048/ 3200]\n",
            "loss: 0.824597  [ 2064/ 3200]\n",
            "loss: 0.990008  [ 2080/ 3200]\n",
            "loss: 0.863675  [ 2096/ 3200]\n",
            "loss: 0.830072  [ 2112/ 3200]\n",
            "loss: 1.127861  [ 2128/ 3200]\n",
            "loss: 1.100726  [ 2144/ 3200]\n",
            "loss: 1.050551  [ 2160/ 3200]\n",
            "loss: 1.264913  [ 2176/ 3200]\n",
            "loss: 1.166631  [ 2192/ 3200]\n",
            "loss: 1.037404  [ 2208/ 3200]\n",
            "loss: 1.168669  [ 2224/ 3200]\n",
            "loss: 1.089215  [ 2240/ 3200]\n",
            "loss: 1.035932  [ 2256/ 3200]\n",
            "loss: 1.058122  [ 2272/ 3200]\n",
            "loss: 1.047118  [ 2288/ 3200]\n",
            "loss: 0.860863  [ 2304/ 3200]\n",
            "loss: 1.118152  [ 2320/ 3200]\n",
            "loss: 1.031852  [ 2336/ 3200]\n",
            "loss: 0.891471  [ 2352/ 3200]\n",
            "loss: 1.160080  [ 2368/ 3200]\n",
            "loss: 1.194233  [ 2384/ 3200]\n",
            "loss: 0.645257  [ 2400/ 3200]\n",
            "loss: 0.857039  [ 2416/ 3200]\n",
            "loss: 1.171094  [ 2432/ 3200]\n",
            "loss: 0.898411  [ 2448/ 3200]\n",
            "loss: 0.817113  [ 2464/ 3200]\n",
            "loss: 1.116829  [ 2480/ 3200]\n",
            "loss: 0.887413  [ 2496/ 3200]\n",
            "loss: 1.087496  [ 2512/ 3200]\n",
            "loss: 1.151466  [ 2528/ 3200]\n",
            "loss: 1.072999  [ 2544/ 3200]\n",
            "loss: 1.061431  [ 2560/ 3200]\n",
            "loss: 1.110681  [ 2576/ 3200]\n",
            "loss: 0.958329  [ 2592/ 3200]\n",
            "loss: 0.886164  [ 2608/ 3200]\n",
            "loss: 0.805720  [ 2624/ 3200]\n",
            "loss: 1.074137  [ 2640/ 3200]\n",
            "loss: 0.877155  [ 2656/ 3200]\n",
            "loss: 0.942711  [ 2672/ 3200]\n",
            "loss: 0.727907  [ 2688/ 3200]\n",
            "loss: 1.087944  [ 2704/ 3200]\n",
            "loss: 0.786876  [ 2720/ 3200]\n",
            "loss: 1.156272  [ 2736/ 3200]\n",
            "loss: 1.011921  [ 2752/ 3200]\n",
            "loss: 0.950005  [ 2768/ 3200]\n",
            "loss: 1.335326  [ 2784/ 3200]\n",
            "loss: 0.786838  [ 2800/ 3200]\n",
            "loss: 1.166083  [ 2816/ 3200]\n",
            "loss: 0.931718  [ 2832/ 3200]\n",
            "loss: 0.894765  [ 2848/ 3200]\n",
            "loss: 1.177158  [ 2864/ 3200]\n",
            "loss: 1.061268  [ 2880/ 3200]\n",
            "loss: 0.788607  [ 2896/ 3200]\n",
            "loss: 0.975000  [ 2912/ 3200]\n",
            "loss: 0.865768  [ 2928/ 3200]\n",
            "loss: 0.961270  [ 2944/ 3200]\n",
            "loss: 0.678423  [ 2960/ 3200]\n",
            "loss: 0.975707  [ 2976/ 3200]\n",
            "loss: 0.912484  [ 2992/ 3200]\n",
            "loss: 0.820192  [ 3008/ 3200]\n",
            "loss: 1.274804  [ 3024/ 3200]\n",
            "loss: 0.676999  [ 3040/ 3200]\n",
            "loss: 1.053279  [ 3056/ 3200]\n",
            "loss: 0.852946  [ 3072/ 3200]\n",
            "loss: 0.988646  [ 3088/ 3200]\n",
            "loss: 0.893559  [ 3104/ 3200]\n",
            "loss: 0.920278  [ 3120/ 3200]\n",
            "loss: 0.801927  [ 3136/ 3200]\n",
            "loss: 0.994063  [ 3152/ 3200]\n",
            "loss: 0.755945  [ 3168/ 3200]\n",
            "loss: 0.889842  [ 3184/ 3200]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.885516  [    0/ 3200]\n",
            "loss: 0.855636  [   16/ 3200]\n",
            "loss: 1.105384  [   32/ 3200]\n",
            "loss: 1.260095  [   48/ 3200]\n",
            "loss: 1.050498  [   64/ 3200]\n",
            "loss: 0.960831  [   80/ 3200]\n",
            "loss: 0.980719  [   96/ 3200]\n",
            "loss: 1.188771  [  112/ 3200]\n",
            "loss: 0.929549  [  128/ 3200]\n",
            "loss: 1.179862  [  144/ 3200]\n",
            "loss: 0.827664  [  160/ 3200]\n",
            "loss: 0.801035  [  176/ 3200]\n",
            "loss: 0.880450  [  192/ 3200]\n",
            "loss: 0.887161  [  208/ 3200]\n",
            "loss: 0.891568  [  224/ 3200]\n",
            "loss: 0.849911  [  240/ 3200]\n",
            "loss: 0.832955  [  256/ 3200]\n",
            "loss: 1.178138  [  272/ 3200]\n",
            "loss: 0.844432  [  288/ 3200]\n",
            "loss: 1.102482  [  304/ 3200]\n",
            "loss: 1.050396  [  320/ 3200]\n",
            "loss: 1.144895  [  336/ 3200]\n",
            "loss: 0.904089  [  352/ 3200]\n",
            "loss: 1.002400  [  368/ 3200]\n",
            "loss: 1.174588  [  384/ 3200]\n",
            "loss: 1.011829  [  400/ 3200]\n",
            "loss: 0.985077  [  416/ 3200]\n",
            "loss: 1.069742  [  432/ 3200]\n",
            "loss: 1.039425  [  448/ 3200]\n",
            "loss: 0.827204  [  464/ 3200]\n",
            "loss: 1.010971  [  480/ 3200]\n",
            "loss: 1.090538  [  496/ 3200]\n",
            "loss: 0.808992  [  512/ 3200]\n",
            "loss: 1.204454  [  528/ 3200]\n",
            "loss: 0.908053  [  544/ 3200]\n",
            "loss: 0.981858  [  560/ 3200]\n",
            "loss: 0.936142  [  576/ 3200]\n",
            "loss: 0.890528  [  592/ 3200]\n",
            "loss: 0.911629  [  608/ 3200]\n",
            "loss: 0.888458  [  624/ 3200]\n",
            "loss: 0.925060  [  640/ 3200]\n",
            "loss: 1.013889  [  656/ 3200]\n",
            "loss: 0.916909  [  672/ 3200]\n",
            "loss: 0.672089  [  688/ 3200]\n",
            "loss: 0.927541  [  704/ 3200]\n",
            "loss: 0.999758  [  720/ 3200]\n",
            "loss: 1.089338  [  736/ 3200]\n",
            "loss: 0.759133  [  752/ 3200]\n",
            "loss: 1.174354  [  768/ 3200]\n",
            "loss: 0.947594  [  784/ 3200]\n",
            "loss: 0.833690  [  800/ 3200]\n",
            "loss: 1.025393  [  816/ 3200]\n",
            "loss: 0.830733  [  832/ 3200]\n",
            "loss: 1.124278  [  848/ 3200]\n",
            "loss: 0.741159  [  864/ 3200]\n",
            "loss: 1.041838  [  880/ 3200]\n",
            "loss: 0.970730  [  896/ 3200]\n",
            "loss: 1.026156  [  912/ 3200]\n",
            "loss: 0.702947  [  928/ 3200]\n",
            "loss: 0.980371  [  944/ 3200]\n",
            "loss: 0.861181  [  960/ 3200]\n",
            "loss: 0.816371  [  976/ 3200]\n",
            "loss: 0.878511  [  992/ 3200]\n",
            "loss: 0.865962  [ 1008/ 3200]\n",
            "loss: 0.859824  [ 1024/ 3200]\n",
            "loss: 1.094177  [ 1040/ 3200]\n",
            "loss: 0.932244  [ 1056/ 3200]\n",
            "loss: 1.532846  [ 1072/ 3200]\n",
            "loss: 1.114631  [ 1088/ 3200]\n",
            "loss: 1.239416  [ 1104/ 3200]\n",
            "loss: 0.986088  [ 1120/ 3200]\n",
            "loss: 1.194838  [ 1136/ 3200]\n",
            "loss: 0.806422  [ 1152/ 3200]\n",
            "loss: 1.113855  [ 1168/ 3200]\n",
            "loss: 0.892442  [ 1184/ 3200]\n",
            "loss: 0.942409  [ 1200/ 3200]\n",
            "loss: 0.618935  [ 1216/ 3200]\n",
            "loss: 0.950288  [ 1232/ 3200]\n",
            "loss: 1.007064  [ 1248/ 3200]\n",
            "loss: 1.095644  [ 1264/ 3200]\n",
            "loss: 0.826630  [ 1280/ 3200]\n",
            "loss: 1.104285  [ 1296/ 3200]\n",
            "loss: 0.819674  [ 1312/ 3200]\n",
            "loss: 0.703769  [ 1328/ 3200]\n",
            "loss: 1.007747  [ 1344/ 3200]\n",
            "loss: 0.849903  [ 1360/ 3200]\n",
            "loss: 0.923057  [ 1376/ 3200]\n",
            "loss: 0.712335  [ 1392/ 3200]\n",
            "loss: 0.874844  [ 1408/ 3200]\n",
            "loss: 0.978988  [ 1424/ 3200]\n",
            "loss: 0.922670  [ 1440/ 3200]\n",
            "loss: 1.092535  [ 1456/ 3200]\n",
            "loss: 0.835775  [ 1472/ 3200]\n",
            "loss: 1.028828  [ 1488/ 3200]\n",
            "loss: 0.924476  [ 1504/ 3200]\n",
            "loss: 0.920288  [ 1520/ 3200]\n",
            "loss: 1.140653  [ 1536/ 3200]\n",
            "loss: 1.070991  [ 1552/ 3200]\n",
            "loss: 1.097409  [ 1568/ 3200]\n",
            "loss: 0.949129  [ 1584/ 3200]\n",
            "loss: 1.070670  [ 1600/ 3200]\n",
            "loss: 1.029604  [ 1616/ 3200]\n",
            "loss: 0.743188  [ 1632/ 3200]\n",
            "loss: 0.858689  [ 1648/ 3200]\n",
            "loss: 1.036655  [ 1664/ 3200]\n",
            "loss: 0.878585  [ 1680/ 3200]\n",
            "loss: 1.109304  [ 1696/ 3200]\n",
            "loss: 0.929960  [ 1712/ 3200]\n",
            "loss: 0.786181  [ 1728/ 3200]\n",
            "loss: 0.794959  [ 1744/ 3200]\n",
            "loss: 0.927208  [ 1760/ 3200]\n",
            "loss: 1.155957  [ 1776/ 3200]\n",
            "loss: 0.997411  [ 1792/ 3200]\n",
            "loss: 0.876507  [ 1808/ 3200]\n",
            "loss: 1.177986  [ 1824/ 3200]\n",
            "loss: 0.630299  [ 1840/ 3200]\n",
            "loss: 0.976423  [ 1856/ 3200]\n",
            "loss: 0.967647  [ 1872/ 3200]\n",
            "loss: 1.111074  [ 1888/ 3200]\n",
            "loss: 1.003137  [ 1904/ 3200]\n",
            "loss: 0.917898  [ 1920/ 3200]\n",
            "loss: 1.174823  [ 1936/ 3200]\n",
            "loss: 1.094667  [ 1952/ 3200]\n",
            "loss: 1.126760  [ 1968/ 3200]\n",
            "loss: 0.706379  [ 1984/ 3200]\n",
            "loss: 0.944431  [ 2000/ 3200]\n",
            "loss: 0.855434  [ 2016/ 3200]\n",
            "loss: 1.041057  [ 2032/ 3200]\n",
            "loss: 1.001810  [ 2048/ 3200]\n",
            "loss: 0.953675  [ 2064/ 3200]\n",
            "loss: 1.026652  [ 2080/ 3200]\n",
            "loss: 0.881835  [ 2096/ 3200]\n",
            "loss: 1.361005  [ 2112/ 3200]\n",
            "loss: 1.050975  [ 2128/ 3200]\n",
            "loss: 0.784473  [ 2144/ 3200]\n",
            "loss: 0.847306  [ 2160/ 3200]\n",
            "loss: 0.993752  [ 2176/ 3200]\n",
            "loss: 1.422026  [ 2192/ 3200]\n",
            "loss: 0.914074  [ 2208/ 3200]\n",
            "loss: 0.921773  [ 2224/ 3200]\n",
            "loss: 0.996906  [ 2240/ 3200]\n",
            "loss: 0.932691  [ 2256/ 3200]\n",
            "loss: 0.891142  [ 2272/ 3200]\n",
            "loss: 0.856204  [ 2288/ 3200]\n",
            "loss: 1.057370  [ 2304/ 3200]\n",
            "loss: 0.809663  [ 2320/ 3200]\n",
            "loss: 1.143659  [ 2336/ 3200]\n",
            "loss: 1.013364  [ 2352/ 3200]\n",
            "loss: 1.025845  [ 2368/ 3200]\n",
            "loss: 1.331905  [ 2384/ 3200]\n",
            "loss: 0.860823  [ 2400/ 3200]\n",
            "loss: 1.032614  [ 2416/ 3200]\n",
            "loss: 0.897003  [ 2432/ 3200]\n",
            "loss: 0.857391  [ 2448/ 3200]\n",
            "loss: 1.194818  [ 2464/ 3200]\n",
            "loss: 0.992373  [ 2480/ 3200]\n",
            "loss: 0.915044  [ 2496/ 3200]\n",
            "loss: 0.864290  [ 2512/ 3200]\n",
            "loss: 0.923235  [ 2528/ 3200]\n",
            "loss: 0.947155  [ 2544/ 3200]\n",
            "loss: 0.833798  [ 2560/ 3200]\n",
            "loss: 1.178801  [ 2576/ 3200]\n",
            "loss: 0.877258  [ 2592/ 3200]\n",
            "loss: 0.976557  [ 2608/ 3200]\n",
            "loss: 0.698800  [ 2624/ 3200]\n",
            "loss: 0.657575  [ 2640/ 3200]\n",
            "loss: 1.013265  [ 2656/ 3200]\n",
            "loss: 0.935664  [ 2672/ 3200]\n",
            "loss: 1.375486  [ 2688/ 3200]\n",
            "loss: 0.909160  [ 2704/ 3200]\n",
            "loss: 1.033909  [ 2720/ 3200]\n",
            "loss: 0.856767  [ 2736/ 3200]\n",
            "loss: 0.675869  [ 2752/ 3200]\n",
            "loss: 1.159556  [ 2768/ 3200]\n",
            "loss: 0.899923  [ 2784/ 3200]\n",
            "loss: 1.018582  [ 2800/ 3200]\n",
            "loss: 1.362087  [ 2816/ 3200]\n",
            "loss: 0.929257  [ 2832/ 3200]\n",
            "loss: 1.024709  [ 2848/ 3200]\n",
            "loss: 0.950539  [ 2864/ 3200]\n",
            "loss: 1.125041  [ 2880/ 3200]\n",
            "loss: 0.797137  [ 2896/ 3200]\n",
            "loss: 1.004845  [ 2912/ 3200]\n",
            "loss: 1.294729  [ 2928/ 3200]\n",
            "loss: 1.087436  [ 2944/ 3200]\n",
            "loss: 0.828759  [ 2960/ 3200]\n",
            "loss: 0.709318  [ 2976/ 3200]\n",
            "loss: 0.817954  [ 2992/ 3200]\n",
            "loss: 1.227399  [ 3008/ 3200]\n",
            "loss: 1.020782  [ 3024/ 3200]\n",
            "loss: 0.906962  [ 3040/ 3200]\n",
            "loss: 0.881468  [ 3056/ 3200]\n",
            "loss: 0.880898  [ 3072/ 3200]\n",
            "loss: 0.861179  [ 3088/ 3200]\n",
            "loss: 1.076656  [ 3104/ 3200]\n",
            "loss: 0.801949  [ 3120/ 3200]\n",
            "loss: 0.908159  [ 3136/ 3200]\n",
            "loss: 0.996189  [ 3152/ 3200]\n",
            "loss: 0.957031  [ 3168/ 3200]\n",
            "loss: 1.095247  [ 3184/ 3200]\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=26, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model1 = train_nn(num_epochs, optimizer, train_dataloader, loss_fn, model, learning_rate)\n",
        "print(model1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjbtDiStQWYI",
        "outputId": "dbf40217-2788-460b-ff18-2cb19d136a34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.0007137712091207504,\n",
              " 0.5377352237701416,\n",
              " 0.5850290697674418,\n",
              " tensor([[ 24.,  24.,  87., 189.],\n",
              "         [ 16., 218.,  15.,  48.],\n",
              "         [ 24.,  14., 231.,  87.],\n",
              "         [ 14.,  17.,  36., 332.]]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_nn(test_dataloader, model1, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oQ7YDcQmQWYJ"
      },
      "outputs": [],
      "source": [
        "def choose_model(num_epochs, optimizer, train, loss_fn, model, learning_rate, val):\n",
        "    # copy by value the model\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    optimizer = optimizer(model_copy.parameters(), lr=learning_rate)\n",
        "    f1_max = 0\n",
        "    for e in range(num_epochs):\n",
        "        model_copy.train()\n",
        "        for (data, label) in train:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            target = model_copy(data)\n",
        "            # loss_fn defined above to be  nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(target, label)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step() # updating the weights of neural network\n",
        "\n",
        "        train_f1 = f1_score(target, label, num_classes=4, average='macro')\n",
        "\n",
        "        model_copy.eval()\n",
        "        for (X, y) in val:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            pred = model_copy(X)\n",
        "\n",
        "        val_f1 = f1_score(pred, y, num_classes=4, average='macro')\n",
        "\n",
        "        \n",
        "        print(f'Epoch {e+1} \\t\\t Training f1: {train_f1} \\t\\t Validation f1: {val_f1}')\n",
        "\n",
        "\n",
        "        if f1_max < val_f1:\n",
        "            print(f'Validation f1 Increased({f1_max:.6f}--->{val_f1:.6f}) \\t Saving The Model')\n",
        "            f1_max = val_f1\n",
        "            # Saving State Dict\n",
        "            torch.save(model_copy.state_dict(), 'optimal.pth')\n",
        "    torch.save(model_copy.state_dict(), 'last.pth')\n",
        "    print('-----------------------------------------------------')\n",
        "    print(f'f1 optimal: {f1_max:.6f} \\nf1 train at end of epoch: {train_f1:.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxKMbBaGQWYK",
        "outputId": "193896b5-b4c8-4f50-c262-782962b072de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training f1: 0.22692307829856873 \t\t Validation f1: 0.2544112801551819\n",
            "Validation f1 Increased(0.000000--->0.254411) \t Saving The Model\n",
            "Epoch 2 \t\t Training f1: 0.13636364042758942 \t\t Validation f1: 0.10258766263723373\n",
            "Epoch 3 \t\t Training f1: 0.1964285671710968 \t\t Validation f1: 0.45308488607406616\n",
            "Validation f1 Increased(0.254411--->0.453085) \t Saving The Model\n",
            "Epoch 4 \t\t Training f1: 0.36666667461395264 \t\t Validation f1: 0.17432507872581482\n",
            "Epoch 5 \t\t Training f1: 0.28131866455078125 \t\t Validation f1: 0.48506635427474976\n",
            "Validation f1 Increased(0.453085--->0.485066) \t Saving The Model\n",
            "Epoch 6 \t\t Training f1: 0.14215686917304993 \t\t Validation f1: 0.3554023802280426\n",
            "Epoch 7 \t\t Training f1: 0.38611114025115967 \t\t Validation f1: 0.4987509250640869\n",
            "Validation f1 Increased(0.485066--->0.498751) \t Saving The Model\n",
            "Epoch 8 \t\t Training f1: 0.3743131756782532 \t\t Validation f1: 0.5737300515174866\n",
            "Validation f1 Increased(0.498751--->0.573730) \t Saving The Model\n",
            "Epoch 9 \t\t Training f1: 0.3680555522441864 \t\t Validation f1: 0.28610438108444214\n",
            "Epoch 10 \t\t Training f1: 0.398717999458313 \t\t Validation f1: 0.5336906909942627\n",
            "Epoch 11 \t\t Training f1: 0.4452381134033203 \t\t Validation f1: 0.6105533838272095\n",
            "Validation f1 Increased(0.573730--->0.610553) \t Saving The Model\n",
            "Epoch 12 \t\t Training f1: 0.550000011920929 \t\t Validation f1: 0.33476245403289795\n",
            "Epoch 13 \t\t Training f1: 0.2716450095176697 \t\t Validation f1: 0.5164415836334229\n",
            "Epoch 14 \t\t Training f1: 0.4913420081138611 \t\t Validation f1: 0.4874039888381958\n",
            "Epoch 15 \t\t Training f1: 0.5892857313156128 \t\t Validation f1: 0.531872570514679\n",
            "Epoch 16 \t\t Training f1: 0.7559524178504944 \t\t Validation f1: 0.6177252531051636\n",
            "Validation f1 Increased(0.610553--->0.617725) \t Saving The Model\n",
            "Epoch 17 \t\t Training f1: 0.5647727251052856 \t\t Validation f1: 0.6236867904663086\n",
            "Validation f1 Increased(0.617725--->0.623687) \t Saving The Model\n",
            "Epoch 18 \t\t Training f1: 0.41874998807907104 \t\t Validation f1: 0.5154629349708557\n",
            "Epoch 19 \t\t Training f1: 0.4583333730697632 \t\t Validation f1: 0.5048565864562988\n",
            "Epoch 20 \t\t Training f1: 0.5309523940086365 \t\t Validation f1: 0.6048152446746826\n",
            "Epoch 21 \t\t Training f1: 0.6124999523162842 \t\t Validation f1: 0.5876995325088501\n",
            "Epoch 22 \t\t Training f1: 0.5540404319763184 \t\t Validation f1: 0.5608263611793518\n",
            "Epoch 23 \t\t Training f1: 0.548214316368103 \t\t Validation f1: 0.6431719064712524\n",
            "Validation f1 Increased(0.623687--->0.643172) \t Saving The Model\n",
            "Epoch 24 \t\t Training f1: 0.5476190447807312 \t\t Validation f1: 0.5749797821044922\n",
            "Epoch 25 \t\t Training f1: 0.33392855525016785 \t\t Validation f1: 0.6217716932296753\n",
            "Epoch 26 \t\t Training f1: 0.5297619104385376 \t\t Validation f1: 0.6287344694137573\n",
            "Epoch 27 \t\t Training f1: 0.7423076629638672 \t\t Validation f1: 0.5777765512466431\n",
            "Epoch 28 \t\t Training f1: 0.3142856955528259 \t\t Validation f1: 0.6534711122512817\n",
            "Validation f1 Increased(0.643172--->0.653471) \t Saving The Model\n",
            "Epoch 29 \t\t Training f1: 0.5690476298332214 \t\t Validation f1: 0.6474287509918213\n",
            "Epoch 30 \t\t Training f1: 0.7689394354820251 \t\t Validation f1: 0.6125658750534058\n",
            "-----------------------------------------------------\n",
            "f1 optimal: 0.653471 \n",
            "f1 train at end of epoch: 0.768939\n"
          ]
        }
      ],
      "source": [
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, model, learning_rate, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTnlGcKDQWYL",
        "outputId": "f31d69a7-cf52-45d4-d09c-45549fd43d38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "last_model = NeuralNetwork().to(device)\n",
        "last_model.load_state_dict(torch.load('last.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlyqRu0YQWYM",
        "outputId": "a24bad54-4db7-4846-8ce9-c6ee0fbbceb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimal_model = NeuralNetwork().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9SBmz5YQWYM",
        "outputId": "7d830d5a-59db-4681-e09b-d81d6b158a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.0007050397351037624, 0.5754448175430298, 0.6184593023255814, tensor([[ 41.,  27., 127., 129.],\n",
            "        [ 20., 226.,  22.,  29.],\n",
            "        [ 18.,  16., 286.,  36.],\n",
            "        [ 21.,  28.,  52., 298.]]))\n",
            "(0.0007182826656241749, 0.6009374856948853, 0.6206395348837209, tensor([[ 84.,  45.,  89., 106.],\n",
            "        [ 26., 249.,  11.,  11.],\n",
            "        [ 47.,  27., 254.,  28.],\n",
            "        [ 44.,  47.,  41., 267.]]))\n"
          ]
        }
      ],
      "source": [
        "print(test_nn(test_dataloader, last_model, loss_fn))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vRkdPJ9QWYN",
        "outputId": "5dcd7b95-7f0c-4c47-ad38-600bff3b54c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 21, 128])\n"
          ]
        }
      ],
      "source": [
        "rep = 'melgrams'\n",
        "\n",
        "training_data = CustomDataset(npy_loader(get_path('train', rep, 0)), npy_loader(get_path('train', rep, 1)))\n",
        "val_data = CustomDataset(npy_loader(get_path('val', rep, 0)), npy_loader(get_path('val', rep, 1)))\n",
        "test_data = CustomDataset(npy_loader(get_path('test', rep, 0)), npy_loader(get_path('test', rep, 1)))\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=800, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1376, shuffle=False)\n",
        "\n",
        "print(training_data[0][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xLo3I5AIQWYO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def get_random_id_from_class(i):\n",
        "\twhile True:\n",
        "\t\tidx = random.randint(0, len(training_data)-1)\n",
        "\t\tif training_data[idx][1] == i:\n",
        "\t\t\treturn idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0VKTRSOMQWYO",
        "outputId": "49c05a6c-78f6-40a0-deb4-47359515513b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x504 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize a random melgram from each class\n",
        "plt.figure(figsize=(15,7))\n",
        "for i in range(4):\n",
        "\tplt.subplot(4, 1, i+1)\n",
        "\timage = training_data[get_random_id_from_class(i)][0].numpy()\n",
        "\timage = np.squeeze(image, axis=0)\n",
        "\tplt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "45mECMv5QWYP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "harT25NYQWYP"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "    self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc1 = nn.Linear(1024, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 256)\n",
        "    self.fc3 = nn.Linear(256, 32)\n",
        "    self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.max_pool(F.relu(self.conv1(x)))\n",
        "    x = self.max_pool(F.relu(self.conv2(x)))\n",
        "    x = self.max_pool(F.relu(self.conv3(x)))\n",
        "    x = self.max_pool(F.relu(self.conv4(x)))\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x\n",
        "\n",
        "net = LeNet().to(device)\n",
        "\n",
        "# define the corresponding loss function and the optimizer\n",
        "learning_rate = 1e-3\n",
        "loss_fn_1 = nn.CrossEntropyLoss()\n",
        "optimizer_1 = optim.Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aRvaTVX0QWYQ"
      },
      "outputs": [],
      "source": [
        "def choose_model(num_epochs, optimizer, train, loss_fn, model, learning_rate, val, reproducibility=False, show=True, lbfgs=False):\n",
        "    # set seed before create your model  \n",
        "    if reproducibility:\n",
        "        torch_seed(seed=0)\n",
        "    \n",
        "    # copy by value the model\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    optimizer = optimizer(model_copy.parameters(), lr=learning_rate)\n",
        "    f1_max = 0\n",
        "    for e in range(num_epochs):\n",
        "        model_copy.train()\n",
        "        for (data, label) in train:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                target = model_copy(data)\n",
        "                loss = loss_fn(target, label)\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            if lbfgs:\n",
        "                optimizer.step(closure)\n",
        "                target = model_copy(data)\n",
        "            else:\n",
        "                # Compute prediction and loss\n",
        "                target = model_copy(data)\n",
        "                # loss_fn defined above to be  nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(target, label)\n",
        "\n",
        "                # Backpropagation\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step() # updating the weights of neural network\n",
        "\n",
        "        train_f1 = f1_score(target, label, num_classes=4, average='macro')\n",
        "\n",
        "        model_copy.eval()\n",
        "        for (X, y) in val:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            pred = model_copy(X)\n",
        "\n",
        "        val_f1 = f1_score(pred, y, num_classes=4, average='macro')\n",
        "\n",
        "        if show:\n",
        "            print(f'Epoch {e+1} \\t\\t Training f1: {train_f1} \\t\\t Validation f1: {val_f1}', end='')\n",
        "            if (reproducibility):\n",
        "                print(f'\\t\\t Loss: {loss}')\n",
        "            else:\n",
        "                print()\n",
        "\n",
        "\n",
        "        if f1_max < val_f1:\n",
        "            if show:\n",
        "                print(f'Validation f1 Increased({f1_max:.6f}--->{val_f1:.6f}) \\t Saving The Model')\n",
        "            f1_max = val_f1\n",
        "            # Saving State Dict\n",
        "            torch.save(model_copy.state_dict(), 'optimal.pth')\n",
        "    torch.save(model_copy.state_dict(), 'last.pth')\n",
        "    if show:\n",
        "        print('-----------------------------------------------------')\n",
        "        print(f'f1 optimal: {f1_max:.6f} \\nf1 train at end of epoch: {train_f1:.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "p6SFUsCEQWYQ"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.functional import f1_score\n",
        "\n",
        "def test_nn(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    # initialize the loss function\n",
        "    loss = 0\n",
        "    # initialize the number of correct predictions\n",
        "    correct = 0\n",
        "    # initialize the confusion matrix\n",
        "    confusion_matrix = torch.zeros(4, 4)\n",
        "    # initialize the f1 score\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            pred = model(X)\n",
        "            loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            _, preds = torch.max(pred, 1)\n",
        "            for t, p in zip(y.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "    loss /= size\n",
        "    correct /= size\n",
        "    # calculate f1 macro averaged\n",
        "    f1 = f1_score(pred, y, num_classes=4, average='macro')\n",
        "    # calculate confusion matrix\n",
        "    return loss, f1.item(), correct, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efPqaaEAQWYR",
        "outputId": "394f063a-eb38-435e-c428-b8693ec00aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training f1: 0.3639705777168274 \t\t Validation f1: 0.4182340204715729\n",
            "Validation f1 Increased(0.000000--->0.418234) \t Saving The Model\n",
            "Epoch 2 \t\t Training f1: 0.3102564215660095 \t\t Validation f1: 0.4746589958667755\n",
            "Validation f1 Increased(0.418234--->0.474659) \t Saving The Model\n",
            "Epoch 3 \t\t Training f1: 0.740079402923584 \t\t Validation f1: 0.5961748361587524\n",
            "Validation f1 Increased(0.474659--->0.596175) \t Saving The Model\n",
            "Epoch 4 \t\t Training f1: 0.3333333432674408 \t\t Validation f1: 0.45436084270477295\n",
            "Epoch 5 \t\t Training f1: 0.5198413133621216 \t\t Validation f1: 0.6479859352111816\n",
            "Validation f1 Increased(0.596175--->0.647986) \t Saving The Model\n",
            "Epoch 6 \t\t Training f1: 0.7984848618507385 \t\t Validation f1: 0.7224270105361938\n",
            "Validation f1 Increased(0.647986--->0.722427) \t Saving The Model\n",
            "Epoch 7 \t\t Training f1: 0.8687499761581421 \t\t Validation f1: 0.7267510294914246\n",
            "Validation f1 Increased(0.722427--->0.726751) \t Saving The Model\n",
            "Epoch 8 \t\t Training f1: 0.8309524059295654 \t\t Validation f1: 0.6963160037994385\n",
            "Epoch 9 \t\t Training f1: 0.7142857313156128 \t\t Validation f1: 0.7090632319450378\n",
            "Epoch 10 \t\t Training f1: 0.9580419659614563 \t\t Validation f1: 0.7058069109916687\n",
            "Epoch 11 \t\t Training f1: 0.9272727370262146 \t\t Validation f1: 0.6846367120742798\n",
            "Epoch 12 \t\t Training f1: 1.0 \t\t Validation f1: 0.692311704158783\n",
            "Epoch 13 \t\t Training f1: 0.9222222566604614 \t\t Validation f1: 0.6815844178199768\n",
            "Epoch 14 \t\t Training f1: 1.0 \t\t Validation f1: 0.6896291375160217\n",
            "Epoch 15 \t\t Training f1: 1.0 \t\t Validation f1: 0.6422111988067627\n",
            "Epoch 16 \t\t Training f1: 0.9272727370262146 \t\t Validation f1: 0.6767181158065796\n",
            "Epoch 17 \t\t Training f1: 1.0 \t\t Validation f1: 0.6875023245811462\n",
            "Epoch 18 \t\t Training f1: 0.9000000357627869 \t\t Validation f1: 0.6759796142578125\n",
            "Epoch 19 \t\t Training f1: 1.0 \t\t Validation f1: 0.6943798661231995\n",
            "Epoch 20 \t\t Training f1: 1.0 \t\t Validation f1: 0.6929521560668945\n",
            "Epoch 21 \t\t Training f1: 1.0 \t\t Validation f1: 0.6710218191146851\n",
            "Epoch 22 \t\t Training f1: 0.7333333492279053 \t\t Validation f1: 0.6701590418815613\n",
            "Epoch 23 \t\t Training f1: 1.0 \t\t Validation f1: 0.6951526403427124\n",
            "Epoch 24 \t\t Training f1: 0.8809523582458496 \t\t Validation f1: 0.7024720907211304\n",
            "Epoch 25 \t\t Training f1: 0.8790584802627563 \t\t Validation f1: 0.6936035752296448\n",
            "Epoch 26 \t\t Training f1: 1.0 \t\t Validation f1: 0.6882504820823669\n",
            "Epoch 27 \t\t Training f1: 0.8666666746139526 \t\t Validation f1: 0.6773531436920166\n",
            "Epoch 28 \t\t Training f1: 0.9222222566604614 \t\t Validation f1: 0.6835064888000488\n",
            "Epoch 29 \t\t Training f1: 1.0 \t\t Validation f1: 0.6778151988983154\n",
            "Epoch 30 \t\t Training f1: 1.0 \t\t Validation f1: 0.7016816139221191\n",
            "-----------------------------------------------------\n",
            "f1 optimal: 0.726751 \n",
            "f1 train at end of epoch: 1.000000\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "choose_model(num_epochs, optimizer_1, train_dataloader, loss_fn_1, net, learning_rate, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvjwlByCQWYR",
        "outputId": "15f1bf28-6804-49d0-c158-28e0e621f196"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimal_model = LeNet().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L2KNpmdQWYR",
        "outputId": "5a69227b-a8e3-48ca-d640-0a90839aad71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.0005381266738093177, 0.7114886045455933, 0.7093023255813954, tensor([[203.,  42.,  14.,  65.],\n",
            "        [  6., 287.,   1.,   3.],\n",
            "        [ 26.,   9., 279.,  42.],\n",
            "        [121.,  30.,  41., 207.]]))\n"
          ]
        }
      ],
      "source": [
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcW-iU-GQWYS"
      },
      "outputs": [],
      "source": [
        "Algorithms = ['Adadelta', 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'ASGD', 'LBFGS', 'NAdam', 'RAdam', 'RMSprop', 'Rprop', 'SGD']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvN8WtRMQWYS",
        "outputId": "bf4083c0-9119-40b1-90ae-89248d6e76a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╒══════════════╤═════════════════════╤════════════════════╤════════════════════╤════════════════════╤════════════════════╤════════════════════╤═════════════════════╤════════════════════╤════════════════════╤════════════════════╤════════════════════╤════════════════════╕\n",
            "│ Algorithms   │ Adadelta            │ Adagrad            │ Adam               │ AdamW              │ Adamax             │ ASGD               │ LBFGS               │ NAdam              │ RAdam              │ RMSprop            │ Rprop              │ SGD                │\n",
            "├──────────────┼─────────────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┼─────────────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┤\n",
            "│ Accuracy (%) │ 49.41860465116279   │ 74.63662790697676  │ 69.18604651162791  │ 71.51162790697676  │ 74.70930232558139  │ 69.11337209302324  │ 22.819767441860463  │ 67.07848837209302  │ 71.00290697674419  │ 68.53197674418605  │ 68.1686046511628   │ 61.48255813953488  │\n",
            "├──────────────┼─────────────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┼─────────────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┼────────────────────┤\n",
            "│ f1           │ 0.49544715881347656 │ 0.7561036348342896 │ 0.6979794502258301 │ 0.7147524356842041 │ 0.7507884502410889 │ 0.6831039786338806 │ 0.09793341904878616 │ 0.6661769151687622 │ 0.7215027809143066 │ 0.6930823922157288 │ 0.6818387508392334 │ 0.6162289381027222 │\n",
            "╘══════════════╧═════════════════════╧════════════════════╧════════════════════╧════════════════════╧════════════════════╧════════════════════╧═════════════════════╧════════════════════╧════════════════════╧════════════════════╧════════════════════╧════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "temp = [['Algorithms', 'Accuracy (%)', 'f1']]\n",
        "for a in Algorithms:\n",
        "\tnet = LeNet().to(device)\n",
        "\toptimizer = getattr(optim, a)\n",
        "\tif a == 'LBFGS':\n",
        "\t\tchoose_model(num_epochs, optimizer, train_dataloader, loss_fn_1, net, learning_rate, val_dataloader, show=False, lbfgs=True)\n",
        "\telse:\n",
        "\t\tchoose_model(num_epochs, optimizer, train_dataloader, loss_fn_1, net, learning_rate, val_dataloader, show=False)\n",
        "\toptimal_model = LeNet().to(device)\n",
        "\toptimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "\t_, f1, accuracy, _ = test_nn(test_dataloader, optimal_model, loss_fn_1)\n",
        "\ttemp.append([a, accuracy*100, f1])\n",
        "\n",
        "rotated = [[None for j in range(len(temp))] for i in range(3)]\n",
        "for i, row in enumerate(temp):\n",
        "    for j, value in enumerate(row):\n",
        "        rotated[j][i] = value\n",
        "\n",
        "print(tabulate(rotated, tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "G7lX-8zOpy8K"
      },
      "outputs": [],
      "source": [
        "import random, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_GLRkb5Opy8K"
      },
      "outputs": [],
      "source": [
        "def torch_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ccPmP2py8K",
        "outputId": "f8c159ce-a0ab-4a0b-ee8d-2ce9f6f5b31f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training f1: 0.45384615659713745 \t\t Validation f1: 0.4746466279029846\t\t Loss: 0.8585479259490967\n",
            "Validation f1 Increased(0.000000--->0.474647) \t Saving The Model\n",
            "Epoch 2 \t\t Training f1: 0.7142857313156128 \t\t Validation f1: 0.5901783108711243\t\t Loss: 0.647953987121582\n",
            "Validation f1 Increased(0.474647--->0.590178) \t Saving The Model\n",
            "Epoch 3 \t\t Training f1: 0.7003968358039856 \t\t Validation f1: 0.6463372707366943\t\t Loss: 0.6502941846847534\n",
            "Validation f1 Increased(0.590178--->0.646337) \t Saving The Model\n",
            "Epoch 4 \t\t Training f1: 0.8439394235610962 \t\t Validation f1: 0.6773467063903809\t\t Loss: 0.5651968121528625\n",
            "Validation f1 Increased(0.646337--->0.677347) \t Saving The Model\n",
            "Epoch 5 \t\t Training f1: 0.622619092464447 \t\t Validation f1: 0.6677638292312622\t\t Loss: 0.5089439153671265\n",
            "Epoch 6 \t\t Training f1: 0.6788420081138611 \t\t Validation f1: 0.665547251701355\t\t Loss: 0.8748731017112732\n",
            "Epoch 7 \t\t Training f1: 0.7756410241127014 \t\t Validation f1: 0.6426476240158081\t\t Loss: 0.5718334317207336\n",
            "Epoch 8 \t\t Training f1: 0.6866071224212646 \t\t Validation f1: 0.7019595503807068\t\t Loss: 0.5810132026672363\n",
            "Validation f1 Increased(0.677347--->0.701960) \t Saving The Model\n",
            "Epoch 9 \t\t Training f1: 0.8865079283714294 \t\t Validation f1: 0.730506956577301\t\t Loss: 0.4979093372821808\n",
            "Validation f1 Increased(0.701960--->0.730507) \t Saving The Model\n",
            "Epoch 10 \t\t Training f1: 0.9285714626312256 \t\t Validation f1: 0.7416549921035767\t\t Loss: 0.17836885154247284\n",
            "Validation f1 Increased(0.730507--->0.741655) \t Saving The Model\n",
            "Epoch 11 \t\t Training f1: 0.7413420081138611 \t\t Validation f1: 0.7361158132553101\t\t Loss: 0.4667530357837677\n",
            "Epoch 12 \t\t Training f1: 1.0 \t\t Validation f1: 0.7228450179100037\t\t Loss: 0.10481881350278854\n",
            "Epoch 13 \t\t Training f1: 0.8809524178504944 \t\t Validation f1: 0.7062368392944336\t\t Loss: 0.22178594768047333\n",
            "Epoch 14 \t\t Training f1: 1.0 \t\t Validation f1: 0.7124139070510864\t\t Loss: 0.06135961040854454\n",
            "Epoch 15 \t\t Training f1: 0.9476190805435181 \t\t Validation f1: 0.7191836833953857\t\t Loss: 0.41968223452568054\n",
            "Epoch 16 \t\t Training f1: 0.8888888955116272 \t\t Validation f1: 0.7033629417419434\t\t Loss: 0.11184179037809372\n",
            "Epoch 17 \t\t Training f1: 1.0 \t\t Validation f1: 0.681174635887146\t\t Loss: 0.04429519549012184\n",
            "Epoch 18 \t\t Training f1: 0.9019607901573181 \t\t Validation f1: 0.6869175434112549\t\t Loss: 0.19379839301109314\n",
            "Epoch 19 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.6986160278320312\t\t Loss: 0.07612384110689163\n",
            "Epoch 20 \t\t Training f1: 1.0 \t\t Validation f1: 0.7111790180206299\t\t Loss: 0.10474548488855362\n",
            "Epoch 21 \t\t Training f1: 1.0 \t\t Validation f1: 0.7137147784233093\t\t Loss: 0.009130913764238358\n",
            "Epoch 22 \t\t Training f1: 1.0 \t\t Validation f1: 0.7232005000114441\t\t Loss: 0.0033366966526955366\n",
            "Epoch 23 \t\t Training f1: 1.0 \t\t Validation f1: 0.7051079869270325\t\t Loss: 0.005466945469379425\n",
            "Epoch 24 \t\t Training f1: 1.0 \t\t Validation f1: 0.7004059553146362\t\t Loss: 0.024385161697864532\n",
            "Epoch 25 \t\t Training f1: 0.9450549483299255 \t\t Validation f1: 0.673587441444397\t\t Loss: 0.31174570322036743\n",
            "Epoch 26 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.7315292358398438\t\t Loss: 0.19256792962551117\n",
            "Epoch 27 \t\t Training f1: 0.9415584802627563 \t\t Validation f1: 0.7147899866104126\t\t Loss: 0.20268918573856354\n",
            "Epoch 28 \t\t Training f1: 1.0 \t\t Validation f1: 0.6946648359298706\t\t Loss: 0.044194404035806656\n",
            "Epoch 29 \t\t Training f1: 1.0 \t\t Validation f1: 0.7002233862876892\t\t Loss: 0.002164312871173024\n",
            "Epoch 30 \t\t Training f1: 1.0 \t\t Validation f1: 0.712714433670044\t\t Loss: 0.0036278082989156246\n",
            "-----------------------------------------------------\n",
            "f1 optimal: 0.741655 \n",
            "f1 train at end of epoch: 1.000000\n",
            "(0.0006363468547893124, 0.72129225730896, 0.717296511627907, tensor([[201.,  36.,  22.,  65.],\n",
            "        [ 13., 275.,   1.,   8.],\n",
            "        [ 41.,   7., 274.,  34.],\n",
            "        [102.,  20.,  40., 237.]]))\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "optimizer_1 = optim.Adam\n",
        "choose_model(num_epochs, optimizer_1, train_dataloader, loss_fn_1, net, learning_rate, val_dataloader, reproducibility=True)\n",
        "optimal_model = LeNet().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0yxVPfNpy8K",
        "outputId": "45b41f61-f660-41ab-dab4-ab6b546ddb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training f1: 0.45384615659713745 \t\t Validation f1: 0.4746466279029846\t\t Loss: 0.8585479259490967\n",
            "Validation f1 Increased(0.000000--->0.474647) \t Saving The Model\n",
            "Epoch 2 \t\t Training f1: 0.7142857313156128 \t\t Validation f1: 0.5901783108711243\t\t Loss: 0.647953987121582\n",
            "Validation f1 Increased(0.474647--->0.590178) \t Saving The Model\n",
            "Epoch 3 \t\t Training f1: 0.7003968358039856 \t\t Validation f1: 0.6463372707366943\t\t Loss: 0.6502941846847534\n",
            "Validation f1 Increased(0.590178--->0.646337) \t Saving The Model\n",
            "Epoch 4 \t\t Training f1: 0.8439394235610962 \t\t Validation f1: 0.6773467063903809\t\t Loss: 0.5651968121528625\n",
            "Validation f1 Increased(0.646337--->0.677347) \t Saving The Model\n",
            "Epoch 5 \t\t Training f1: 0.622619092464447 \t\t Validation f1: 0.6677638292312622\t\t Loss: 0.5089439153671265\n",
            "Epoch 6 \t\t Training f1: 0.6788420081138611 \t\t Validation f1: 0.665547251701355\t\t Loss: 0.8748731017112732\n",
            "Epoch 7 \t\t Training f1: 0.7756410241127014 \t\t Validation f1: 0.6426476240158081\t\t Loss: 0.5718334317207336\n",
            "Epoch 8 \t\t Training f1: 0.6866071224212646 \t\t Validation f1: 0.7019595503807068\t\t Loss: 0.5810132026672363\n",
            "Validation f1 Increased(0.677347--->0.701960) \t Saving The Model\n",
            "Epoch 9 \t\t Training f1: 0.8865079283714294 \t\t Validation f1: 0.730506956577301\t\t Loss: 0.4979093372821808\n",
            "Validation f1 Increased(0.701960--->0.730507) \t Saving The Model\n",
            "Epoch 10 \t\t Training f1: 0.9285714626312256 \t\t Validation f1: 0.7416549921035767\t\t Loss: 0.17836885154247284\n",
            "Validation f1 Increased(0.730507--->0.741655) \t Saving The Model\n",
            "Epoch 11 \t\t Training f1: 0.7413420081138611 \t\t Validation f1: 0.7361158132553101\t\t Loss: 0.4667530357837677\n",
            "Epoch 12 \t\t Training f1: 1.0 \t\t Validation f1: 0.7228450179100037\t\t Loss: 0.10481881350278854\n",
            "Epoch 13 \t\t Training f1: 0.8809524178504944 \t\t Validation f1: 0.7062368392944336\t\t Loss: 0.22178594768047333\n",
            "Epoch 14 \t\t Training f1: 1.0 \t\t Validation f1: 0.7124139070510864\t\t Loss: 0.06135961040854454\n",
            "Epoch 15 \t\t Training f1: 0.9476190805435181 \t\t Validation f1: 0.7191836833953857\t\t Loss: 0.41968223452568054\n",
            "Epoch 16 \t\t Training f1: 0.8888888955116272 \t\t Validation f1: 0.7033629417419434\t\t Loss: 0.11184179037809372\n",
            "Epoch 17 \t\t Training f1: 1.0 \t\t Validation f1: 0.681174635887146\t\t Loss: 0.04429519549012184\n",
            "Epoch 18 \t\t Training f1: 0.9019607901573181 \t\t Validation f1: 0.6869175434112549\t\t Loss: 0.19379839301109314\n",
            "Epoch 19 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.6986160278320312\t\t Loss: 0.07612384110689163\n",
            "Epoch 20 \t\t Training f1: 1.0 \t\t Validation f1: 0.7111790180206299\t\t Loss: 0.10474548488855362\n",
            "Epoch 21 \t\t Training f1: 1.0 \t\t Validation f1: 0.7137147784233093\t\t Loss: 0.009130913764238358\n",
            "Epoch 22 \t\t Training f1: 1.0 \t\t Validation f1: 0.7232005000114441\t\t Loss: 0.0033366966526955366\n",
            "Epoch 23 \t\t Training f1: 1.0 \t\t Validation f1: 0.7051079869270325\t\t Loss: 0.005466945469379425\n",
            "Epoch 24 \t\t Training f1: 1.0 \t\t Validation f1: 0.7004059553146362\t\t Loss: 0.024385161697864532\n",
            "Epoch 25 \t\t Training f1: 0.9450549483299255 \t\t Validation f1: 0.673587441444397\t\t Loss: 0.31174570322036743\n",
            "Epoch 26 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.7315292358398438\t\t Loss: 0.19256792962551117\n",
            "Epoch 27 \t\t Training f1: 0.9415584802627563 \t\t Validation f1: 0.7147899866104126\t\t Loss: 0.20268918573856354\n",
            "Epoch 28 \t\t Training f1: 1.0 \t\t Validation f1: 0.6946648359298706\t\t Loss: 0.044194404035806656\n",
            "Epoch 29 \t\t Training f1: 1.0 \t\t Validation f1: 0.7002233862876892\t\t Loss: 0.002164312871173024\n",
            "Epoch 30 \t\t Training f1: 1.0 \t\t Validation f1: 0.712714433670044\t\t Loss: 0.0036278082989156246\n",
            "-----------------------------------------------------\n",
            "f1 optimal: 0.741655 \n",
            "f1 train at end of epoch: 1.000000\n",
            "(0.0006363468547893124, 0.72129225730896, 0.717296511627907, tensor([[201.,  36.,  22.,  65.],\n",
            "        [ 13., 275.,   1.,   8.],\n",
            "        [ 41.,   7., 274.,  34.],\n",
            "        [102.,  20.,  40., 237.]]))\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "optimizer_1 = optim.Adam\n",
        "choose_model(num_epochs, optimizer_1, train_dataloader, loss_fn_1, net, learning_rate, val_dataloader, reproducibility=True)\n",
        "optimal_model = LeNet().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE6FWxGeV8As"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self, fun):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "    self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc1 = nn.Linear(1024, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 256)\n",
        "    self.fc3 = nn.Linear(256, 32)\n",
        "    self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    self.fun = fun\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.fun.__name__.lower() == 'leakyrelu':\n",
        "      fun = getattr(F, 'leaky_relu')\n",
        "    elif self.fun.__name__.lower() == 'sigmoid':\n",
        "      fun = getattr(torch, 'sigmoid')\n",
        "    elif self.fun.__name__.lower() == 'tanh':\n",
        "      fun = getattr(torch, 'tanh')\n",
        "    else:\n",
        "      fun = getattr(F, self.fun.__name__.lower())\n",
        "    x = self.max_pool(fun(self.conv1(x)))\n",
        "    x = self.max_pool(fun(self.conv2(x)))\n",
        "    x = self.max_pool(fun(self.conv3(x)))\n",
        "    x = self.max_pool(fun(self.conv4(x)))\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    x = fun(self.fc1(x))\n",
        "    x = fun(self.fc2(x))\n",
        "    x = fun(self.fc3(x))\n",
        "    x = fun(self.fc4(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFQmZoI0V8As"
      },
      "outputs": [],
      "source": [
        "NLAF = ['ELU', 'Hardshrink', 'Hardsigmoid', 'Hardtanh', 'Hardswish', 'LeakyReLU', 'LogSigmoid', 'ReLU', 'ReLU6', 'RReLU', 'SELU', 'CELU', 'GELU', 'Sigmoid', 'SiLU', 'Mish', 'Softplus', 'Softshrink', 'Softsign', 'Tanh', 'Tanhshrink']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bBLbYvVV8As",
        "outputId": "372d9999-4ffc-4625-bd11-d76a0d1b39fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ELU\n",
            "Hardshrink\n",
            "Hardsigmoid\n",
            "Hardtanh\n",
            "Hardswish\n",
            "LeakyReLU\n",
            "LogSigmoid\n",
            "ReLU\n",
            "ReLU6\n",
            "RReLU\n",
            "SELU\n",
            "CELU\n",
            "GELU\n",
            "Sigmoid\n",
            "SiLU\n",
            "Mish\n",
            "Softplus\n",
            "Softshrink\n",
            "Softsign\n",
            "Tanh\n",
            "Tanhshrink\n",
            "╒══════════════════════╤═════════════════════╤════════════════════╕\n",
            "│ Activation functions │ f1                  │ Accuracy (%)       │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ ELU                  │ 0.14176298677921295 │ 22.238372093023255 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Hardshrink           │ 0.3045373857021332  │ 38.08139534883721  │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Hardsigmoid          │ 0.10277137160301208 │ 25.872093023255815 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Hardtanh             │ 0.09529411792755127 │ 23.546511627906977 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Hardswish            │ 0.7175038456916809  │ 71.58430232558139  │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ LeakyReLU            │ 0.7049323916435242  │ 70.13081395348837  │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ LogSigmoid           │ 0.08876270800828934 │ 21.584302325581394 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ ReLU                 │ 0.09529411792755127 │ 23.546511627906977 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ ReLU6                │ 0.6901417374610901  │ 68.75              │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ RReLU                │ 0.7290894985198975  │ 71.875             │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ SELU                 │ 0.09529411792755127 │ 23.546511627906977 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ CELU                 │ 0.14176298677921295 │ 22.238372093023255 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ GELU                 │ 0.6931672692298889  │ 68.1686046511628   │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Sigmoid              │ 0.10277137160301208 │ 25.872093023255815 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ SiLU                 │ 0.7419730424880981  │ 73.61918604651163  │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Mish                 │ 0.7142317295074463  │ 70.85755813953489  │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Softplus             │ 0.09529411792755127 │ 23.546511627906977 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Softshrink           │ 0.09529411792755127 │ 23.546511627906977 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Softsign             │ 0.10277137160301208 │ 25.872093023255815 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Tanh                 │ 0.09529411792755127 │ 23.546511627906977 │\n",
            "├──────────────────────┼─────────────────────┼────────────────────┤\n",
            "│ Tanhshrink           │ 0.7342555522918701  │ 72.96511627906976  │\n",
            "╘══════════════════════╧═════════════════════╧════════════════════╛\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "temp = [['Activation functions', 'f1', 'Accuracy (%)']]\n",
        "for a in NLAF:\n",
        "    print(a)\n",
        "    atr = getattr(nn, a)\n",
        "    net = LeNet(atr).to(device)\n",
        "    choose_model(num_epochs, optimizer_1, train_dataloader, loss_fn_1, net, learning_rate, val_dataloader, show=False, reproducibility=True)\n",
        "    optimal_model = LeNet(atr).to(device)\n",
        "    optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "    _, f1, accuracy, _ = test_nn(test_dataloader, optimal_model, loss_fn_1)\n",
        "    temp.append([a, f1, accuracy*100])\n",
        "\n",
        "print(tabulate(temp, tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLGEw_PqYFY-"
      },
      "outputs": [],
      "source": [
        "def choose_model(num_epochs, optimizer, train, loss_fn, model, learning_rate, val, reproducibility=False, show=True, lbfgs=False, scheduler=None, verbose=False):\n",
        "    # set seed before create your model  \n",
        "    if reproducibility:\n",
        "        torch_seed(seed=0)\n",
        "    \n",
        "    # copy by value the model\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    optimizer = optimizer(model_copy.parameters(), lr=learning_rate)\n",
        "    if scheduler != None:\n",
        "      if scheduler == torch.optim.lr_scheduler.ExponentialLR:\n",
        "        scheduler = scheduler(optimizer, gamma=0.9, verbose=verbose)\n",
        "      elif scheduler == torch.optim.lr_scheduler.LambdaLR or scheduler == torch.optim.lr_scheduler.MultiplicativeLR:\n",
        "        lmbda = lambda epoch: 0.95\n",
        "        scheduler = scheduler(optimizer, verbose=verbose, lr_lambda=lmbda)\n",
        "      elif scheduler == torch.optim.lr_scheduler.StepLR:\n",
        "        scheduler = scheduler(optimizer, step_size=30, gamma=0.1, verbose=verbose)\n",
        "      elif scheduler == torch.optim.lr_scheduler.MultiStepLR:\n",
        "        scheduler = scheduler(optimizer, milestones=[30], verbose=verbose)\n",
        "      elif scheduler == torch.optim.lr_scheduler.CosineAnnealingLR:\n",
        "        scheduler = scheduler(optimizer, T_max=30, verbose=verbose)\n",
        "      elif scheduler == torch.optim.lr_scheduler.ChainedScheduler:\n",
        "        scheduler1 = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=0.1, total_iters=2)\n",
        "        scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "        scheduler = scheduler([scheduler1, scheduler2])\n",
        "      elif scheduler == torch.optim.lr_scheduler.CyclicLR:\n",
        "        scheduler = scheduler(optimizer, base_lr=0.01, max_lr=0.1, verbose=verbose)\n",
        "      else:\n",
        "        scheduler = scheduler(optimizer, verbose=verbose)\n",
        "    f1_max = 0\n",
        "    for e in range(num_epochs):\n",
        "        model_copy.train()\n",
        "        for (data, label) in train:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                target = model_copy(data)\n",
        "                loss = loss_fn(target, label)\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            if lbfgs:\n",
        "                optimizer.step(closure)\n",
        "                target = model_copy(data)\n",
        "            else:\n",
        "                # Compute prediction and loss\n",
        "                target = model_copy(data)\n",
        "                # loss_fn defined above to be  nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(target, label)\n",
        "\n",
        "                # Backpropagation\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step() # updating the weights of neural network\n",
        "\n",
        "        if scheduler != None:\n",
        "          scheduler.step()\n",
        "\n",
        "        train_f1 = f1_score(target, label, num_classes=4, average='macro')\n",
        "\n",
        "        model_copy.eval()\n",
        "        for (X, y) in val:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            pred = model_copy(X)\n",
        "\n",
        "        val_f1 = f1_score(pred, y, num_classes=4, average='macro')\n",
        "\n",
        "        if show:\n",
        "            print(f'Epoch {e+1} \\t\\t Training f1: {train_f1} \\t\\t Validation f1: {val_f1}', end='')\n",
        "            if (reproducibility):\n",
        "                print(f'\\t\\t Loss: {loss}')\n",
        "            else:\n",
        "                print()\n",
        "\n",
        "\n",
        "        if f1_max < val_f1:\n",
        "            if show:\n",
        "                print(f'Validation f1 Increased({f1_max:.6f}--->{val_f1:.6f}) \\t Saving The Model')\n",
        "            f1_max = val_f1\n",
        "            # Saving State Dict\n",
        "            torch.save(model_copy.state_dict(), 'optimal.pth')\n",
        "    torch.save(model_copy.state_dict(), 'last.pth')\n",
        "    if show:\n",
        "        print('-----------------------------------------------------')\n",
        "        print(f'f1 optimal: {f1_max:.6f} \\nf1 train at end of epoch: {train_f1:.6f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLqWeIGrYI88",
        "outputId": "7b5ed2dc-b063-4b74-d9b0-53b14f1923df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 1.8000e-03.\n",
            "Adjusting learning rate of group 0 to 1.6200e-03.\n",
            "Adjusting learning rate of group 0 to 1.4580e-03.\n",
            "Adjusting learning rate of group 0 to 1.3122e-03.\n",
            "Adjusting learning rate of group 0 to 1.1810e-03.\n",
            "Adjusting learning rate of group 0 to 1.0629e-03.\n",
            "Adjusting learning rate of group 0 to 9.5659e-04.\n",
            "Adjusting learning rate of group 0 to 8.6093e-04.\n",
            "Adjusting learning rate of group 0 to 7.7484e-04.\n",
            "Adjusting learning rate of group 0 to 6.9736e-04.\n",
            "Adjusting learning rate of group 0 to 6.2762e-04.\n",
            "Adjusting learning rate of group 0 to 5.6486e-04.\n",
            "Adjusting learning rate of group 0 to 5.0837e-04.\n",
            "Adjusting learning rate of group 0 to 4.5754e-04.\n",
            "Adjusting learning rate of group 0 to 4.1178e-04.\n",
            "Adjusting learning rate of group 0 to 3.7060e-04.\n",
            "Adjusting learning rate of group 0 to 3.3354e-04.\n",
            "Adjusting learning rate of group 0 to 3.0019e-04.\n",
            "Adjusting learning rate of group 0 to 2.7017e-04.\n",
            "Adjusting learning rate of group 0 to 2.4315e-04.\n",
            "Adjusting learning rate of group 0 to 2.1884e-04.\n",
            "Adjusting learning rate of group 0 to 1.9695e-04.\n",
            "Adjusting learning rate of group 0 to 1.7726e-04.\n",
            "Adjusting learning rate of group 0 to 1.5953e-04.\n",
            "Adjusting learning rate of group 0 to 1.4358e-04.\n",
            "Adjusting learning rate of group 0 to 1.2922e-04.\n",
            "Adjusting learning rate of group 0 to 1.1630e-04.\n",
            "Adjusting learning rate of group 0 to 1.0467e-04.\n",
            "Adjusting learning rate of group 0 to 9.4203e-05.\n",
            "Adjusting learning rate of group 0 to 8.4782e-05.\n",
            "(0.0006817509095336116, 0.6644099950790405, 0.6613372093023255, tensor([[179.,  32.,  27.,  86.],\n",
            "        [ 32., 253.,   5.,   7.],\n",
            "        [ 31.,  13., 284.,  28.],\n",
            "        [116.,  20.,  69., 194.]]))\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "num_epochs = 30\n",
        "optimizer = optim.Adam\n",
        "scheduler = ExponentialLR\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 2e-3\n",
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net, learning_rate, val_dataloader, reproducibility=True, show = False, scheduler=scheduler, verbose = True)\n",
        "optimal_model = LeNet().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0fXtDwKbDXn"
      },
      "outputs": [],
      "source": [
        "sch = ['LambdaLR', 'MultiplicativeLR', 'StepLR', 'MultiStepLR', 'ConstantLR', 'LinearLR', 'ExponentialLR', 'CosineAnnealingLR', 'ChainedScheduler']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leVAaZ4SbESN",
        "outputId": "d6b09661-b05d-4625-e852-ac90e5f96648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LambdaLR\n",
            "MultiplicativeLR\n",
            "StepLR\n",
            "MultiStepLR\n",
            "ConstantLR\n",
            "LinearLR\n",
            "ExponentialLR\n",
            "CosineAnnealingLR\n",
            "ChainedScheduler\n",
            "╒═══════════════════╤════════════════════╤═══════════════════╕\n",
            "│ Schedulers        │ f1                 │ Accuracy (%)      │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ LambdaLR          │ 0.6959515810012817 │ 69.3313953488372  │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ MultiplicativeLR  │ 0.6795541644096375 │ 66.86046511627907 │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ StepLR            │ 0.6766413450241089 │ 67.44186046511628 │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ MultiStepLR       │ 0.6766413450241089 │ 67.44186046511628 │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ ConstantLR        │ 0.700812816619873  │ 69.98546511627907 │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ LinearLR          │ 0.7183283567428589 │ 71.51162790697676 │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ ExponentialLR     │ 0.6644099950790405 │ 66.13372093023256 │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ CosineAnnealingLR │ 0.6637973785400391 │ 65.91569767441861 │\n",
            "├───────────────────┼────────────────────┼───────────────────┤\n",
            "│ ChainedScheduler  │ 0.7422173619270325 │ 73.76453488372093 │\n",
            "╘═══════════════════╧════════════════════╧═══════════════════╛\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "temp = [['Schedulers', 'f1', 'Accuracy (%)']]\n",
        "for a in sch:\n",
        "    print(a)\n",
        "    atr = getattr(torch.optim.lr_scheduler, a)\n",
        "    net = LeNet().to(device)\n",
        "    choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net, learning_rate, val_dataloader, show=False, reproducibility=True, scheduler = atr)\n",
        "    optimal_model = LeNet().to(device)\n",
        "    optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "    _, f1, accuracy, _ = test_nn(test_dataloader, optimal_model, loss_fn_1)\n",
        "    temp.append([a, f1, accuracy*100])\n",
        "\n",
        "print(tabulate(temp, tablefmt=\"fancy_grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "msRPJYpdqxJa"
      },
      "outputs": [],
      "source": [
        "class LeNet1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet1, self).__init__()\n",
        "    self.convolution_layer = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "    )\n",
        "\n",
        "    self.linear_relu = nn.Sequential(\n",
        "        nn.Linear(1024, 1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 4),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolution_layer(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    logits = self.linear_relu(x)\n",
        "    return logits\n",
        "\n",
        "net1 = LeNet1().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAGwEEncwjaZ",
        "outputId": "2f07ef52-3928-4bbf-db83-2ad504fc0855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.000669376231556715, 0.7792076468467712, 0.7747093023255814, tensor([[230.,  10.,  38.,  46.],\n",
            "        [ 19., 270.,   4.,   4.],\n",
            "        [ 16.,   1., 317.,  22.],\n",
            "        [102.,  12.,  36., 249.]]))\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "optimizer = optim.Adam\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 2e-3\n",
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net1, learning_rate, val_dataloader, reproducibility=True, show = False)\n",
        "optimal_model = LeNet1().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCeukpXvrxYH"
      },
      "outputs": [],
      "source": [
        "def choose_model(num_epochs, optimizer, train, loss_fn, model, learning_rate, val, reproducibility=False, wd=0.0, show=True, lbfgs=False):\n",
        "    # set seed before create your model  \n",
        "    if reproducibility:\n",
        "        torch_seed(seed=0)\n",
        "    \n",
        "    # copy by value the model\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    optimizer = optimizer(model_copy.parameters(), lr=learning_rate, weight_decay=wd )\n",
        "    f1_max = 0\n",
        "    for e in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        model_copy.train()\n",
        "        for (data, label) in train:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                target = model_copy(data)\n",
        "                loss = loss_fn(target, label)\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            if lbfgs:\n",
        "                optimizer.step(closure)\n",
        "                target = model_copy(data)\n",
        "            else:\n",
        "                # Compute prediction and loss\n",
        "                target = model_copy(data)\n",
        "                # loss_fn defined above to be  nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(target, label)\n",
        "\n",
        "                # Backpropagation\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step() # updating the weights of neural network\n",
        "                train_loss += loss.item()\n",
        "\n",
        "        train_f1 = f1_score(target, label, num_classes=4, average='macro')\n",
        "\n",
        "        valid_loss = 0.0\n",
        "        model_copy.eval()\n",
        "        for (X, y) in val:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            pred = model_copy(X)\n",
        "\n",
        "            loss = loss_fn(pred, y)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "        val_f1 = f1_score(pred, y, num_classes=4, average='macro')\n",
        "\n",
        "        if show:\n",
        "            print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train)} \\t\\t Validation Loss: {valid_loss / len(val)}')\n",
        "\n",
        "\n",
        "        if f1_max < val_f1:\n",
        "            f1_max = val_f1\n",
        "            # Saving State Dict\n",
        "            torch.save(model_copy.state_dict(), 'optimal.pth')\n",
        "    torch.save(model_copy.state_dict(), 'last.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYDbgOIpw4mu",
        "outputId": "1803da08-d073-4235-a91b-acba9c7aa182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training Loss: 1.0227228454500437 \t\t Validation Loss: 0.9796355366706848\n",
            "Epoch 2 \t\t Training Loss: 0.759075411260128 \t\t Validation Loss: 0.7287921905517578\n",
            "Epoch 3 \t\t Training Loss: 0.6666638664901257 \t\t Validation Loss: 0.8450986742973328\n",
            "Epoch 4 \t\t Training Loss: 0.6045088172703982 \t\t Validation Loss: 0.7578967809677124\n",
            "Epoch 5 \t\t Training Loss: 0.5483834965899587 \t\t Validation Loss: 0.9878327250480652\n",
            "Epoch 6 \t\t Training Loss: 0.5044965025037527 \t\t Validation Loss: 0.8449056148529053\n",
            "Epoch 7 \t\t Training Loss: 0.49651616752147676 \t\t Validation Loss: 1.1259151697158813\n",
            "Epoch 8 \t\t Training Loss: 0.43058030627667904 \t\t Validation Loss: 0.5439536571502686\n",
            "Epoch 9 \t\t Training Loss: 0.4044629971310496 \t\t Validation Loss: 2.6772091388702393\n",
            "Epoch 10 \t\t Training Loss: 0.41922647718340156 \t\t Validation Loss: 0.9468997716903687\n",
            "Epoch 11 \t\t Training Loss: 0.3464579423330724 \t\t Validation Loss: 0.9095437526702881\n",
            "Epoch 12 \t\t Training Loss: 0.3236820953898132 \t\t Validation Loss: 0.6053068041801453\n",
            "Epoch 13 \t\t Training Loss: 0.29361295646755026 \t\t Validation Loss: 1.0244674682617188\n",
            "Epoch 14 \t\t Training Loss: 0.2535429348424077 \t\t Validation Loss: 1.0777682065963745\n",
            "Epoch 15 \t\t Training Loss: 0.22595158204552718 \t\t Validation Loss: 0.7188482880592346\n",
            "Epoch 16 \t\t Training Loss: 0.22702851748093963 \t\t Validation Loss: 0.7365981936454773\n",
            "Epoch 17 \t\t Training Loss: 0.1962534345756285 \t\t Validation Loss: 0.8164304494857788\n",
            "Epoch 18 \t\t Training Loss: 0.17354011951130816 \t\t Validation Loss: 1.689194679260254\n",
            "Epoch 19 \t\t Training Loss: 0.1295043843903113 \t\t Validation Loss: 0.9426295757293701\n",
            "Epoch 20 \t\t Training Loss: 0.10441520210064482 \t\t Validation Loss: 0.7966793775558472\n",
            "Epoch 21 \t\t Training Loss: 0.10293311786721461 \t\t Validation Loss: 0.9072368741035461\n",
            "Epoch 22 \t\t Training Loss: 0.09275281778034696 \t\t Validation Loss: 3.110077142715454\n",
            "Epoch 23 \t\t Training Loss: 0.0812068103688216 \t\t Validation Loss: 0.9601784348487854\n",
            "Epoch 24 \t\t Training Loss: 0.1137839402433019 \t\t Validation Loss: 1.2972999811172485\n",
            "Epoch 25 \t\t Training Loss: 0.07833980434159457 \t\t Validation Loss: 1.1484746932983398\n",
            "Epoch 26 \t\t Training Loss: 0.04706687477268133 \t\t Validation Loss: 1.299917459487915\n",
            "Epoch 27 \t\t Training Loss: 0.09788639154106932 \t\t Validation Loss: 0.955881655216217\n",
            "Epoch 28 \t\t Training Loss: 0.048343814363252025 \t\t Validation Loss: 1.2338340282440186\n",
            "Epoch 29 \t\t Training Loss: 0.054463318642956435 \t\t Validation Loss: 1.0774439573287964\n",
            "Epoch 30 \t\t Training Loss: 0.062472404817635836 \t\t Validation Loss: 0.8856526017189026\n",
            "Epoch 31 \t\t Training Loss: 0.06034491350275857 \t\t Validation Loss: 1.2242956161499023\n",
            "Epoch 32 \t\t Training Loss: 0.02458634028101187 \t\t Validation Loss: 1.661469578742981\n",
            "Epoch 33 \t\t Training Loss: 0.026859359819982272 \t\t Validation Loss: 1.387825608253479\n",
            "Epoch 34 \t\t Training Loss: 0.04879310669555707 \t\t Validation Loss: 1.193811058998108\n",
            "Epoch 35 \t\t Training Loss: 0.04604656225761119 \t\t Validation Loss: 0.9522918462753296\n",
            "Epoch 36 \t\t Training Loss: 0.03586011641546065 \t\t Validation Loss: 1.5773682594299316\n",
            "Epoch 37 \t\t Training Loss: 0.03582032407416591 \t\t Validation Loss: 1.233421802520752\n",
            "Epoch 38 \t\t Training Loss: 0.051656668481391534 \t\t Validation Loss: 1.091444730758667\n",
            "Epoch 39 \t\t Training Loss: 0.053476539022703946 \t\t Validation Loss: 2.819681406021118\n",
            "Epoch 40 \t\t Training Loss: 0.04667475262495827 \t\t Validation Loss: 1.43446946144104\n",
            "Epoch 41 \t\t Training Loss: 0.018053964496297682 \t\t Validation Loss: 1.5991750955581665\n",
            "Epoch 42 \t\t Training Loss: 0.022510439610548473 \t\t Validation Loss: 1.9368351697921753\n",
            "Epoch 43 \t\t Training Loss: 0.04744676242440619 \t\t Validation Loss: 1.3939875364303589\n",
            "Epoch 44 \t\t Training Loss: 0.03534447548436788 \t\t Validation Loss: 1.2027368545532227\n",
            "Epoch 45 \t\t Training Loss: 0.04223728245333376 \t\t Validation Loss: 1.1677806377410889\n",
            "Epoch 46 \t\t Training Loss: 0.013094598224172386 \t\t Validation Loss: 1.6800568103790283\n",
            "Epoch 47 \t\t Training Loss: 0.040979887175584935 \t\t Validation Loss: 1.2190440893173218\n",
            "Epoch 48 \t\t Training Loss: 0.032007649072895675 \t\t Validation Loss: 1.3607096672058105\n",
            "Epoch 49 \t\t Training Loss: 0.02747995601169805 \t\t Validation Loss: 1.4021046161651611\n",
            "Epoch 50 \t\t Training Loss: 0.021793199141290104 \t\t Validation Loss: 1.5552208423614502\n",
            "Epoch 51 \t\t Training Loss: 0.02627690120372108 \t\t Validation Loss: 1.2165915966033936\n",
            "Epoch 52 \t\t Training Loss: 0.0316048690300363 \t\t Validation Loss: 3.532170295715332\n",
            "Epoch 53 \t\t Training Loss: 0.043990848291994095 \t\t Validation Loss: 1.2022794485092163\n",
            "Epoch 54 \t\t Training Loss: 0.026196138851195518 \t\t Validation Loss: 1.0804427862167358\n",
            "Epoch 55 \t\t Training Loss: 0.01975916013696393 \t\t Validation Loss: 2.165553092956543\n",
            "Epoch 56 \t\t Training Loss: 0.03490260474951526 \t\t Validation Loss: 0.9368267059326172\n",
            "Epoch 57 \t\t Training Loss: 0.06308276143942294 \t\t Validation Loss: 0.8928695917129517\n",
            "Epoch 58 \t\t Training Loss: 0.03556110000214176 \t\t Validation Loss: 1.2438411712646484\n",
            "Epoch 59 \t\t Training Loss: 0.031961076043941716 \t\t Validation Loss: 1.8041702508926392\n",
            "Epoch 60 \t\t Training Loss: 0.021944407465545056 \t\t Validation Loss: 2.1736631393432617\n",
            "(0.0004885048973698948, 0.7414819002151489, 0.7398255813953488, tensor([[203.,  37.,  45.,  39.],\n",
            "        [  7., 289.,   1.,   0.],\n",
            "        [ 45.,   2., 300.,   9.],\n",
            "        [126.,  23.,  24., 226.]]))\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 60\n",
        "optimizer = optim.Adam\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 2e-3\n",
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net1, learning_rate, val_dataloader, reproducibility=True)\n",
        "optimal_model = LeNet1().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTbbksgQ2jEj",
        "outputId": "4dbe059d-2d03-4611-c39a-b82531cfe544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training Loss: 1.1940588575601578 \t\t Validation Loss: 1.1537491083145142\n",
            "Epoch 2 \t\t Training Loss: 1.0899036577343941 \t\t Validation Loss: 9.296834945678711\n",
            "Epoch 3 \t\t Training Loss: 1.0222694870829583 \t\t Validation Loss: 1.2506409883499146\n",
            "Epoch 4 \t\t Training Loss: 0.9714676383137703 \t\t Validation Loss: 0.943543553352356\n",
            "Epoch 5 \t\t Training Loss: 0.9301613396406174 \t\t Validation Loss: 0.9249820709228516\n",
            "Epoch 6 \t\t Training Loss: 0.9124936556816101 \t\t Validation Loss: 1.107026219367981\n",
            "Epoch 7 \t\t Training Loss: 0.874731510579586 \t\t Validation Loss: 2.1015889644622803\n",
            "Epoch 8 \t\t Training Loss: 0.8625895711779594 \t\t Validation Loss: 0.8712542653083801\n",
            "Epoch 9 \t\t Training Loss: 0.8551184546947479 \t\t Validation Loss: 0.8063517808914185\n",
            "Epoch 10 \t\t Training Loss: 0.8429585041105747 \t\t Validation Loss: 0.7877652049064636\n",
            "Epoch 11 \t\t Training Loss: 0.8394316101074218 \t\t Validation Loss: 0.814486563205719\n",
            "Epoch 12 \t\t Training Loss: 0.8336206349730492 \t\t Validation Loss: 0.872101366519928\n",
            "Epoch 13 \t\t Training Loss: 0.7987347604334354 \t\t Validation Loss: 1.004318118095398\n",
            "Epoch 14 \t\t Training Loss: 0.8063135465979576 \t\t Validation Loss: 0.8258864879608154\n",
            "Epoch 15 \t\t Training Loss: 0.7993354950845242 \t\t Validation Loss: 1.114814281463623\n",
            "Epoch 16 \t\t Training Loss: 0.8088009563088417 \t\t Validation Loss: 0.8217279314994812\n",
            "Epoch 17 \t\t Training Loss: 0.7884008553624153 \t\t Validation Loss: 0.8696229457855225\n",
            "Epoch 18 \t\t Training Loss: 0.7953648403286934 \t\t Validation Loss: 0.8333295583724976\n",
            "Epoch 19 \t\t Training Loss: 0.7832044108211994 \t\t Validation Loss: 0.8241967558860779\n",
            "Epoch 20 \t\t Training Loss: 0.7896320958435535 \t\t Validation Loss: 0.7887428998947144\n",
            "Epoch 21 \t\t Training Loss: 0.7857911248505115 \t\t Validation Loss: 1.167222023010254\n",
            "Epoch 22 \t\t Training Loss: 0.7856598085165024 \t\t Validation Loss: 0.8352785706520081\n",
            "Epoch 23 \t\t Training Loss: 0.7814304323494434 \t\t Validation Loss: 0.8020418286323547\n",
            "Epoch 24 \t\t Training Loss: 0.7831150729954243 \t\t Validation Loss: 0.8495339751243591\n",
            "Epoch 25 \t\t Training Loss: 0.771323352009058 \t\t Validation Loss: 0.7800597548484802\n",
            "Epoch 26 \t\t Training Loss: 0.7634187062084675 \t\t Validation Loss: 0.8488949537277222\n",
            "Epoch 27 \t\t Training Loss: 0.7635907205939293 \t\t Validation Loss: 0.7405813336372375\n",
            "Epoch 28 \t\t Training Loss: 0.77265989869833 \t\t Validation Loss: 1.0894553661346436\n",
            "Epoch 29 \t\t Training Loss: 0.7824075827002526 \t\t Validation Loss: 0.8596414923667908\n",
            "Epoch 30 \t\t Training Loss: 0.7677716854214668 \t\t Validation Loss: 0.7510979175567627\n",
            "Epoch 31 \t\t Training Loss: 0.7698997636139393 \t\t Validation Loss: 0.956478476524353\n",
            "Epoch 32 \t\t Training Loss: 0.7518912860751152 \t\t Validation Loss: 0.8639890551567078\n",
            "Epoch 33 \t\t Training Loss: 0.7643228855729103 \t\t Validation Loss: 0.7688848972320557\n",
            "Epoch 34 \t\t Training Loss: 0.7681766188144684 \t\t Validation Loss: 0.8483714461326599\n",
            "Epoch 35 \t\t Training Loss: 0.7748719914257527 \t\t Validation Loss: 0.825370192527771\n",
            "Epoch 36 \t\t Training Loss: 0.7544249115884304 \t\t Validation Loss: 0.8979841470718384\n",
            "Epoch 37 \t\t Training Loss: 0.7560226790606975 \t\t Validation Loss: 0.9527685642242432\n",
            "Epoch 38 \t\t Training Loss: 0.7429789650440216 \t\t Validation Loss: 0.8194370269775391\n",
            "Epoch 39 \t\t Training Loss: 0.7471065784990788 \t\t Validation Loss: 0.9890430569648743\n",
            "Epoch 40 \t\t Training Loss: 0.7639083530008793 \t\t Validation Loss: 0.7341123223304749\n",
            "Epoch 41 \t\t Training Loss: 0.7724861630797386 \t\t Validation Loss: 0.852520763874054\n",
            "Epoch 42 \t\t Training Loss: 0.7463110490143299 \t\t Validation Loss: 0.8623978495597839\n",
            "Epoch 43 \t\t Training Loss: 0.7472270825505256 \t\t Validation Loss: 0.8518417477607727\n",
            "Epoch 44 \t\t Training Loss: 0.7570797710120678 \t\t Validation Loss: 0.8821150064468384\n",
            "Epoch 45 \t\t Training Loss: 0.7628191176056862 \t\t Validation Loss: 1.083929181098938\n",
            "Epoch 46 \t\t Training Loss: 0.7580197423696518 \t\t Validation Loss: 0.7782397270202637\n",
            "Epoch 47 \t\t Training Loss: 0.7590888433158398 \t\t Validation Loss: 0.7726871371269226\n",
            "Epoch 48 \t\t Training Loss: 0.7467519026994706 \t\t Validation Loss: 0.806537926197052\n",
            "Epoch 49 \t\t Training Loss: 0.7468491868674755 \t\t Validation Loss: 0.996264636516571\n",
            "Epoch 50 \t\t Training Loss: 0.7405695286393166 \t\t Validation Loss: 0.774604856967926\n",
            "Epoch 51 \t\t Training Loss: 0.726927110850811 \t\t Validation Loss: 1.2163844108581543\n",
            "Epoch 52 \t\t Training Loss: 0.7546669618785381 \t\t Validation Loss: 0.9037593007087708\n",
            "Epoch 53 \t\t Training Loss: 0.7299946156144143 \t\t Validation Loss: 0.9151841998100281\n",
            "Epoch 54 \t\t Training Loss: 0.7542684599757195 \t\t Validation Loss: 0.7689988613128662\n",
            "Epoch 55 \t\t Training Loss: 0.7549476681649685 \t\t Validation Loss: 1.0848251581192017\n",
            "Epoch 56 \t\t Training Loss: 0.733087588250637 \t\t Validation Loss: 0.7888967394828796\n",
            "Epoch 57 \t\t Training Loss: 0.7322834955155849 \t\t Validation Loss: 0.8182471990585327\n",
            "Epoch 58 \t\t Training Loss: 0.7343323864042759 \t\t Validation Loss: 0.794053852558136\n",
            "Epoch 59 \t\t Training Loss: 0.7543346090614795 \t\t Validation Loss: 0.7545512914657593\n",
            "Epoch 60 \t\t Training Loss: 0.742410801500082 \t\t Validation Loss: 0.7763221263885498\n",
            "(0.0006184786298247271, 0.6811754703521729, 0.6940406976744186, tensor([[118.,  70.,  22., 114.],\n",
            "        [  0., 295.,   0.,   2.],\n",
            "        [ 65.,  13., 261.,  17.],\n",
            "        [ 54.,  45.,  19., 281.]]))\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 60\n",
        "optimizer = optim.Adam\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 2e-3\n",
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net1, learning_rate, val_dataloader, reproducibility=True, wd=0.05)\n",
        "optimal_model = LeNet1().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bLfGQot5FnL",
        "outputId": "78b05e55-8d96-4476-fb9a-4a0aba5f0eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training Loss: 1.3811385637521745 \t\t Validation Loss: 1.38637113571167\n",
            "Epoch 2 \t\t Training Loss: 1.386584571003914 \t\t Validation Loss: 1.3863071203231812\n",
            "Epoch 3 \t\t Training Loss: 1.386584106683731 \t\t Validation Loss: 1.3863036632537842\n",
            "Epoch 4 \t\t Training Loss: 1.3866084283590316 \t\t Validation Loss: 1.3863126039505005\n",
            "Epoch 5 \t\t Training Loss: 1.3865894573926925 \t\t Validation Loss: 1.3863091468811035\n",
            "Epoch 6 \t\t Training Loss: 1.3865668588876725 \t\t Validation Loss: 1.3863497972488403\n",
            "Epoch 7 \t\t Training Loss: 1.386678120493889 \t\t Validation Loss: 1.3862981796264648\n",
            "Epoch 8 \t\t Training Loss: 1.3866682326793671 \t\t Validation Loss: 1.3863105773925781\n",
            "Epoch 9 \t\t Training Loss: 1.3867467308044434 \t\t Validation Loss: 1.3863019943237305\n",
            "Epoch 10 \t\t Training Loss: 1.3866607224941254 \t\t Validation Loss: 1.3863083124160767\n",
            "Epoch 11 \t\t Training Loss: 1.3866775476932525 \t\t Validation Loss: 1.3863006830215454\n",
            "Epoch 12 \t\t Training Loss: 1.3866899460554123 \t\t Validation Loss: 1.3863035440444946\n",
            "Epoch 13 \t\t Training Loss: 1.386622535586357 \t\t Validation Loss: 1.3863062858581543\n",
            "Epoch 14 \t\t Training Loss: 1.3865930956602097 \t\t Validation Loss: 1.3863019943237305\n",
            "Epoch 15 \t\t Training Loss: 1.3865779012441635 \t\t Validation Loss: 1.3863341808319092\n",
            "Epoch 16 \t\t Training Loss: 1.386673412322998 \t\t Validation Loss: 1.3863013982772827\n",
            "Epoch 17 \t\t Training Loss: 1.3866242277622223 \t\t Validation Loss: 1.3863121271133423\n",
            "Epoch 18 \t\t Training Loss: 1.386622365117073 \t\t Validation Loss: 1.3862965106964111\n",
            "Epoch 19 \t\t Training Loss: 1.3866238856315614 \t\t Validation Loss: 1.3863110542297363\n",
            "Epoch 20 \t\t Training Loss: 1.3866463470458985 \t\t Validation Loss: 1.386298418045044\n",
            "Epoch 21 \t\t Training Loss: 1.3865772479772567 \t\t Validation Loss: 1.386309027671814\n",
            "Epoch 22 \t\t Training Loss: 1.386545547246933 \t\t Validation Loss: 1.3863308429718018\n",
            "Epoch 23 \t\t Training Loss: 1.386686362028122 \t\t Validation Loss: 1.3863072395324707\n",
            "Epoch 24 \t\t Training Loss: 1.3866579174995421 \t\t Validation Loss: 1.3863023519515991\n",
            "Epoch 25 \t\t Training Loss: 1.3865417778491973 \t\t Validation Loss: 1.386308193206787\n",
            "Epoch 26 \t\t Training Loss: 1.3865852642059326 \t\t Validation Loss: 1.3863047361373901\n",
            "Epoch 27 \t\t Training Loss: 1.3865639382600785 \t\t Validation Loss: 1.3862974643707275\n",
            "Epoch 28 \t\t Training Loss: 1.3865606439113618 \t\t Validation Loss: 1.3863129615783691\n",
            "Epoch 29 \t\t Training Loss: 1.386668044924736 \t\t Validation Loss: 1.3863439559936523\n",
            "Epoch 30 \t\t Training Loss: 1.3865799617767334 \t\t Validation Loss: 1.3862974643707275\n",
            "Epoch 31 \t\t Training Loss: 1.3865913301706314 \t\t Validation Loss: 1.3863120079040527\n",
            "Epoch 32 \t\t Training Loss: 1.3865741419792175 \t\t Validation Loss: 1.3863056898117065\n",
            "Epoch 33 \t\t Training Loss: 1.3866074198484422 \t\t Validation Loss: 1.3863106966018677\n",
            "Epoch 34 \t\t Training Loss: 1.3866280007362366 \t\t Validation Loss: 1.3863013982772827\n",
            "Epoch 35 \t\t Training Loss: 1.386634606719017 \t\t Validation Loss: 1.3862990140914917\n",
            "Epoch 36 \t\t Training Loss: 1.386698009967804 \t\t Validation Loss: 1.386350154876709\n",
            "Epoch 37 \t\t Training Loss: 1.3865359169244766 \t\t Validation Loss: 1.3863036632537842\n",
            "Epoch 38 \t\t Training Loss: 1.3865531742572785 \t\t Validation Loss: 1.3863272666931152\n",
            "Epoch 39 \t\t Training Loss: 1.386677589416504 \t\t Validation Loss: 1.3863049745559692\n",
            "Epoch 40 \t\t Training Loss: 1.3866456335783004 \t\t Validation Loss: 1.3863147497177124\n",
            "Epoch 41 \t\t Training Loss: 1.3865796005725861 \t\t Validation Loss: 1.3863019943237305\n",
            "Epoch 42 \t\t Training Loss: 1.3865971875190735 \t\t Validation Loss: 1.386296272277832\n",
            "Epoch 43 \t\t Training Loss: 1.3866899013519287 \t\t Validation Loss: 1.386300802230835\n",
            "Epoch 44 \t\t Training Loss: 1.3865982347726822 \t\t Validation Loss: 1.3862969875335693\n",
            "Epoch 45 \t\t Training Loss: 1.3866125398874283 \t\t Validation Loss: 1.3863166570663452\n",
            "Epoch 46 \t\t Training Loss: 1.3867579060792923 \t\t Validation Loss: 1.3863414525985718\n",
            "Epoch 47 \t\t Training Loss: 1.386690801382065 \t\t Validation Loss: 1.386335849761963\n",
            "Epoch 48 \t\t Training Loss: 1.3865938168764114 \t\t Validation Loss: 1.386301875114441\n",
            "Epoch 49 \t\t Training Loss: 1.3865603494644165 \t\t Validation Loss: 1.3863416910171509\n",
            "Epoch 50 \t\t Training Loss: 1.386647003889084 \t\t Validation Loss: 1.386297583580017\n",
            "Epoch 51 \t\t Training Loss: 1.3865826600790023 \t\t Validation Loss: 1.3863403797149658\n",
            "Epoch 52 \t\t Training Loss: 1.3865856766700744 \t\t Validation Loss: 1.3863558769226074\n",
            "Epoch 53 \t\t Training Loss: 1.3865726834535599 \t\t Validation Loss: 1.3863109350204468\n",
            "Epoch 54 \t\t Training Loss: 1.3865720611810683 \t\t Validation Loss: 1.3863191604614258\n",
            "Epoch 55 \t\t Training Loss: 1.3866382950544358 \t\t Validation Loss: 1.3863110542297363\n",
            "Epoch 56 \t\t Training Loss: 1.3865923738479615 \t\t Validation Loss: 1.3863115310668945\n",
            "Epoch 57 \t\t Training Loss: 1.3864932548999787 \t\t Validation Loss: 1.3863426446914673\n",
            "Epoch 58 \t\t Training Loss: 1.386627897620201 \t\t Validation Loss: 1.3863028287887573\n",
            "Epoch 59 \t\t Training Loss: 1.3866890680789947 \t\t Validation Loss: 1.3863025903701782\n",
            "Epoch 60 \t\t Training Loss: 1.3866448402404785 \t\t Validation Loss: 1.3863133192062378\n",
            "(0.001006886276394822, 0.10277137160301208, 0.25872093023255816, tensor([[  0.,   0., 324.,   0.],\n",
            "        [  0.,   0., 297.,   0.],\n",
            "        [  0.,   0., 356.,   0.],\n",
            "        [  0.,   0., 399.,   0.]]))\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 60\n",
        "optimizer = optim.Adam\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 2e-3\n",
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net1, learning_rate, val_dataloader, reproducibility=True, wd=0.1)\n",
        "optimal_model = LeNet1().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_ZZi-dqb6s_K"
      },
      "outputs": [],
      "source": [
        "class LeNetDrop(nn.Module):\n",
        "  def __init__(self, do):\n",
        "    super(LeNetDrop, self).__init__()\n",
        "    self.convolution_layer = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "    )\n",
        "\n",
        "    self.linear_relu = nn.Sequential(\n",
        "        nn.Dropout(do),\n",
        "        nn.Linear(1024, 1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(do),\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(do),\n",
        "        nn.Linear(256, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(do),\n",
        "        nn.Linear(32, 4),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolution_layer(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    logits = self.linear_relu(x)\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXhP38iH7lHT",
        "outputId": "85c538ee-29ee-4a64-f3a1-6b0690afba68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training Loss: 1.321156659722328 \t\t Validation Loss: 1.1661072969436646\n",
            "Epoch 2 \t\t Training Loss: 1.23952639490366 \t\t Validation Loss: 1.1802884340286255\n",
            "Epoch 3 \t\t Training Loss: 1.1338506725430488 \t\t Validation Loss: 1.5577853918075562\n",
            "Epoch 4 \t\t Training Loss: 0.9656573216617107 \t\t Validation Loss: 1.7948826551437378\n",
            "Epoch 5 \t\t Training Loss: 0.9202900639176369 \t\t Validation Loss: 2.8437509536743164\n",
            "Epoch 6 \t\t Training Loss: 0.8378933982551098 \t\t Validation Loss: 0.6299632787704468\n",
            "Epoch 7 \t\t Training Loss: 0.773862941712141 \t\t Validation Loss: 0.7210538387298584\n",
            "Epoch 8 \t\t Training Loss: 0.7453544361144304 \t\t Validation Loss: 0.6883983016014099\n",
            "Epoch 9 \t\t Training Loss: 0.7253496524691582 \t\t Validation Loss: 0.5949004888534546\n",
            "Epoch 10 \t\t Training Loss: 0.7558138680458069 \t\t Validation Loss: 0.9200499057769775\n",
            "Epoch 11 \t\t Training Loss: 0.6716405886411667 \t\t Validation Loss: 0.7231597900390625\n",
            "Epoch 12 \t\t Training Loss: 0.6441356351971627 \t\t Validation Loss: 0.7933148145675659\n",
            "Epoch 13 \t\t Training Loss: 0.6458980417996645 \t\t Validation Loss: 0.6318975687026978\n",
            "Epoch 14 \t\t Training Loss: 0.6029743369668722 \t\t Validation Loss: 0.7554040551185608\n",
            "Epoch 15 \t\t Training Loss: 0.6144880777224898 \t\t Validation Loss: 0.9419734477996826\n",
            "Epoch 16 \t\t Training Loss: 0.5754149561375379 \t\t Validation Loss: 0.878018856048584\n",
            "Epoch 17 \t\t Training Loss: 0.572568428479135 \t\t Validation Loss: 0.7042397856712341\n",
            "Epoch 18 \t\t Training Loss: 0.5559658256918192 \t\t Validation Loss: 0.586631715297699\n",
            "Epoch 19 \t\t Training Loss: 0.5168239551782609 \t\t Validation Loss: 0.9823631048202515\n",
            "Epoch 20 \t\t Training Loss: 0.48895085491240026 \t\t Validation Loss: 0.6425619721412659\n",
            "Epoch 21 \t\t Training Loss: 0.4772123448178172 \t\t Validation Loss: 0.8079134225845337\n",
            "Epoch 22 \t\t Training Loss: 0.4705481153726578 \t\t Validation Loss: 0.6214329600334167\n",
            "Epoch 23 \t\t Training Loss: 0.45954723164439204 \t\t Validation Loss: 0.6439283490180969\n",
            "Epoch 24 \t\t Training Loss: 0.4366875891108066 \t\t Validation Loss: 0.8716886639595032\n",
            "Epoch 25 \t\t Training Loss: 0.4580028904974461 \t\t Validation Loss: 0.7637172937393188\n",
            "Epoch 26 \t\t Training Loss: 0.4025227933563292 \t\t Validation Loss: 0.6314311623573303\n",
            "Epoch 27 \t\t Training Loss: 0.3894241390004754 \t\t Validation Loss: 0.7736672163009644\n",
            "Epoch 28 \t\t Training Loss: 0.3609205567929894 \t\t Validation Loss: 0.6588444709777832\n",
            "Epoch 29 \t\t Training Loss: 0.4205518621020019 \t\t Validation Loss: 0.773455023765564\n",
            "Epoch 30 \t\t Training Loss: 0.3837763547897339 \t\t Validation Loss: 0.7772303223609924\n",
            "Epoch 31 \t\t Training Loss: 0.35180788800120355 \t\t Validation Loss: 0.8082424402236938\n",
            "Epoch 32 \t\t Training Loss: 0.3204188569728285 \t\t Validation Loss: 0.9679707288742065\n",
            "Epoch 33 \t\t Training Loss: 0.3203116869926453 \t\t Validation Loss: 0.8566438555717468\n",
            "Epoch 34 \t\t Training Loss: 0.30556521053018515 \t\t Validation Loss: 0.7106797695159912\n",
            "Epoch 35 \t\t Training Loss: 0.29826458063675093 \t\t Validation Loss: 0.9440659880638123\n",
            "Epoch 36 \t\t Training Loss: 0.2995045974617824 \t\t Validation Loss: 0.8410679697990417\n",
            "Epoch 37 \t\t Training Loss: 0.253306672910403 \t\t Validation Loss: 1.134496808052063\n",
            "Epoch 38 \t\t Training Loss: 0.27811448891297913 \t\t Validation Loss: 0.74763023853302\n",
            "Epoch 39 \t\t Training Loss: 0.2637028994131833 \t\t Validation Loss: 0.9274235367774963\n",
            "Epoch 40 \t\t Training Loss: 0.2594734062068164 \t\t Validation Loss: 0.9092579483985901\n",
            "Epoch 41 \t\t Training Loss: 0.3396804387826705 \t\t Validation Loss: 0.9978904128074646\n",
            "Epoch 42 \t\t Training Loss: 0.2912173954715763 \t\t Validation Loss: 1.2409461736679077\n",
            "Epoch 43 \t\t Training Loss: 0.2797263372689486 \t\t Validation Loss: 0.9009645581245422\n",
            "Epoch 44 \t\t Training Loss: 0.2388295558700338 \t\t Validation Loss: 0.9328565001487732\n",
            "Epoch 45 \t\t Training Loss: 0.2480540598963853 \t\t Validation Loss: 1.1906346082687378\n",
            "Epoch 46 \t\t Training Loss: 0.22050627897959202 \t\t Validation Loss: 0.783943235874176\n",
            "Epoch 47 \t\t Training Loss: 0.25267898191232235 \t\t Validation Loss: 1.0505470037460327\n",
            "Epoch 48 \t\t Training Loss: 0.24170707407407463 \t\t Validation Loss: 1.2593798637390137\n",
            "Epoch 49 \t\t Training Loss: 0.2018841521150898 \t\t Validation Loss: 2.431675672531128\n",
            "Epoch 50 \t\t Training Loss: 0.20121384744823445 \t\t Validation Loss: 1.096725344657898\n",
            "Epoch 51 \t\t Training Loss: 0.16460267626360292 \t\t Validation Loss: 1.3333543539047241\n",
            "Epoch 52 \t\t Training Loss: 0.16460588552232366 \t\t Validation Loss: 1.1882017850875854\n",
            "Epoch 53 \t\t Training Loss: 0.13537892894200923 \t\t Validation Loss: 1.2780243158340454\n",
            "Epoch 54 \t\t Training Loss: 0.17402788748440798 \t\t Validation Loss: 1.009984016418457\n",
            "Epoch 55 \t\t Training Loss: 0.15381401167018338 \t\t Validation Loss: 0.9184858798980713\n",
            "Epoch 56 \t\t Training Loss: 0.16077326751721557 \t\t Validation Loss: 1.2682150602340698\n",
            "Epoch 57 \t\t Training Loss: 0.15276903031073744 \t\t Validation Loss: 1.1674610376358032\n",
            "Epoch 58 \t\t Training Loss: 0.14242037151045225 \t\t Validation Loss: 1.652103066444397\n",
            "Epoch 59 \t\t Training Loss: 0.1572852540466556 \t\t Validation Loss: 1.6414049863815308\n",
            "Epoch 60 \t\t Training Loss: 0.23243859529757174 \t\t Validation Loss: 1.0658806562423706\n",
            "(0.0006234189278857652, 0.7591487169265747, 0.7507267441860465, tensor([[223.,  12.,  26.,  63.],\n",
            "        [ 18., 271.,   1.,   7.],\n",
            "        [ 49.,   3., 287.,  17.],\n",
            "        [115.,  15.,  17., 252.]]))\n"
          ]
        }
      ],
      "source": [
        "do = 0.5\n",
        "net2 = LeNetDrop(do).to(device)\n",
        "num_epochs = 60\n",
        "optimizer = optim.Adam\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 2e-3\n",
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, reproducibility=True)\n",
        "optimal_model = LeNetDrop(do).to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UOcs8T8-p6k",
        "outputId": "eb91239f-da67-43eb-8650-ef558a5c2ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training Loss: 1.38892962038517 \t\t Validation Loss: 1.387092113494873\n",
            "Epoch 2 \t\t Training Loss: 1.3869564533233643 \t\t Validation Loss: 1.3865749835968018\n",
            "Epoch 3 \t\t Training Loss: 1.386660338640213 \t\t Validation Loss: 1.3864895105361938\n",
            "Epoch 4 \t\t Training Loss: 1.3866508907079698 \t\t Validation Loss: 1.386462688446045\n",
            "Epoch 5 \t\t Training Loss: 1.3866078811883926 \t\t Validation Loss: 1.3865405321121216\n",
            "Epoch 6 \t\t Training Loss: 1.3866335201263427 \t\t Validation Loss: 1.3866324424743652\n",
            "Epoch 7 \t\t Training Loss: 1.3867101287841797 \t\t Validation Loss: 1.386509895324707\n",
            "Epoch 8 \t\t Training Loss: 1.386686868071556 \t\t Validation Loss: 1.3866510391235352\n",
            "Epoch 9 \t\t Training Loss: 1.3867610388994216 \t\t Validation Loss: 1.3865089416503906\n",
            "Epoch 10 \t\t Training Loss: 1.386669105887413 \t\t Validation Loss: 1.3865808248519897\n",
            "Epoch 11 \t\t Training Loss: 1.3866918116807938 \t\t Validation Loss: 1.3865526914596558\n",
            "Epoch 12 \t\t Training Loss: 1.3867379987239838 \t\t Validation Loss: 1.3865467309951782\n",
            "Epoch 13 \t\t Training Loss: 1.3866359668970107 \t\t Validation Loss: 1.386583685874939\n",
            "Epoch 14 \t\t Training Loss: 1.3866139823198318 \t\t Validation Loss: 1.3864997625350952\n",
            "Epoch 15 \t\t Training Loss: 1.3866000628471375 \t\t Validation Loss: 1.386664867401123\n",
            "Epoch 16 \t\t Training Loss: 1.3866933053731918 \t\t Validation Loss: 1.3865044116973877\n",
            "Epoch 17 \t\t Training Loss: 1.3866349136829377 \t\t Validation Loss: 1.3864874839782715\n",
            "Epoch 18 \t\t Training Loss: 1.3866386592388154 \t\t Validation Loss: 1.386541485786438\n",
            "Epoch 19 \t\t Training Loss: 1.3866433894634247 \t\t Validation Loss: 1.3865418434143066\n",
            "Epoch 20 \t\t Training Loss: 1.3866592293977738 \t\t Validation Loss: 1.3865313529968262\n",
            "Epoch 21 \t\t Training Loss: 1.386589944958687 \t\t Validation Loss: 1.386709451675415\n",
            "Epoch 22 \t\t Training Loss: 1.386599042415619 \t\t Validation Loss: 1.38649582862854\n",
            "Epoch 23 \t\t Training Loss: 1.3867295271158218 \t\t Validation Loss: 1.3865641355514526\n",
            "Epoch 24 \t\t Training Loss: 1.3867027819156648 \t\t Validation Loss: 1.3864496946334839\n",
            "Epoch 25 \t\t Training Loss: 1.386584158539772 \t\t Validation Loss: 1.3866143226623535\n",
            "Epoch 26 \t\t Training Loss: 1.3866487157344818 \t\t Validation Loss: 1.3864697217941284\n",
            "Epoch 27 \t\t Training Loss: 1.3865770763158798 \t\t Validation Loss: 1.3866256475448608\n",
            "Epoch 28 \t\t Training Loss: 1.3865925145149232 \t\t Validation Loss: 1.3865597248077393\n",
            "Epoch 29 \t\t Training Loss: 1.3867564034461974 \t\t Validation Loss: 1.3867623805999756\n",
            "Epoch 30 \t\t Training Loss: 1.3866031855344771 \t\t Validation Loss: 1.3865535259246826\n",
            "Epoch 31 \t\t Training Loss: 1.386603456735611 \t\t Validation Loss: 1.3864902257919312\n",
            "Epoch 32 \t\t Training Loss: 1.3865996938943863 \t\t Validation Loss: 1.3866660594940186\n",
            "Epoch 33 \t\t Training Loss: 1.38662872672081 \t\t Validation Loss: 1.386521339416504\n",
            "Epoch 34 \t\t Training Loss: 1.3866740131378175 \t\t Validation Loss: 1.3864706754684448\n",
            "Epoch 35 \t\t Training Loss: 1.3866386264562607 \t\t Validation Loss: 1.3864998817443848\n",
            "Epoch 36 \t\t Training Loss: 1.386745513677597 \t\t Validation Loss: 1.386616826057434\n",
            "Epoch 37 \t\t Training Loss: 1.3865700250864028 \t\t Validation Loss: 1.386644721031189\n",
            "Epoch 38 \t\t Training Loss: 1.3866232085227965 \t\t Validation Loss: 1.3865596055984497\n",
            "Epoch 39 \t\t Training Loss: 1.3866963070631027 \t\t Validation Loss: 1.3865966796875\n",
            "Epoch 40 \t\t Training Loss: 1.3866796177625655 \t\t Validation Loss: 1.3865630626678467\n",
            "Epoch 41 \t\t Training Loss: 1.3865878117084502 \t\t Validation Loss: 1.3864941596984863\n",
            "Epoch 42 \t\t Training Loss: 1.3866099852323532 \t\t Validation Loss: 1.3865447044372559\n",
            "Epoch 43 \t\t Training Loss: 1.3866996371746063 \t\t Validation Loss: 1.386536717414856\n",
            "Epoch 44 \t\t Training Loss: 1.3866208624839782 \t\t Validation Loss: 1.3864593505859375\n",
            "Epoch 45 \t\t Training Loss: 1.386637375354767 \t\t Validation Loss: 1.3865585327148438\n",
            "Epoch 46 \t\t Training Loss: 1.386805831193924 \t\t Validation Loss: 1.3866589069366455\n",
            "Epoch 47 \t\t Training Loss: 1.3867515712976455 \t\t Validation Loss: 1.3865355253219604\n",
            "Epoch 48 \t\t Training Loss: 1.3866133397817613 \t\t Validation Loss: 1.3866358995437622\n",
            "Epoch 49 \t\t Training Loss: 1.3865929186344146 \t\t Validation Loss: 1.3866328001022339\n",
            "Epoch 50 \t\t Training Loss: 1.3866567742824554 \t\t Validation Loss: 1.3865113258361816\n",
            "Epoch 51 \t\t Training Loss: 1.3866394871473313 \t\t Validation Loss: 1.3865031003952026\n",
            "Epoch 52 \t\t Training Loss: 1.3866077733039857 \t\t Validation Loss: 1.386563777923584\n",
            "Epoch 53 \t\t Training Loss: 1.3865898603200912 \t\t Validation Loss: 1.3864234685897827\n",
            "Epoch 54 \t\t Training Loss: 1.3866014993190765 \t\t Validation Loss: 1.3865220546722412\n",
            "Epoch 55 \t\t Training Loss: 1.3866808307170868 \t\t Validation Loss: 1.3864151239395142\n",
            "Epoch 56 \t\t Training Loss: 1.386618760228157 \t\t Validation Loss: 1.3865571022033691\n",
            "Epoch 57 \t\t Training Loss: 1.386522906422615 \t\t Validation Loss: 1.3864320516586304\n",
            "Epoch 58 \t\t Training Loss: 1.3866449737548827 \t\t Validation Loss: 1.386429786682129\n",
            "Epoch 59 \t\t Training Loss: 1.3867076152563096 \t\t Validation Loss: 1.3864349126815796\n",
            "Epoch 60 \t\t Training Loss: 1.3866533815860749 \t\t Validation Loss: 1.386508822441101\n",
            "(0.001007403138765069, 0.2489425241947174, 0.27180232558139533, tensor([[144.,  74.,  19.,  87.],\n",
            "        [138.,  76.,  26.,  57.],\n",
            "        [220.,  84.,  16.,  36.],\n",
            "        [ 90., 149.,  22., 138.]]))\n"
          ]
        }
      ],
      "source": [
        "do = 1.0\n",
        "net2 = LeNetDrop(do).to(device)\n",
        "num_epochs = 60\n",
        "optimizer = optim.Adam\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 2e-3\n",
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, reproducibility=True)\n",
        "optimal_model = LeNetDrop(do).to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiNeZEGU_hbW",
        "outputId": "f6a5d786-e6d4-4f2f-fd2e-37f69d590ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training Loss: 1.2335909938812255 \t\t Validation Loss: 1.3385474681854248\n",
            "Epoch 2 \t\t Training Loss: 1.1894449400901794 \t\t Validation Loss: 1.4474892616271973\n",
            "Epoch 3 \t\t Training Loss: 1.1453183922171593 \t\t Validation Loss: 1.1718875169754028\n",
            "Epoch 4 \t\t Training Loss: 1.1020587733387948 \t\t Validation Loss: 1.0539872646331787\n",
            "Epoch 5 \t\t Training Loss: 1.0652547317743302 \t\t Validation Loss: 0.9664693474769592\n",
            "Epoch 6 \t\t Training Loss: 1.0343206685781479 \t\t Validation Loss: 1.0430060625076294\n",
            "Epoch 7 \t\t Training Loss: 1.0098350906372071 \t\t Validation Loss: 0.9287013411521912\n",
            "Epoch 8 \t\t Training Loss: 1.0030429551005364 \t\t Validation Loss: 1.0003821849822998\n",
            "Epoch 9 \t\t Training Loss: 1.0298863199353219 \t\t Validation Loss: 0.9350055456161499\n",
            "Epoch 10 \t\t Training Loss: 0.9551158028841019 \t\t Validation Loss: 0.8740501999855042\n",
            "Epoch 11 \t\t Training Loss: 0.9528282302618026 \t\t Validation Loss: 0.9858938455581665\n",
            "Epoch 12 \t\t Training Loss: 0.9597271606326103 \t\t Validation Loss: 0.9364455938339233\n",
            "Epoch 13 \t\t Training Loss: 0.9520761013031006 \t\t Validation Loss: 0.9975590705871582\n",
            "Epoch 14 \t\t Training Loss: 0.9472233608365059 \t\t Validation Loss: 1.0363174676895142\n",
            "Epoch 15 \t\t Training Loss: 0.9401660799980164 \t\t Validation Loss: 1.240116000175476\n",
            "Epoch 16 \t\t Training Loss: 0.9388399139046669 \t\t Validation Loss: 0.9839720129966736\n",
            "Epoch 17 \t\t Training Loss: 0.9436582073569297 \t\t Validation Loss: 1.0074584484100342\n",
            "Epoch 18 \t\t Training Loss: 0.9513434678316116 \t\t Validation Loss: 1.0897709131240845\n",
            "Epoch 19 \t\t Training Loss: 0.9323188757896423 \t\t Validation Loss: 0.867575466632843\n",
            "Epoch 20 \t\t Training Loss: 0.9310653273761272 \t\t Validation Loss: 0.9261041283607483\n",
            "Epoch 21 \t\t Training Loss: 0.9313224464654922 \t\t Validation Loss: 1.0474332571029663\n",
            "Epoch 22 \t\t Training Loss: 0.9494122365117073 \t\t Validation Loss: 0.9917084574699402\n",
            "Epoch 23 \t\t Training Loss: 0.9305302540957928 \t\t Validation Loss: 0.932587206363678\n",
            "Epoch 24 \t\t Training Loss: 0.9343350671231747 \t\t Validation Loss: 0.8981151580810547\n",
            "Epoch 25 \t\t Training Loss: 0.9189559578895569 \t\t Validation Loss: 0.9013513326644897\n",
            "Epoch 26 \t\t Training Loss: 0.9119086402654648 \t\t Validation Loss: 0.84984290599823\n",
            "Epoch 27 \t\t Training Loss: 0.9996093013882636 \t\t Validation Loss: 0.9927784204483032\n",
            "Epoch 28 \t\t Training Loss: 0.9475449520349503 \t\t Validation Loss: 0.9254516363143921\n",
            "Epoch 29 \t\t Training Loss: 0.9388306930661201 \t\t Validation Loss: 0.931761622428894\n",
            "Epoch 30 \t\t Training Loss: 0.9276309391856193 \t\t Validation Loss: 0.9120557308197021\n",
            "Epoch 31 \t\t Training Loss: 0.8995296023786068 \t\t Validation Loss: 1.8435125350952148\n",
            "Epoch 32 \t\t Training Loss: 0.9489188948273659 \t\t Validation Loss: 0.9246957898139954\n",
            "Epoch 33 \t\t Training Loss: 0.9246053464710713 \t\t Validation Loss: 0.9278624653816223\n",
            "Epoch 34 \t\t Training Loss: 0.8966100761294364 \t\t Validation Loss: 0.9045889377593994\n",
            "Epoch 35 \t\t Training Loss: 0.9260278978943824 \t\t Validation Loss: 0.9251589775085449\n",
            "Epoch 36 \t\t Training Loss: 0.9118591919541359 \t\t Validation Loss: 0.9378889203071594\n",
            "Epoch 37 \t\t Training Loss: 0.9134445777535438 \t\t Validation Loss: 0.9693889021873474\n",
            "Epoch 38 \t\t Training Loss: 0.9105939519405365 \t\t Validation Loss: 0.8981252312660217\n",
            "Epoch 39 \t\t Training Loss: 0.9386235243082046 \t\t Validation Loss: 0.8553949594497681\n",
            "Epoch 40 \t\t Training Loss: 0.914846475571394 \t\t Validation Loss: 1.0314455032348633\n",
            "Epoch 41 \t\t Training Loss: 0.9192636430263519 \t\t Validation Loss: 0.8398623466491699\n",
            "Epoch 42 \t\t Training Loss: 0.9031585890054703 \t\t Validation Loss: 0.8983350396156311\n",
            "Epoch 43 \t\t Training Loss: 0.9027793926000595 \t\t Validation Loss: 0.9189150929450989\n",
            "Epoch 44 \t\t Training Loss: 0.8928417009115219 \t\t Validation Loss: 0.936722993850708\n",
            "Epoch 45 \t\t Training Loss: 0.9205013689398766 \t\t Validation Loss: 0.879314661026001\n",
            "Epoch 46 \t\t Training Loss: 0.9080248099565505 \t\t Validation Loss: 0.8982704877853394\n",
            "Epoch 47 \t\t Training Loss: 0.9381398499011994 \t\t Validation Loss: 0.9415356516838074\n",
            "Epoch 48 \t\t Training Loss: 0.9144965380430221 \t\t Validation Loss: 1.018476963043213\n",
            "Epoch 49 \t\t Training Loss: 0.9176329928636551 \t\t Validation Loss: 0.9779390692710876\n",
            "Epoch 50 \t\t Training Loss: 0.9004618340730667 \t\t Validation Loss: 0.8955851197242737\n",
            "Epoch 51 \t\t Training Loss: 0.8812035953998566 \t\t Validation Loss: 0.8463084697723389\n",
            "Epoch 52 \t\t Training Loss: 0.8818563710153103 \t\t Validation Loss: 0.9967176914215088\n",
            "Epoch 53 \t\t Training Loss: 0.9216413959860802 \t\t Validation Loss: 1.0699479579925537\n",
            "Epoch 54 \t\t Training Loss: 0.9152720995247364 \t\t Validation Loss: 0.9607596397399902\n",
            "Epoch 55 \t\t Training Loss: 0.8965005198121071 \t\t Validation Loss: 1.0034360885620117\n",
            "Epoch 56 \t\t Training Loss: 0.8984013918042183 \t\t Validation Loss: 0.8952163457870483\n",
            "Epoch 57 \t\t Training Loss: 0.9167856076359748 \t\t Validation Loss: 0.9224207997322083\n",
            "Epoch 58 \t\t Training Loss: 0.916561307311058 \t\t Validation Loss: 0.890322208404541\n",
            "Epoch 59 \t\t Training Loss: 0.920918462574482 \t\t Validation Loss: 0.8789975047111511\n",
            "Epoch 60 \t\t Training Loss: 0.9117003646492958 \t\t Validation Loss: 1.041894793510437\n",
            "(0.000705898544469545, 0.6455793380737305, 0.6453488372093024, tensor([[128.,  38.,  30., 128.],\n",
            "        [  0., 272.,   0.,  25.],\n",
            "        [ 98.,  15., 231.,  12.],\n",
            "        [ 90.,  40.,  12., 257.]]))\n"
          ]
        }
      ],
      "source": [
        "do = 0.5\n",
        "net2 = LeNetDrop(do).to(device)\n",
        "num_epochs = 60\n",
        "optimizer = optim.Adam\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 2e-3\n",
        "choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, reproducibility=True, wd=0.05)\n",
        "optimal_model = LeNetDrop(do).to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "print(test_nn(test_dataloader, optimal_model, loss_fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VcoETk1EHzA"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
        "    self.conv4 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n",
        "\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc1 = nn.Linear(1024, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 256)\n",
        "    self.fc3 = nn.Linear(256, 32)\n",
        "    self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.max_pool(F.relu(self.conv1(x)))\n",
        "    x = self.max_pool(F.relu(self.conv2(x)))\n",
        "    x = self.max_pool(F.relu(self.conv3(x)))\n",
        "    x = self.max_pool(F.relu(self.conv4(x)))\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = self.fc4(x)\n",
        "    return x\n",
        "\n",
        "net = LeNet().to(device)\n",
        "\n",
        "# define the corresponding loss function and the optimizer\n",
        "learning_rate = 1e-3\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam\n",
        "num_epochs = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g5d8iupBrWa",
        "outputId": "fd6f91c9-1752-435c-8441-71991c8bf82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch_size:  2 \t Performance:  0.3333333432674408 \t Time elapsed:  316.5172836780548\n",
            "Batch_size:  4 \t Performance:  0.0 \t Time elapsed:  159.71120071411133\n",
            "Batch_size:  8 \t Performance:  0.1818181872367859 \t Time elapsed:  83.73017811775208\n",
            "Batch_size:  16 \t Performance:  0.2988505959510803 \t Time elapsed:  42.598090171813965\n",
            "Batch_size:  32 \t Performance:  0.2380952388048172 \t Time elapsed:  23.94163155555725\n",
            "Batch_size:  64 \t Performance:  0.3962264060974121 \t Time elapsed:  16.616625547409058\n",
            "Batch_size:  128 \t Performance:  0.14179104566574097 \t Time elapsed:  13.11973261833191\n"
          ]
        }
      ],
      "source": [
        "for i in range(1,8):\n",
        "    batch_size = 2**i\n",
        "\n",
        "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    import time\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, reproducibility=True, show=False)\n",
        "    optimal_model = LeNetDrop(do).to(device)\n",
        "    optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "    _, apodosi, _, _ = test_nn(test_dataloader, optimal_model, loss_fn)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print('Batch_size: ', batch_size, '\\t Performance: ', apodosi, '\\t Time elapsed: ', end-start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS4S-LWUIeC-",
        "outputId": "b3289eb3-167b-4bf9-83df-0e5673dc8bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch_size:  2 \t Performance:  0.7868454456329346 \t Time elapsed:  286.3442385196686\n",
            "Batch_size:  4 \t Performance:  0.7703860998153687 \t Time elapsed:  148.2298481464386\n",
            "Batch_size:  8 \t Performance:  0.7686753273010254 \t Time elapsed:  78.86682510375977\n",
            "Batch_size:  16 \t Performance:  0.7488300800323486 \t Time elapsed:  40.925068855285645\n",
            "Batch_size:  32 \t Performance:  0.7826924324035645 \t Time elapsed:  22.096606969833374\n",
            "Batch_size:  64 \t Performance:  0.7376126646995544 \t Time elapsed:  15.96338701248169\n",
            "Batch_size:  128 \t Performance:  0.7904436588287354 \t Time elapsed:  13.009025812149048\n"
          ]
        }
      ],
      "source": [
        "for i in range(1,8):\n",
        "    batch_size = 2**i\n",
        "\n",
        "    train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_data, batch_size=800, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=1376, shuffle=False)\n",
        "\n",
        "    import time\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, reproducibility=True, show=False)\n",
        "    optimal_model = LeNetDrop(do).to(device)\n",
        "    optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "    _, apodosi, _, _ = test_nn(test_dataloader, optimal_model, loss_fn)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    print('Batch_size: ', batch_size, '\\t Performance: ', apodosi, '\\t Time elapsed: ', end-start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqifhEEpIVFB"
      },
      "source": [
        "### Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "EuvoP64NMeCT"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=800, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=1376, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "c_GlFQARQAxM"
      },
      "outputs": [],
      "source": [
        "def choose_model(num_epochs, optimizer, train, loss_fn, model, learning_rate, val, patience, reproducibility=False, show=True, lbfgs=False):\n",
        "    # set seed before create your model  \n",
        "    if reproducibility:\n",
        "        torch_seed(seed=0)\n",
        "    \n",
        "    # copy by value the model\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    optimizer = optimizer(model_copy.parameters(), lr=learning_rate)\n",
        "    f1_max = 0\n",
        "    iter = 0\n",
        "    import time\n",
        "\n",
        "    start = time.time()\n",
        "    for e in range(num_epochs):\n",
        "        model_copy.train()\n",
        "        for (data, label) in train:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                target = model_copy(data)\n",
        "                loss = loss_fn(target, label)\n",
        "                loss.backward()\n",
        "                return loss\n",
        "            if lbfgs:\n",
        "                optimizer.step(closure)\n",
        "                target = model_copy(data)\n",
        "            else:\n",
        "                # Compute prediction and loss\n",
        "                target = model_copy(data)\n",
        "                # loss_fn defined above to be  nn.CrossEntropyLoss()\n",
        "                loss = loss_fn(target, label)\n",
        "\n",
        "                # Backpropagation\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step() # updating the weights of neural network\n",
        "\n",
        "        train_f1 = f1_score(target, label, num_classes=4, average='macro')\n",
        "\n",
        "        model_copy.eval()\n",
        "        for (X, y) in val:\n",
        "\n",
        "            # if using gpu dont forget to move the data there\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Compute prediction and loss\n",
        "            pred = model_copy(X)\n",
        "\n",
        "        val_f1 = f1_score(pred, y, num_classes=4, average='macro')\n",
        "\n",
        "        if show:\n",
        "            print(f'Epoch {e+1} \\t\\t Training f1: {train_f1} \\t\\t Validation f1: {val_f1}', end='')\n",
        "            if (reproducibility):\n",
        "                print(f'\\t\\t Loss: {loss}')\n",
        "            else:\n",
        "                print()\n",
        "\n",
        "\n",
        "        if f1_max < val_f1:\n",
        "            if show:\n",
        "                print(f'Validation f1 Increased({f1_max:.6f}--->{val_f1:.6f}) \\t Saving The Model')\n",
        "            f1_max = val_f1\n",
        "            # Saving State Dict\n",
        "            torch.save(model_copy.state_dict(), 'optimal.pth')\n",
        "            iter = 0\n",
        "        else:\n",
        "            iter += 1\n",
        "        \n",
        "        if iter == patience:\n",
        "            end = time.time()\n",
        "            return end-start\n",
        "    torch.save(model_copy.state_dict(), 'last.pth')\n",
        "    end = time.time()\n",
        "    if show:\n",
        "        print('-----------------------------------------------------')\n",
        "    return end-start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "hPAr_eSlkIty"
      },
      "outputs": [],
      "source": [
        "net2 = LeNet1().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ElvRZwRYBt"
      },
      "source": [
        "Patience = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1NjbllsPc9r",
        "outputId": "5c82d871-672f-49e6-9caa-03c48534163d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training f1: 0.2874999940395355 \t\t Validation f1: 0.26359012722969055\t\t Loss: 1.360917568206787\n",
            "Validation f1 Increased(0.000000--->0.263590) \t Saving The Model\n",
            "Epoch 2 \t\t Training f1: 0.22499999403953552 \t\t Validation f1: 0.32869386672973633\t\t Loss: 1.3440680503845215\n",
            "Validation f1 Increased(0.263590--->0.328694) \t Saving The Model\n",
            "Epoch 3 \t\t Training f1: 0.4880952835083008 \t\t Validation f1: 0.4309425950050354\t\t Loss: 1.2898279428482056\n",
            "Validation f1 Increased(0.328694--->0.430943) \t Saving The Model\n",
            "Epoch 4 \t\t Training f1: 0.380952388048172 \t\t Validation f1: 0.49946802854537964\t\t Loss: 1.2791883945465088\n",
            "Validation f1 Increased(0.430943--->0.499468) \t Saving The Model\n",
            "Epoch 5 \t\t Training f1: 0.5270562767982483 \t\t Validation f1: 0.5497586727142334\t\t Loss: 1.0769343376159668\n",
            "Validation f1 Increased(0.499468--->0.549759) \t Saving The Model\n",
            "Epoch 6 \t\t Training f1: 0.3888888955116272 \t\t Validation f1: 0.5910330414772034\t\t Loss: 1.1933702230453491\n",
            "Validation f1 Increased(0.549759--->0.591033) \t Saving The Model\n",
            "Epoch 7 \t\t Training f1: 0.7484848499298096 \t\t Validation f1: 0.6140176057815552\t\t Loss: 0.9028472900390625\n",
            "Validation f1 Increased(0.591033--->0.614018) \t Saving The Model\n",
            "Epoch 8 \t\t Training f1: 0.4435897469520569 \t\t Validation f1: 0.6567913293838501\t\t Loss: 1.1427743434906006\n",
            "Validation f1 Increased(0.614018--->0.656791) \t Saving The Model\n",
            "Epoch 9 \t\t Training f1: 0.5129870176315308 \t\t Validation f1: 0.6653271913528442\t\t Loss: 0.7854759693145752\n",
            "Validation f1 Increased(0.656791--->0.665327) \t Saving The Model\n",
            "Epoch 10 \t\t Training f1: 0.7665584683418274 \t\t Validation f1: 0.6973016858100891\t\t Loss: 0.7251682281494141\n",
            "Validation f1 Increased(0.665327--->0.697302) \t Saving The Model\n",
            "Epoch 11 \t\t Training f1: 0.8611111640930176 \t\t Validation f1: 0.7258011102676392\t\t Loss: 0.48277929425239563\n",
            "Validation f1 Increased(0.697302--->0.725801) \t Saving The Model\n",
            "Epoch 12 \t\t Training f1: 0.6484848856925964 \t\t Validation f1: 0.7287673950195312\t\t Loss: 0.7209840416908264\n",
            "Validation f1 Increased(0.725801--->0.728767) \t Saving The Model\n",
            "Epoch 13 \t\t Training f1: 0.5068181753158569 \t\t Validation f1: 0.7515376806259155\t\t Loss: 0.918716311454773\n",
            "Validation f1 Increased(0.728767--->0.751538) \t Saving The Model\n",
            "Epoch 14 \t\t Training f1: 0.5448773503303528 \t\t Validation f1: 0.7498992681503296\t\t Loss: 0.8317070007324219\n",
            "Epoch 15 \t\t Training f1: 0.8101190328598022 \t\t Validation f1: 0.7745910882949829\t\t Loss: 0.6791285276412964\n",
            "Validation f1 Increased(0.751538--->0.774591) \t Saving The Model\n",
            "Epoch 16 \t\t Training f1: 0.6855922341346741 \t\t Validation f1: 0.7854255437850952\t\t Loss: 0.5852464437484741\n",
            "Validation f1 Increased(0.774591--->0.785426) \t Saving The Model\n",
            "Epoch 17 \t\t Training f1: 0.6319444179534912 \t\t Validation f1: 0.7806109189987183\t\t Loss: 0.7455697655677795\n",
            "Epoch 18 \t\t Training f1: 0.7666666507720947 \t\t Validation f1: 0.7765315175056458\t\t Loss: 0.4744921028614044\n",
            "Epoch 19 \t\t Training f1: 0.9222222566604614 \t\t Validation f1: 0.7875152826309204\t\t Loss: 0.3106454014778137\n",
            "Validation f1 Increased(0.785426--->0.787515) \t Saving The Model\n",
            "Epoch 20 \t\t Training f1: 0.8440934419631958 \t\t Validation f1: 0.7952306270599365\t\t Loss: 0.47459858655929565\n",
            "Validation f1 Increased(0.787515--->0.795231) \t Saving The Model\n",
            "Epoch 21 \t\t Training f1: 0.8974359035491943 \t\t Validation f1: 0.7996448874473572\t\t Loss: 0.30826252698898315\n",
            "Validation f1 Increased(0.795231--->0.799645) \t Saving The Model\n",
            "Epoch 22 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.7573345899581909\t\t Loss: 0.25361716747283936\n",
            "Epoch 23 \t\t Training f1: 0.6767677068710327 \t\t Validation f1: 0.784722089767456\t\t Loss: 0.2763324975967407\n",
            "Epoch 24 \t\t Training f1: 0.8279914855957031 \t\t Validation f1: 0.7748688459396362\t\t Loss: 0.44712716341018677\n",
            "Epoch 25 \t\t Training f1: 0.8127706050872803 \t\t Validation f1: 0.8021722435951233\t\t Loss: 0.5612426400184631\n",
            "Validation f1 Increased(0.799645--->0.802172) \t Saving The Model\n",
            "Epoch 26 \t\t Training f1: 0.8452380895614624 \t\t Validation f1: 0.7863485813140869\t\t Loss: 0.37088438868522644\n",
            "Epoch 27 \t\t Training f1: 0.944444477558136 \t\t Validation f1: 0.8027845621109009\t\t Loss: 0.2530975937843323\n",
            "Validation f1 Increased(0.802172--->0.802785) \t Saving The Model\n",
            "Epoch 28 \t\t Training f1: 0.9222222566604614 \t\t Validation f1: 0.7857680916786194\t\t Loss: 0.25084495544433594\n",
            "Epoch 29 \t\t Training f1: 0.8517857193946838 \t\t Validation f1: 0.7553337216377258\t\t Loss: 0.32541385293006897\n",
            "Epoch 30 \t\t Training f1: 1.0 \t\t Validation f1: 0.7204158902168274\t\t Loss: 0.18021543323993683\n",
            "Epoch 31 \t\t Training f1: 1.0 \t\t Validation f1: 0.8045469522476196\t\t Loss: 0.10032087564468384\n",
            "Validation f1 Increased(0.802785--->0.804547) \t Saving The Model\n",
            "Epoch 32 \t\t Training f1: 0.8865079879760742 \t\t Validation f1: 0.6567857265472412\t\t Loss: 0.4547354280948639\n",
            "Epoch 33 \t\t Training f1: 0.85317462682724 \t\t Validation f1: 0.6769362092018127\t\t Loss: 0.2929576635360718\n",
            "Epoch 34 \t\t Training f1: 0.7785087823867798 \t\t Validation f1: 0.7627564668655396\t\t Loss: 0.2598545253276825\n",
            "Epoch 35 \t\t Training f1: 0.9494949579238892 \t\t Validation f1: 0.7995098233222961\t\t Loss: 0.20172393321990967\n",
            "Epoch 36 \t\t Training f1: 1.0 \t\t Validation f1: 0.7835533022880554\t\t Loss: 0.06393943727016449\n",
            "\n",
            " Performance:  0.764070987701416 \t Time elapsed:  37.25425386428833\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 60\n",
        "time = choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, patience=5, reproducibility=True)\n",
        "optimal_model = LeNet1().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "_, apodosi, _, _ = test_nn(test_dataloader, optimal_model, loss_fn)\n",
        "print('\\n Performance: ', apodosi, '\\t Time elapsed: ', time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sAGeywTRbJ5"
      },
      "source": [
        "Patience = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX5alttMRSTL",
        "outputId": "8a964fcd-25b1-4c9d-fb00-e5649d8f526e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training f1: 0.2874999940395355 \t\t Validation f1: 0.26359012722969055\t\t Loss: 1.360917568206787\n",
            "Validation f1 Increased(0.000000--->0.263590) \t Saving The Model\n",
            "Epoch 2 \t\t Training f1: 0.22499999403953552 \t\t Validation f1: 0.32869386672973633\t\t Loss: 1.3440680503845215\n",
            "Validation f1 Increased(0.263590--->0.328694) \t Saving The Model\n",
            "Epoch 3 \t\t Training f1: 0.4880952835083008 \t\t Validation f1: 0.4309425950050354\t\t Loss: 1.2898279428482056\n",
            "Validation f1 Increased(0.328694--->0.430943) \t Saving The Model\n",
            "Epoch 4 \t\t Training f1: 0.380952388048172 \t\t Validation f1: 0.49946802854537964\t\t Loss: 1.2791883945465088\n",
            "Validation f1 Increased(0.430943--->0.499468) \t Saving The Model\n",
            "Epoch 5 \t\t Training f1: 0.5270562767982483 \t\t Validation f1: 0.5497586727142334\t\t Loss: 1.0769343376159668\n",
            "Validation f1 Increased(0.499468--->0.549759) \t Saving The Model\n",
            "Epoch 6 \t\t Training f1: 0.3888888955116272 \t\t Validation f1: 0.5910330414772034\t\t Loss: 1.1933702230453491\n",
            "Validation f1 Increased(0.549759--->0.591033) \t Saving The Model\n",
            "Epoch 7 \t\t Training f1: 0.7484848499298096 \t\t Validation f1: 0.6140176057815552\t\t Loss: 0.9028472900390625\n",
            "Validation f1 Increased(0.591033--->0.614018) \t Saving The Model\n",
            "Epoch 8 \t\t Training f1: 0.4435897469520569 \t\t Validation f1: 0.6567913293838501\t\t Loss: 1.1427743434906006\n",
            "Validation f1 Increased(0.614018--->0.656791) \t Saving The Model\n",
            "Epoch 9 \t\t Training f1: 0.5129870176315308 \t\t Validation f1: 0.6653271913528442\t\t Loss: 0.7854759693145752\n",
            "Validation f1 Increased(0.656791--->0.665327) \t Saving The Model\n",
            "Epoch 10 \t\t Training f1: 0.7665584683418274 \t\t Validation f1: 0.6973016858100891\t\t Loss: 0.7251682281494141\n",
            "Validation f1 Increased(0.665327--->0.697302) \t Saving The Model\n",
            "Epoch 11 \t\t Training f1: 0.8611111640930176 \t\t Validation f1: 0.7258011102676392\t\t Loss: 0.48277929425239563\n",
            "Validation f1 Increased(0.697302--->0.725801) \t Saving The Model\n",
            "Epoch 12 \t\t Training f1: 0.6484848856925964 \t\t Validation f1: 0.7287673950195312\t\t Loss: 0.7209840416908264\n",
            "Validation f1 Increased(0.725801--->0.728767) \t Saving The Model\n",
            "Epoch 13 \t\t Training f1: 0.5068181753158569 \t\t Validation f1: 0.7515376806259155\t\t Loss: 0.918716311454773\n",
            "Validation f1 Increased(0.728767--->0.751538) \t Saving The Model\n",
            "Epoch 14 \t\t Training f1: 0.5448773503303528 \t\t Validation f1: 0.7498992681503296\t\t Loss: 0.8317070007324219\n",
            "Epoch 15 \t\t Training f1: 0.8101190328598022 \t\t Validation f1: 0.7745910882949829\t\t Loss: 0.6791285276412964\n",
            "Validation f1 Increased(0.751538--->0.774591) \t Saving The Model\n",
            "Epoch 16 \t\t Training f1: 0.6855922341346741 \t\t Validation f1: 0.7854255437850952\t\t Loss: 0.5852464437484741\n",
            "Validation f1 Increased(0.774591--->0.785426) \t Saving The Model\n",
            "Epoch 17 \t\t Training f1: 0.6319444179534912 \t\t Validation f1: 0.7806109189987183\t\t Loss: 0.7455697655677795\n",
            "Epoch 18 \t\t Training f1: 0.7666666507720947 \t\t Validation f1: 0.7765315175056458\t\t Loss: 0.4744921028614044\n",
            "Epoch 19 \t\t Training f1: 0.9222222566604614 \t\t Validation f1: 0.7875152826309204\t\t Loss: 0.3106454014778137\n",
            "Validation f1 Increased(0.785426--->0.787515) \t Saving The Model\n",
            "Epoch 20 \t\t Training f1: 0.8440934419631958 \t\t Validation f1: 0.7952306270599365\t\t Loss: 0.47459858655929565\n",
            "Validation f1 Increased(0.787515--->0.795231) \t Saving The Model\n",
            "Epoch 21 \t\t Training f1: 0.8974359035491943 \t\t Validation f1: 0.7996448874473572\t\t Loss: 0.30826252698898315\n",
            "Validation f1 Increased(0.795231--->0.799645) \t Saving The Model\n",
            "Epoch 22 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.7573345899581909\t\t Loss: 0.25361716747283936\n",
            "Epoch 23 \t\t Training f1: 0.6767677068710327 \t\t Validation f1: 0.784722089767456\t\t Loss: 0.2763324975967407\n",
            "Epoch 24 \t\t Training f1: 0.8279914855957031 \t\t Validation f1: 0.7748688459396362\t\t Loss: 0.44712716341018677\n",
            "Epoch 25 \t\t Training f1: 0.8127706050872803 \t\t Validation f1: 0.8021722435951233\t\t Loss: 0.5612426400184631\n",
            "Validation f1 Increased(0.799645--->0.802172) \t Saving The Model\n",
            "Epoch 26 \t\t Training f1: 0.8452380895614624 \t\t Validation f1: 0.7863485813140869\t\t Loss: 0.37088438868522644\n",
            "Epoch 27 \t\t Training f1: 0.944444477558136 \t\t Validation f1: 0.8027845621109009\t\t Loss: 0.2530975937843323\n",
            "Validation f1 Increased(0.802172--->0.802785) \t Saving The Model\n",
            "Epoch 28 \t\t Training f1: 0.9222222566604614 \t\t Validation f1: 0.7857680916786194\t\t Loss: 0.25084495544433594\n",
            "Epoch 29 \t\t Training f1: 0.8517857193946838 \t\t Validation f1: 0.7553337216377258\t\t Loss: 0.32541385293006897\n",
            "Epoch 30 \t\t Training f1: 1.0 \t\t Validation f1: 0.7204158902168274\t\t Loss: 0.18021543323993683\n",
            "Epoch 31 \t\t Training f1: 1.0 \t\t Validation f1: 0.8045469522476196\t\t Loss: 0.10032087564468384\n",
            "Validation f1 Increased(0.802785--->0.804547) \t Saving The Model\n",
            "Epoch 32 \t\t Training f1: 0.8865079879760742 \t\t Validation f1: 0.6567857265472412\t\t Loss: 0.4547354280948639\n",
            "Epoch 33 \t\t Training f1: 0.85317462682724 \t\t Validation f1: 0.6769362092018127\t\t Loss: 0.2929576635360718\n",
            "Epoch 34 \t\t Training f1: 0.7785087823867798 \t\t Validation f1: 0.7627564668655396\t\t Loss: 0.2598545253276825\n",
            "Epoch 35 \t\t Training f1: 0.9494949579238892 \t\t Validation f1: 0.7995098233222961\t\t Loss: 0.20172393321990967\n",
            "Epoch 36 \t\t Training f1: 1.0 \t\t Validation f1: 0.7835533022880554\t\t Loss: 0.06393943727016449\n",
            "Epoch 37 \t\t Training f1: 1.0 \t\t Validation f1: 0.7961419224739075\t\t Loss: 0.08355396240949631\n",
            "Epoch 38 \t\t Training f1: 1.0 \t\t Validation f1: 0.7599027752876282\t\t Loss: 0.08220293372869492\n",
            "Epoch 39 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.7927092909812927\t\t Loss: 0.11014730483293533\n",
            "\n",
            " Performance:  0.764070987701416 \t Time elapsed:  37.624778270721436\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 60\n",
        "time = choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, patience=8, reproducibility=True)\n",
        "optimal_model = LeNet1().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "_, apodosi, _, _ = test_nn(test_dataloader, optimal_model, loss_fn)\n",
        "print('\\n Performance: ', apodosi, '\\t Time elapsed: ', time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2VQiIO3Rimn"
      },
      "source": [
        "Patience = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCPDseLrRg7a",
        "outputId": "22c8b0e9-9aa4-4aff-bddd-dcf5dbe451a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training f1: 0.2874999940395355 \t\t Validation f1: 0.26359012722969055\t\t Loss: 1.360917568206787\n",
            "Validation f1 Increased(0.000000--->0.263590) \t Saving The Model\n",
            "Epoch 2 \t\t Training f1: 0.22499999403953552 \t\t Validation f1: 0.32869386672973633\t\t Loss: 1.3440680503845215\n",
            "Validation f1 Increased(0.263590--->0.328694) \t Saving The Model\n",
            "Epoch 3 \t\t Training f1: 0.4880952835083008 \t\t Validation f1: 0.4309425950050354\t\t Loss: 1.2898279428482056\n",
            "Validation f1 Increased(0.328694--->0.430943) \t Saving The Model\n",
            "Epoch 4 \t\t Training f1: 0.380952388048172 \t\t Validation f1: 0.49946802854537964\t\t Loss: 1.2791883945465088\n",
            "Validation f1 Increased(0.430943--->0.499468) \t Saving The Model\n",
            "Epoch 5 \t\t Training f1: 0.5270562767982483 \t\t Validation f1: 0.5497586727142334\t\t Loss: 1.0769343376159668\n",
            "Validation f1 Increased(0.499468--->0.549759) \t Saving The Model\n",
            "Epoch 6 \t\t Training f1: 0.3888888955116272 \t\t Validation f1: 0.5910330414772034\t\t Loss: 1.1933702230453491\n",
            "Validation f1 Increased(0.549759--->0.591033) \t Saving The Model\n",
            "Epoch 7 \t\t Training f1: 0.7484848499298096 \t\t Validation f1: 0.6140176057815552\t\t Loss: 0.9028472900390625\n",
            "Validation f1 Increased(0.591033--->0.614018) \t Saving The Model\n",
            "Epoch 8 \t\t Training f1: 0.4435897469520569 \t\t Validation f1: 0.6567913293838501\t\t Loss: 1.1427743434906006\n",
            "Validation f1 Increased(0.614018--->0.656791) \t Saving The Model\n",
            "Epoch 9 \t\t Training f1: 0.5129870176315308 \t\t Validation f1: 0.6653271913528442\t\t Loss: 0.7854759693145752\n",
            "Validation f1 Increased(0.656791--->0.665327) \t Saving The Model\n",
            "Epoch 10 \t\t Training f1: 0.7665584683418274 \t\t Validation f1: 0.6973016858100891\t\t Loss: 0.7251682281494141\n",
            "Validation f1 Increased(0.665327--->0.697302) \t Saving The Model\n",
            "Epoch 11 \t\t Training f1: 0.8611111640930176 \t\t Validation f1: 0.7258011102676392\t\t Loss: 0.48277929425239563\n",
            "Validation f1 Increased(0.697302--->0.725801) \t Saving The Model\n",
            "Epoch 12 \t\t Training f1: 0.6484848856925964 \t\t Validation f1: 0.7287673950195312\t\t Loss: 0.7209840416908264\n",
            "Validation f1 Increased(0.725801--->0.728767) \t Saving The Model\n",
            "Epoch 13 \t\t Training f1: 0.5068181753158569 \t\t Validation f1: 0.7515376806259155\t\t Loss: 0.918716311454773\n",
            "Validation f1 Increased(0.728767--->0.751538) \t Saving The Model\n",
            "Epoch 14 \t\t Training f1: 0.5448773503303528 \t\t Validation f1: 0.7498992681503296\t\t Loss: 0.8317070007324219\n",
            "Epoch 15 \t\t Training f1: 0.8101190328598022 \t\t Validation f1: 0.7745910882949829\t\t Loss: 0.6791285276412964\n",
            "Validation f1 Increased(0.751538--->0.774591) \t Saving The Model\n",
            "Epoch 16 \t\t Training f1: 0.6855922341346741 \t\t Validation f1: 0.7854255437850952\t\t Loss: 0.5852464437484741\n",
            "Validation f1 Increased(0.774591--->0.785426) \t Saving The Model\n",
            "Epoch 17 \t\t Training f1: 0.6319444179534912 \t\t Validation f1: 0.7806109189987183\t\t Loss: 0.7455697655677795\n",
            "Epoch 18 \t\t Training f1: 0.7666666507720947 \t\t Validation f1: 0.7765315175056458\t\t Loss: 0.4744921028614044\n",
            "Epoch 19 \t\t Training f1: 0.9222222566604614 \t\t Validation f1: 0.7875152826309204\t\t Loss: 0.3106454014778137\n",
            "Validation f1 Increased(0.785426--->0.787515) \t Saving The Model\n",
            "Epoch 20 \t\t Training f1: 0.8440934419631958 \t\t Validation f1: 0.7952306270599365\t\t Loss: 0.47459858655929565\n",
            "Validation f1 Increased(0.787515--->0.795231) \t Saving The Model\n",
            "Epoch 21 \t\t Training f1: 0.8974359035491943 \t\t Validation f1: 0.7996448874473572\t\t Loss: 0.30826252698898315\n",
            "Validation f1 Increased(0.795231--->0.799645) \t Saving The Model\n",
            "Epoch 22 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.7573345899581909\t\t Loss: 0.25361716747283936\n",
            "Epoch 23 \t\t Training f1: 0.6767677068710327 \t\t Validation f1: 0.784722089767456\t\t Loss: 0.2763324975967407\n",
            "Epoch 24 \t\t Training f1: 0.8279914855957031 \t\t Validation f1: 0.7748688459396362\t\t Loss: 0.44712716341018677\n",
            "Epoch 25 \t\t Training f1: 0.8127706050872803 \t\t Validation f1: 0.8021722435951233\t\t Loss: 0.5612426400184631\n",
            "Validation f1 Increased(0.799645--->0.802172) \t Saving The Model\n",
            "Epoch 26 \t\t Training f1: 0.8452380895614624 \t\t Validation f1: 0.7863485813140869\t\t Loss: 0.37088438868522644\n",
            "Epoch 27 \t\t Training f1: 0.944444477558136 \t\t Validation f1: 0.8027845621109009\t\t Loss: 0.2530975937843323\n",
            "Validation f1 Increased(0.802172--->0.802785) \t Saving The Model\n",
            "Epoch 28 \t\t Training f1: 0.9222222566604614 \t\t Validation f1: 0.7857680916786194\t\t Loss: 0.25084495544433594\n",
            "Epoch 29 \t\t Training f1: 0.8517857193946838 \t\t Validation f1: 0.7553337216377258\t\t Loss: 0.32541385293006897\n",
            "Epoch 30 \t\t Training f1: 1.0 \t\t Validation f1: 0.7204158902168274\t\t Loss: 0.18021543323993683\n",
            "Epoch 31 \t\t Training f1: 1.0 \t\t Validation f1: 0.8045469522476196\t\t Loss: 0.10032087564468384\n",
            "Validation f1 Increased(0.802785--->0.804547) \t Saving The Model\n",
            "Epoch 32 \t\t Training f1: 0.8865079879760742 \t\t Validation f1: 0.6567857265472412\t\t Loss: 0.4547354280948639\n",
            "Epoch 33 \t\t Training f1: 0.85317462682724 \t\t Validation f1: 0.6769362092018127\t\t Loss: 0.2929576635360718\n",
            "Epoch 34 \t\t Training f1: 0.7785087823867798 \t\t Validation f1: 0.7627564668655396\t\t Loss: 0.2598545253276825\n",
            "Epoch 35 \t\t Training f1: 0.9494949579238892 \t\t Validation f1: 0.7995098233222961\t\t Loss: 0.20172393321990967\n",
            "Epoch 36 \t\t Training f1: 1.0 \t\t Validation f1: 0.7835533022880554\t\t Loss: 0.06393943727016449\n",
            "Epoch 37 \t\t Training f1: 1.0 \t\t Validation f1: 0.7961419224739075\t\t Loss: 0.08355396240949631\n",
            "Epoch 38 \t\t Training f1: 1.0 \t\t Validation f1: 0.7599027752876282\t\t Loss: 0.08220293372869492\n",
            "Epoch 39 \t\t Training f1: 0.9365079402923584 \t\t Validation f1: 0.7927092909812927\t\t Loss: 0.11014730483293533\n",
            "Epoch 40 \t\t Training f1: 0.9307692050933838 \t\t Validation f1: 0.7009004950523376\t\t Loss: 0.16249458491802216\n",
            "Epoch 41 \t\t Training f1: 1.0 \t\t Validation f1: 0.7812616229057312\t\t Loss: 0.07795479893684387\n",
            "Epoch 42 \t\t Training f1: 1.0 \t\t Validation f1: 0.7743085026741028\t\t Loss: 0.06366018205881119\n",
            "Epoch 43 \t\t Training f1: 1.0 \t\t Validation f1: 0.7894840240478516\t\t Loss: 0.10174720734357834\n",
            "Epoch 44 \t\t Training f1: 1.0 \t\t Validation f1: 0.7909612655639648\t\t Loss: 0.04117460921406746\n",
            "Epoch 45 \t\t Training f1: 1.0 \t\t Validation f1: 0.7601165175437927\t\t Loss: 0.033569544553756714\n",
            "Epoch 46 \t\t Training f1: 1.0 \t\t Validation f1: 0.8088158369064331\t\t Loss: 0.015447013080120087\n",
            "Validation f1 Increased(0.804547--->0.808816) \t Saving The Model\n",
            "Epoch 47 \t\t Training f1: 1.0 \t\t Validation f1: 0.7812912464141846\t\t Loss: 0.013507508672773838\n",
            "Epoch 48 \t\t Training f1: 1.0 \t\t Validation f1: 0.7833083868026733\t\t Loss: 0.0281685758382082\n",
            "Epoch 49 \t\t Training f1: 1.0 \t\t Validation f1: 0.7461555004119873\t\t Loss: 0.01139027252793312\n",
            "Epoch 50 \t\t Training f1: 1.0 \t\t Validation f1: 0.7808631658554077\t\t Loss: 0.013013914227485657\n",
            "Epoch 51 \t\t Training f1: 1.0 \t\t Validation f1: 0.7729518413543701\t\t Loss: 0.04061232507228851\n",
            "Epoch 52 \t\t Training f1: 1.0 \t\t Validation f1: 0.7889200448989868\t\t Loss: 0.043613750487565994\n",
            "Epoch 53 \t\t Training f1: 1.0 \t\t Validation f1: 0.7890982627868652\t\t Loss: 0.009015326388180256\n",
            "Epoch 54 \t\t Training f1: 1.0 \t\t Validation f1: 0.758470892906189\t\t Loss: 0.02215982973575592\n",
            "Epoch 55 \t\t Training f1: 1.0 \t\t Validation f1: 0.7927511930465698\t\t Loss: 0.016277791932225227\n",
            "Epoch 56 \t\t Training f1: 1.0 \t\t Validation f1: 0.7842631340026855\t\t Loss: 0.00431501679122448\n",
            "Epoch 57 \t\t Training f1: 1.0 \t\t Validation f1: 0.778154730796814\t\t Loss: 0.018722623586654663\n",
            "Epoch 58 \t\t Training f1: 1.0 \t\t Validation f1: 0.791757345199585\t\t Loss: 0.0067556011490523815\n",
            "Epoch 59 \t\t Training f1: 1.0 \t\t Validation f1: 0.7872302532196045\t\t Loss: 0.006382491905242205\n",
            "Epoch 60 \t\t Training f1: 1.0 \t\t Validation f1: 0.774796724319458\t\t Loss: 0.015050339512526989\n",
            "-----------------------------------------------------\n",
            "\n",
            " Performance:  0.7699640989303589 \t Time elapsed:  58.777973651885986\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 60\n",
        "time = choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, patience=15, reproducibility=True)\n",
        "optimal_model = LeNet1().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "_, apodosi, _, _ = test_nn(test_dataloader, optimal_model, loss_fn)\n",
        "print('\\n Performance: ', apodosi, '\\t Time elapsed: ', time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyt1P1HoR6Y-"
      },
      "source": [
        "Patience = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btpoepMZRyS_",
        "outputId": "ef3cbd58-cdd6-4880-ed54-b7c817699691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \t\t Training f1: 0.2874999940395355 \t\t Validation f1: 0.26359012722969055\t\t Loss: 1.360917568206787\n",
            "Validation f1 Increased(0.000000--->0.263590) \t Saving The Model\n",
            "Epoch 2 \t\t Training f1: 0.22499999403953552 \t\t Validation f1: 0.32869386672973633\t\t Loss: 1.3440680503845215\n",
            "Validation f1 Increased(0.263590--->0.328694) \t Saving The Model\n",
            "Epoch 3 \t\t Training f1: 0.4880952835083008 \t\t Validation f1: 0.4309425950050354\t\t Loss: 1.2898279428482056\n",
            "Validation f1 Increased(0.328694--->0.430943) \t Saving The Model\n",
            "Epoch 4 \t\t Training f1: 0.380952388048172 \t\t Validation f1: 0.49946802854537964\t\t Loss: 1.2791883945465088\n",
            "Validation f1 Increased(0.430943--->0.499468) \t Saving The Model\n",
            "Epoch 5 \t\t Training f1: 0.5270562767982483 \t\t Validation f1: 0.5497586727142334\t\t Loss: 1.0769343376159668\n",
            "Validation f1 Increased(0.499468--->0.549759) \t Saving The Model\n",
            "Epoch 6 \t\t Training f1: 0.3888888955116272 \t\t Validation f1: 0.5910330414772034\t\t Loss: 1.1933702230453491\n",
            "Validation f1 Increased(0.549759--->0.591033) \t Saving The Model\n",
            "Epoch 7 \t\t Training f1: 0.7484848499298096 \t\t Validation f1: 0.6140176057815552\t\t Loss: 0.9028472900390625\n",
            "Validation f1 Increased(0.591033--->0.614018) \t Saving The Model\n",
            "Epoch 8 \t\t Training f1: 0.4435897469520569 \t\t Validation f1: 0.6567913293838501\t\t Loss: 1.1427743434906006\n",
            "Validation f1 Increased(0.614018--->0.656791) \t Saving The Model\n",
            "Epoch 9 \t\t Training f1: 0.5129870176315308 \t\t Validation f1: 0.6653271913528442\t\t Loss: 0.7854759693145752\n",
            "Validation f1 Increased(0.656791--->0.665327) \t Saving The Model\n",
            "Epoch 10 \t\t Training f1: 0.7665584683418274 \t\t Validation f1: 0.6973016858100891\t\t Loss: 0.7251682281494141\n",
            "Validation f1 Increased(0.665327--->0.697302) \t Saving The Model\n",
            "Epoch 11 \t\t Training f1: 0.8611111640930176 \t\t Validation f1: 0.7258011102676392\t\t Loss: 0.48277929425239563\n",
            "Validation f1 Increased(0.697302--->0.725801) \t Saving The Model\n",
            "Epoch 12 \t\t Training f1: 0.6484848856925964 \t\t Validation f1: 0.7287673950195312\t\t Loss: 0.7209840416908264\n",
            "Validation f1 Increased(0.725801--->0.728767) \t Saving The Model\n",
            "Epoch 13 \t\t Training f1: 0.5068181753158569 \t\t Validation f1: 0.7515376806259155\t\t Loss: 0.918716311454773\n",
            "Validation f1 Increased(0.728767--->0.751538) \t Saving The Model\n",
            "Epoch 14 \t\t Training f1: 0.5448773503303528 \t\t Validation f1: 0.7498992681503296\t\t Loss: 0.8317070007324219\n",
            "Epoch 15 \t\t Training f1: 0.8101190328598022 \t\t Validation f1: 0.7745910882949829\t\t Loss: 0.6791285276412964\n",
            "Validation f1 Increased(0.751538--->0.774591) \t Saving The Model\n",
            "Epoch 16 \t\t Training f1: 0.6855922341346741 \t\t Validation f1: 0.7854255437850952\t\t Loss: 0.5852464437484741\n",
            "Validation f1 Increased(0.774591--->0.785426) \t Saving The Model\n",
            "Epoch 17 \t\t Training f1: 0.6319444179534912 \t\t Validation f1: 0.7806109189987183\t\t Loss: 0.7455697655677795\n",
            "Epoch 18 \t\t Training f1: 0.7666666507720947 \t\t Validation f1: 0.7765315175056458\t\t Loss: 0.4744921028614044\n",
            "\n",
            " Performance:  0.7496848702430725 \t Time elapsed:  19.044478178024292\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 60\n",
        "time = choose_model(num_epochs, optimizer, train_dataloader, loss_fn, net2, learning_rate, val_dataloader, patience=2, reproducibility=True)\n",
        "optimal_model = LeNet1().to(device)\n",
        "optimal_model.load_state_dict(torch.load('optimal.pth'))\n",
        "_, apodosi, _, _ = test_nn(test_dataloader, optimal_model, loss_fn)\n",
        "print('\\n Performance: ', apodosi, '\\t Time elapsed: ', time)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "4639953fc310b90d39c886c321531fbb14f4e717de41ce7b5552e47b9b7e1799"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
